00007936-8061-4fc5-b3f6-a0710e0d787a gps data based urban guidance in many metropolitan areas, traffic congestion is an escalating problem which causes a significant waste of money and time. nowadays, cars equipped with gps devices become widespread. the location information of those cars is very useful for estimate traffic condition in the complex city road network. using the accurate and real time traffic condition, we can provide dynamic route guidance to ease traffic congestion. in this paper, we proposed a speed pattern model, called two phase piecewise linear speed model (2peed), to estimate traffic condition and represent speed pattern in a road network using gps data collected vehicles. with the estimated traffic condition and speed pattern, a proposed classification-based route guidance approach using machine learning technique provides dynamic routing for drivers. using both current traffic data and the experience learned from history data, our route guidance approach is able to accurately predict the future traffic condition and selects a best route. we give simulation results to show that the proposed approach is able to select and dynamically update a route to prove drivers a best (e.g., less traffic and shortest travel time) route to their destination.
000152be-81d0-4b83-91bf-59c9d4ca2061 a role-based customer review mining system with the development of www (world wide web), more and more people surf on the web, more and more web sites provide forum for people to publish their reviews, so there are many reviews on some special topic in many forums. our system mainly aims at the reviews of customer for some product. the data set comes from forum data or emails or any other form of reviews. the mining result can guide the company's ceo to make science decision for company's products research and market development and the mining result can guide the customers to purchase more satisfying products. our system mainly adopts data mining technology, natural language processing technology, web text mining technology and so on. in the realization of our system, we adopt role based concepts and theory, this makes the realization more reasonable and more efficiency and this makes the system function more perfect and more science.
00026405-7b8d-4446-9e8e-032913c4d420 introduction to developmental robotics developmental robotics is a broad, new discipline that lies at the intersections of psychology, biology, artificial intelligence (ai) and robotics. this new field was inspired by the fact that most complex and intelligent biological organisms (as opposed to artificial ones) undergo an extended period of development before reaching their adult form and adult abilities. this new rubric captures the essential features of many related, previous research agendas, including embodied cognition, evolutionary robotics and machine learning. although developmental robotics combines many of these previous efforts, it also has fundamental differences that separate it in a number of interesting ways. to appreciate these differences, it is useful to reflect on the history of robotics and ai. since the inception of ai in the 1950s, its practitioners have been striving to create intelligent machines. there have been some notable successes in restricted domains, such as game playing. however, the vision of creating general-purpose, human-like intelligence has not yet been achieved. to date, there have been three primary approaches to trying to create intelligent robots: direct programming; supervised machine learning; and evolutionary adaptation (weng et al. 2001). in direct programming, a human engineer analyses the problem domain, determines a solution and then implements the solution in a program. here the intelligence resides solely in the human engineer, the robot is merely acting out the pre-programmed commands. robots created by direct programming tend to be brittle and fail in new situations not anticipated by the human engineer. in supervised learning, a human engineer creates a series of training situations describing how the robot should respond to particular sensory inputs. the robot learns to mimic the training data and typically makes useful generalizations that apply to novel situations that were never seen during training. this is an improvement over direct programming in that the robot, rather than the human engineer, determines how to solve the problem, and the robot can go beyond what it was initially exposed to, leading to more robust behaviour. however,
0003e9bf-4798-4a21-ae31-e4ede65ea939 performance evaluation of tile coding in reinforcement learning reinforcement learning is one of research fields in artificial intelligence. the learning method usually assumes a discrete state in computer simulations. however, we must treat a continuous value in a realistic situation. in this paper, we investigate various techniques of the tile cording scheme which is a representative technique to handle continuous states. we check the performance of single tiling, multiple tiling, time-shift method and the proposed method in the issue of space search.
0004879d-c6fd-46c9-88e8-636addcabdbf swarm intelligence solution to large scale thermal power plant load dispatch economic load dispatch is one of the major functions of modern energy management system (ems), which determines the optimal real power settings of generating units with an objective of minimizing the total fuel cost. all industrial practice, the fuel cost of generators can be treated as a quadratic function of real power generation. in fact, valve point loading effect in thermal power plants calls discontinuity. the classical optimization methods require continuous differentiable objective functions; therefore they fail to provide global minima. the evolutionary computation methods can handle non-differential and non-convex objective functions and give global or near global optimum solutions. evolutionary techniques such as genetic algorithm (ga), evolutionary programming (ep) are applied to economic dispatch problem. similar to evolutionary computation, social behaviour artificial intelligence called swarm intelligence is applied in many fields. use of swarm intelligence not only avoids coding and monotonous decoding as prevalent transformations of ga and also results in less burden on parameter settings, population size and number of iterations. in this paper, swarm intelligence is applied to solve large scale economic load dispatch problem with valve point loading. particle swarm optimization (pso) algorithm is tested on a 13 and 40 unit test system. results obtained show that pso algorithm has a great potential in handling large size complex optimization problems and capable to produce results in very less time.
0004c39c-3b32-4175-8320-b7563f97b83a a course for data modelling what direction or path of learning should data modelling take? this author suggests a  course  for data modelling to follow. artificial intelligence, databases, and programming languages are attempting to meet demands requiring a higher level of conceptual thinking than ever before. likewise special tools and techniques are needed for that level of thought. therefore data modelling methodologies are designed to provide an abstract level of system descriptions. a course based upon the comparative study of these various data modelling principles is described in this paper. this course stresses a transition from intuitive to formal development of data models. such an approach facilitates the investigation of the concept of equivalence for data models, application models, and operations defined on data models.
000523f9-1bb6-4f8f-a665-0ca6062077a8 probabilistic model building genetic network programming using multiple probability vectors as an extension of ga and gp, a new evolutionary algorithm named genetic network programming (gnp) has been proposed. gnp uses the directed graph structure to represent its solutions, which can express the dynamic environment efficiently. the reusable nodes of gnp can construct compact structures, leading to a good performance in complex problems. in addition, a probabilistic model building gnp named gnp with estimation of distribution algorithm (gnp-eda) has been proposed to improve the evolution efficiency. gnp-eda outperforms the conventional gnp by constructing a probabilistic model by estimating the probability distribution from the selected elite individuals of the previous generation. in this paper, a probabilistic model building gnp with multiple probability vectors (pmbgnp m ) is proposed. in the proposed algorithm, multiple probability vectors are used in order to escape from premature convergence, and genetic operations like crossover and mutation are carried out to the probability vectors to maintain the diversities of the populations. the proposed algorithm is applied to the controller of autonomous robots and its performance is evaluated.
00059ae1-256a-4545-aa0a-8559f92bab23 using ontologies to facilitate post-processing of association rules by domain experts data mining is used to discover hidden patterns or structures in large databases. association rule induction extracts frequently occurring patterns in the form of association rules. however, this technique has a drawback as it typically generates a large number of association rules. several methods have been proposed to prune the set of extracted rules in order to present only those which are of interest to the domain experts. some of these methods involve subjective analysis based on prior domain knowledge, while others can be considered to involve objective, data-driven analysis based on numerical measures that provide a partial description of the interestingness of the extracted association rules. recently it has been proposed that ontologies could be used to guide the data mining process. in this paper, we propose a hybrid pruning method that involve the use of objective analysis and subjective analysis, with the latter involving the use of an ontology. we demonstrate the applicability of this hybrid method using a medical database.
00060b86-de8c-4ee6-8fb6-6fbddb9acb4d knowledge discovery for tourism using data mining and qualitative analysis: a case study at johor bahru, malaysia this paper aims to propose a new guideline for analyzing tourist profiles as found in www.tripadvisor.com. these have been examined from two different aspects so as to gain conclusive results. tourist data were acrawleda from tripadvisor.com through a specific web crawler. mining techniques using a combination of visualization, clustering, and association rules were instrumental in discovering the first set of interesting knowledge. this was followed by a qualitative analysis applied through nvivo software via coding of the tourist's comments in order to define the design of the prospective model. a final set of results was obtained once both results confirmed each other. in this study, results show that there are several types of tourists; with each group having different preferences. for example: male singaporean visitors to hotels tend to enjoy wine and food in addition to outdoor activities; while local visitors to legoland are not satisfied with certain aspects, such as the price of food. international tourists, however, consider the affirmative points of legoland. this research can be very useful for tourist associations and hotel managers in johor bahru.
00066e77-2076-42da-9a29-e6ecd67a9bdc nowledge acquisition an support scientific communities widespread access to the internet has led to the formation of geographically dispersed scientific communities collaborating through the network. the tools supporting such collaboration currently are based primarily on electronic mail through mailing list servers, and access to archives of research reports through ftp, gopher and world wide web. however, electronic communication can support the knowledge processes of scientific communities more directly through overtly represented knowledge structures. this paper describes some experiments in the use of knowledge acquisition (isa) and representation (kr) tools to define and analyze major policy and technical issues in an international research community responsible for one of the test cases in the intelligent manufacturing systems (ims) research program. it is concluded that distributed knowledge support systems in routine use by world-class scientific communities collaborating through the internet will provide a major impetus to artificial intelligence research.
0006838d-7042-4050-81c5-5a720ba98140 influence of statins on postoperative wound complications after inguinal or ventral herniorrhaphy the lipid-lowering agents, statins, are the most commonly prescribed class of drugs in the western world. because of their widespread use, many patients undergo surgical procedures while on statins. statins, in addition to cholesterol-lowering effects, also have anticoagulant, immunosuppressive, and antiproliferative properties that may affect the risk of local wound complications. this study investigated the relationship between statins and postoperative wound complications in a large cohort of patients undergoing inguinal or ventral hernia repair. data mining was performed in the veterans integrated service network (visn)16 data warehouse. this database contains clinical and demographic information about all veterans cared for at the ten va medical centers that comprise the south central va healthcare network in the mid-south region of the us. aggregate data (age, body mass index, smoking history, gender, race, history of diabetes, statin use, and postoperative wound complications) were obtained for all patients who underwent inguinal or ventral hernia repair during the period october 1, 1996november 30, 2004. during the period of the query, 10,782 patients (10,676 male, 106 female), 1,242 (11.5%) of whom received statins, underwent herniorrhaphy. statin use did not affect the risk of wound infection or delayed wound healing. statin use was, however, associated with an increased rate of local postoperative bleeding complications (p=0.01). when the type of hernia, age, smoking, diabetes, and body mass index were included in a multivariate analysis, statins remained borderline significant as an independent predictor of wound hematoma/postoperative bleeding (p=0.04), odds ratio 1.6 (95% ci 1.032.44). patients who undergo inguinal herniorrhaphy while on statins have an increased risk of postoperative wound hematoma/hemorrhage. focus on additional factors that may affect the propensity to postoperative bleeding and on meticulous intraoperative hemostasis are particularly important in such patients.
0006d240-14e3-4aaf-9644-2df77eb2b5b6 towards more intelligent and adaptive user interfaces for control center applications user interfaces are a key factor for the success of any computer application and assume an enormous importance in the case of applications used in critical and emergency conditions, as real time applications in control centers. the introduction of intelligent applications in these centers, makes more obvious the need of better user interfaces, presenting more intelligent and adaptive behaviour. these new generation of user interfaces makes use of full-graphics displays but also of artificial intelligence concepts. knowledge bases are used in order to provide the user interface with the required knowledge about the computer application and its users. this paper deals with the evolution of the user interface of sparse, an expert system for alarm processing and operator assistance in service restoration developed for the portuguese transmission network. the architecture of this interface is presented and the results obtained are discussed.
000717bb-53f9-4840-b66e-602fc99759fc detection of iupac and iupac-like chemical names motivation: chemical compounds like small signal molecules or other biological active chemical substances are an important entity class in life science publications and patents. several representations and nomenclatures for chemicals like smiles, inchi, iupac or trivial names exist. only smiles and inchi names allow a direct structure search, but in biomedical texts trivial names and iupac like names are used more frequent. while trivial names can be found with a dictionary-based approach and in such a way mapped to their corresponding structures, it is not possible to enumerate all iupac names. in this work, we present a new machine learning approach based on conditional random fields (crf) to find mentions of iupac and iupac-like names in scientific text as well as its evaluation and the conversion rate with available name-to-structure tools.#r##n##r##n#results: we present an iupac name recognizer with an f1 measure of 85.6% on a medline corpus. the evaluation of different crf orders and offset conjunction orders demonstrates the importance of these parameters. an evaluation of hand-selected patent sections containing large enumerations and terms with mixed nomenclature shows a good performance on these cases (f1 measure 81.5%). remaining recognition problems are to detect correct borders of the typically long terms, especially when occurring in parentheses or enumerations. we demonstrate the scalability of our implementation by providing results from a full medline run.#r##n##r##n#availability: we plan to publish the corpora, annotation guideline as well as the conditional random field model as a uima component.#r##n##r##n#contact: roman.klinger@scai.fraunhofer.de
00083ba9-18d8-4ca3-8d81-8b169b9ea58e image analysis for soil dye tracer infiltration studies flow processes in soils are closely related to groundwater quality often affected by human activities. because hydrological models usually lack explanatory power, direct visualization of flow paths in dye tracer infiltration studies has become a standard tool in soil hydrology. these experiments provide images of dye-stained paths in soils and help evaluating the vulnerability or understanding the general hydrological functioning of a given site. extracting relevant information demands expertise in hydrology as well as in image analysis and statistics. to our knowledge, no agreed and effective method to analyze large collections of such images exists in the soil hydrology community. in this paper we propose a general framework consisting of index functions and visual tools to support the expert in his/her evaluation of dye tracer infiltration images.
00086598-d607-4eef-8f78-7c22e92e2beb robot uses emotions to detect and learn the unknown humans can perceive and learn new information from novel, previously unknown to them kinds of experiences, which can be very challenging for an artificial system. here, a cognitive architecture is presented that uses its emotional intelligence to learn new concepts from previously unknown kinds of experiences. the underlying principle is that emotional appraisals of experience expressed internally as several monads help the architecture to detect conceptual novelty and facilitate the generation and learning of new concepts. with the goal of measuring effects of emotional cognition on learning, the architecture was implemented in a robot and studied in a number of paradigms involving variable color settings. the key findings are the following. initially, the dynamic state of the model neural network does not converge to some attractor when it receives an unknown kind of input. on the other hand, it quickly converges to an attractor in response to a familiar input. with time, the system develops the ability to learn previously unknown categories and concepts as new monad. it is proposed that the model simulates a subliminal response of a human brain to an unknown situation. the findings have broad implications for future emotional artificial intelligence.
0008c70e-e45b-453a-8a7d-8ff14dfca371 weighted markov chains and graphic state nodes for information retrieval abstract#r##n##r##n#decision-making in uncertain environments, such as data mining, involves a computer user navigating through multiple steps, from initial submission of a query through evaluating retrieval results, determining degrees of acceptability of the results, and advancing to a terminal state of evaluating where the interaction is successful or not. this paper describes iterative information seeking (is) as a markov process during which users advance through states of nodes. nodes are graphic objects on a computer screen that represent both the state of the system and the group of users' or an individual user's degree of confidence in an individual node. after examining nodes to establish a confidence level, the system records the decision as weights affecting the probability of the transition paths between nodes. by training the system in this way, the model incorporates into the underlying markov process users' decisions as a means to reduce uncertainty. the markov chain becomes a weighted one whereby the is makes justified suggestions.
0009e067-05b5-4cba-ae36-e16e4d8a711e the development, control and operation of an autonomous robotic excavator the excavation of foundations, general earthworks and earth removal tasks are activities which involve the machine operator in a series of repetitive operations, suggesting opportunities for the automation through the introduction of robotic technologies with subsequent improvements in machine utilisation and throughput. the automation of the earth removal process is also likely to provide a number of other benefits such as a reduced dependence on operator skills and a lower operator work load, both of which might be expected to contribute to improvements in quality and, in particular, the removal of the need for a local operator when working in hazardous environments.#r##n##r##n#the lancaster university computerised intelligent excavator or lucie has demonstrated the achievement of automated and robotic excavation through the implementation of an integrated, real-time, artificial intelligence based control system utilising a novel form of motion control strategy for movement of the excavator bucket through ground. having its origins in the systematic observation of a range of machine operators of differing levels of expertise, the control strategy as evolved enables the autonomous excavation of a high quality rectangular trench in a wide variety of types and conditions of ground and the autonomous removal of obstacles such as boulders along the line of that trench.#r##n##r##n#the paper considers the development of the lucie programme since its inception and sets out in terms of the machine kinematics the evolution and development of the real-time control strategy from an implementation on a one-fifth scale model of a back-hoe arm to a full working system on a jcb801 360 tracked excavator.
000a3e1b-804b-41b3-a6d2-6e0d543687d1 auto-encoder-based shared mid-level visual dictionary learning for scene classification using very high resolution remote sensing images effective representation and classification of scenes using very high resolution (vhr) remote sensing images cover a wide range of applications. although robust low-level image features have been proven to be effective for scene classification, they are not semantically meaningful and thus have difficulty to deal with challenging visual recognition tasks. in this study, the authors propose a new and effective auto-encoder-based method to learn a shared mid-level visual dictionary. this dictionary serves as a shared and universal basis to discover mid-level visual elements. on the one hand, the mid-level visual dictionary learnt using machine learning technique is more discriminative and contains rich semantic information, compared with the traditional low-level visual words. on the other hand, the mid-level visual dictionary is more robust to occlusions and image clutters. in the authors' scene-classification scheme, they use discriminative mid-level visual elements, rather than individual pixels or low-level image features, to represent images. this new image representation is able to capture much of the high-level meaning and contents of the image, facilitating challenging remote sensing image scene-classification tasks. comprehensive evaluations on a challenging vhr remote sensing images data set and comparisons with state-of-the-art approaches demonstrate the effectiveness and superiority of their study.
000a6681-6007-4864-bd70-028c09999766 vq-based written language identification humans can recognize different types of written languages by their grammars and vocabularies. however, computers see everything as numbers. we present a computational algorithm for machine classification of written languages using the method of vector quantization. for a language document, each word is converted to a sequence of numbers and forms as a vector of numerical values according to its characters. this collection of vectors is then represented by a codebook that contains a number of template vectors for classification. the proposed method is more effective for machine learning than the n-gram based method, which has been widely used for written language identification. experimental results of classifying a set of five closely roman-typed scripts show the promising application of the proposed method.
000b145a-79b6-4c43-ab82-2a033d6f52c2 providing deliberation to emotional agents 
000b1710-e111-48d3-9a89-375d1bd894b3 a wireless mobile sensor platform for structural health monitoring modern system identification (sid) procedures rely on fixed sensor networks for data collection. ideally, sensors are fixed at locations and contain profitable structural responses, however, such sensing areas are often limited by the accessibility of the structure and environmental hazards. not only are fixed sensor placements limited, the data contains restricted spatial information. mobile sensors simultaneously record data in time while moving in space, so that few sensors collect data containing dense, less-restricted spatial information, providing a more cost-effective solution than a dense array of static sensors. mobile sensor data contain fundamentally different attributes than fixed sensor data. such data can be classified as dynamic sensor network (dsn) data, which inherently include spatial discontinuities whenever sensors change position. despite this challenge, such data can be processed for identification using an iterative machine learning technique structural identification using expectation maximization (stride). furthermore, the preservation of spatial information in mobile sensor networks has been quantified throughout simulations: given the same number of sensors, a mobile sensor network produces superior spatial information when compared to a static sensor network. in this paper, ambient vibrations of a simple beam test-bed are measured by a wireless mobile sensor network which includes four parallel lines of motor-driven belts that tow an array of sensor carts along the span of the beam. feedback between the step motor and a computer was established to achieve a precise spatial grid for the mobile sensors. modal identification results are presented, documenting the accuracy and feasibility of a realworld mobile sensor network for sid.
000b483d-7c18-4f9d-a880-4792135b9a06 human factors in webserver log file analysis: a controlled experiment on investigating malicious activity while automated methods are the first line of defense for detecting attacks on webservers, a human agent is required to understand the attacker's intent and the attack process. the goal of this research is to understand the value of various log fields and the cognitive processes by which log information is grouped, searched, and correlated. such knowledge will enable the development of human-focused log file investigation technologies. we performed controlled experiments with 65 subjects (it professionals and novices) who investigated excerpts from six webserver log files. quantitative and qualitative data were gathered to: 1) analyze subject accuracy in identifying malicious activity; 2) identify the most useful pieces of log file information; and 3) understand the techniques and strategies used by subjects to process the information. statistically significant effects were observed in the accuracy of identifying attacks and time taken depending on the type of attack. systematic differences were also observed in the log fields used by high-performing and low-performing groups. the findings include: 1) new insights into how specific log data fields are used to effectively assess potentially malicious activity; 2) obfuscating factors in log data from a human cognitive perspective; and 3) practical implications for tools to support log file investigations.
000b7263-4310-4a34-ba4c-e4ae03ae5fe9 modified neural network algorithms for predicting trading signals of stock market indices the aim of this paper is to present modified neural network algorithms to predict whether it is best to buy, hold, or sell shares (trading signals) of stock market indices. most commonly used classification techniques are not successful in predicting trading signals when the distribution of the actual trading signals, among these three classes, is imbalanced. the modified network algorithms are based on the structure of feedforward neural networks and a modified ordinary least squares (olss) error function. an adjustment relating to the contribution from the historical data used for training the networks and penalisation of incorrectly classified trading signals were accounted for, when modifying the ols function. a global optimization algorithm was employed to train these networks. these algorithms were employed to predict the trading signals of the australian all ordinary index. the algorithms with the modified error functions introduced by this study produced better predictions.
000b8337-9b9e-40d5-aaac-2ac0ddca3a88 global and local features for accurate impression estimation of cloth fabric images consumers' psychological feeling or impression is an important factor for product design. the impression estimation becomes an important issue. in this paper, we proposed a machine learning based impression estimation method for cloth fabric images. we use a semantic differential (sd) method to measure the user's impression such as bright, warm while they viewing a cloth fabric image. we also extract both global and local features of cloth fabric images such as color and texture using computer vision techniques. then we use support vector regression to model the mapping function between the impression and image features. the learned mapping function is used to estimate the impression of cloth fabric images.
000b98ff-070b-4d21-a24b-737b18862604 application of type-2 neuro-fuzzy modeling in stock price prediction we present an application of type-2 neuro-fuzzy modeling to stock price prediction based on a given set of training data. type-2 fuzzy rules can be generated automatically by a self-constructing clustering method and the obtained type-2 fuzzy rules cab be refined by a hybrid learning algorithm. the given training data set is partitioned into clusters through input-similarity and output-similarity tests, and a type-2 tsk rule is derived from each cluster to form a fuzzy rule base. then the antecedent and consequent parameters associated with the rules are refined by particle swarm optimization and least squares estimation. experimental results, obtained by running on several datasets taken from taiex and nasdaq, demonstrate the effectiveness of the type-2 neuro-fuzzy modeling approach in stock price prediction.
000be0bf-fda1-478a-b128-703cd85e99d7 privacy preserving data mining using particle swarm optimisation trained auto-associative neural network: an application to bankruptcy prediction in banks while data mining made inroads into the diverse areas it also entails violation of individual privacy leading to legal complications in areas like medicine and finance as consequently, privacy preserving data mining (ppdm) emerged as a new area. to achieve an equitable solution to this problem, data owners must not only preserve privacy and but also guarantee valid data mining results. this paper proposes a novel particle swarm optimisation (pso) trained auto associative neural network (psoaann) for privacy preservation. then, decision tree and logistic regression are invoked for data mining purpose, leading to psoaann + dt and psoaann + lr hybrids. the efficacy of hybrids is tested on five benchmark and four bankruptcy datasets. the results are compared with those of ramu and ravi (2009) and others. it was observed that the proposed hybrids yielded better or comparable results. we conclude that psoaann can be used as viable approach for privacy preservation.
000c0aa5-fc2b-4d6b-9610-57abbbead9f7 customer defections analysis: an examination of online bookstores purpose  the cost of retaining a customer is lower than that of obtaining a new one, so potential customer defection is an important issue in the fiercely competitive environment of electronic commerce. accordingly, this paper aims to present a new way for gauging customer loyalty and predicting their possibility of defection reference to a set of quality attributes satisfaction and three types of belief in the theory of planned behavior (tpb).design/methodology/approach  the performance of the classification utilization artificial neural networks (anns) was compared to that of traditional analytic tools, such as multiple discriminant analysis (mda) and classificatory data mining technique  decision tree.findings  the analytical result represented that the predicted accuracy of anns is better then mda and decision tree in both training and testing phases. degree of repurchase intention has been classified correctly with a success rate of 83 percent using neural networks.research limitations/implicatio...
000c7fb7-420c-4826-87bd-488fa3c836fe an intelligent portfolio-management approach to gas storage field deliverability maintenance and enhancement: part one--database development and model building the main goal of this paper is to modify and apply the stateof-the-art intelligent, optimum portfolio management to the gas storage field in order to optimize the return on investment associated with well remedial operations. it continues the development of a methodology for candidate selection and stimulation design and optimization using artificial intelligence techniques. the data of an actual gas storage field was used to test the results. the project data include well-bore, completion, perforation, stimulation, well-test and reservoir data. to make candidate selection for gas storage fields operators predict the effectiveness of the stimulation commonly using three parameters. one in peak day rate second is absolute open flow and third is change in skin provided permeability values in the field dont vary much. the software developed in parallel with this selection methodology includes an easy to use interface that allows the user to edit the data for a gas storage field, perform well-test analysis and use neural networks in association with genetic optimization tool. the software ranks the well according to maximum change in skin value and recommends the best stimulation slurry based on the weitage given to the skin and cost of stimulation. a decision to select the ranked wells for re-stimulate can be made accordingly. background
000d2dea-7d92-41ac-ae39-d4eaf7fb3aad reasoning about actions and change in argumentation this paper studies how logic-based reasoning about actions and change (rac) with its problems of temporal projection and qualification can be formalised in terms of argumentation. in particular, we extend earlier work of translating the language for rac into a logic-based argumentation framework, by introducing new types of arguments for (i) backward persistence and (ii) persistence from observations. this forms a conservative extension of the language that gives a semantic meaning to domains that cannot be interpreted under thus addressing further the frame and (exogenous) qualification problems. as such the paper strengthens the link between argumentation theory and rac in artificial intelligence.
000d84fb-22ac-44f6-bed6-fd62d211324c bayesian analysis of cosmic-ray propagation: evidence against homogeneous diffusion we present the results of the most complete ever scan of the parameter space for cosmic ray (cr) injection and propagation. we perform a bayesian search of the main galprop parameters, using the multinest nested sampling algorithm, augmented by the bambi neural network machine learning package. this is the first such study to separate out low-mass isotopes ($p$, $\bar p$ and he) from the usual light elements (be, b, c, n, o). we find that the propagation parameters that best fit $p$, $\bar p$, he data are significantly different from those that fit light elements, including the b/c and $^{10}$be/$^9$be secondary-to-primary ratios normally used to calibrate propagation parameters. this suggests each set of species is probing a very different interstellar medium, and that the standard approach of calibrating propagation parameters using b/c can lead to incorrect results. we present posterior distributions and best fit parameters for propagation of both sets of nuclei, as well as for the injection abundances of elements from h to si. the input galdef files with these new parameters will be included in an upcoming public galprop update.
000d9c65-b685-4908-b5ae-885a4e0acc6e a selective survey of the use of artificial intelligence for database design systems one of the significant developments of research in artificial intelligence and databases is the adoption of knowledge-based techniques for automating database design. this paper examines the database design process by phases, identifies the key challenges of each, and discusses how a knowledge-based approach could contribute to meeting these challenges. for each design phase, two or three prototype systems that employ interesting knowledge-based techniques are analyzed using a common framework and example. based on this discussion, suggestions are made for future research.
000de4f3-59eb-458a-9bdf-480213015593 map based representation of navigation information for robust machines learning much research is being carried out in autonomous driving of vehicles under various disciplines but very few autonomous navigating vehicles are developed till date. this paper presents a novel technique, with a practical example, for the representation of navigational information under real-time considerations. machine learning algorithms can easily be trained with the data sets developed from the representation and to testify this, an artificial neural network is trained with a represented data set. this technique is independent of driving capabilities of vehicle and provides simple directional instructions for navigation. using this method, an autonomous driving vehicle can be made to learn to navigate between locations in a known region. moreover, the usage of ann makes it completely adaptive and any changes or modifications in the trained region can easily be updated into the knowledge base.
000ea975-1c97-4ba7-87ff-f15adfa749de random effects logistic regression model for anomaly detection as the influence of the internet continues to expand as a medium for communications and commerce, the threat from spammers, system attackers, and criminal enterprises has grown accordingly. this paper proposes a random effects logistic regression model to predict anomaly detection. unlike the previous studies on anomaly detection, a random effects model was applied, which accommodates not only the risk factors of the exposures but also the uncertainty not explained by such factors. the specific factors of the risk category such as retained 'protocol type' and 'logged in' are included in the proposed model. the research is based on a sample of 49,427 random observations for 42 variables of the kdd-cup 1999 (data mining and knowledge discovery competition) data set that contains 'normal' and 'anomaly' connections. the proposed model has a classification accuracy of 98.94% for the training data set, while that for the validation data set is 98.68%.
00118430-a879-4d28-8270-c8414c5caf00 unified video annotation via multigraph learning learning-based video annotation is a promising approach to facilitating video retrieval and it can avoid the intensive labor costs of pure manual annotation. but it frequently encounters several difficulties, such as insufficiency of training data and the curse of dimensionality. in this paper, we propose a method named optimized multigraph-based semi-supervised learning (omg-ssl), which aims to simultaneously tackle these difficulties in a unified scheme. we show that various crucial factors in video annotation, including multiple modalities, multiple distance functions, and temporal consistency, all correspond to different relationships among video units, and hence they can be represented by different graphs. therefore, these factors can be simultaneously dealt with by learning with multiple graphs, namely, the proposed omg-ssl approach. different from the existing graph-based semi-supervised learning methods that only utilize one graph, omg-ssl integrates multiple graphs into a regularization framework in order to sufficiently explore their complementation. we show that this scheme is equivalent to first fusing multiple graphs and then conducting semi-supervised learning on the fused graph. through an optimization approach, it is able to assign suitable weights to the graphs. furthermore, we show that the proposed method can be implemented through a computationally efficient iterative process. extensive experiments on the trec video retrieval evaluation (trecvid) benchmark have demonstrated the effectiveness and efficiency of our proposed approach.
0011a3a9-2e48-419b-8915-74bc9506cd12 predictors of individual response to placebo or tadalafil 5mg among men with lower urinary tract symptoms secondary to benign prostatic hyperplasia: an integrated clinical data mining analysis background #r##n#a significant percentage of patients with lower urinary tract symptoms (luts) secondary to benign prostatic hyperplasia (bph) achieve clinically meaningful improvement when receiving placebo or tadalafil 5mg once daily. however, individual patient characteristics associated with treatment response are unknown.#r##n#methods #r##n#this integrated clinical data mining analysis was designed to identify factors associated with a clinically meaningful response to placebo or tadalafil 5mg once daily in an individual patient with luts-bph. analyses were performed on pooled data from four randomized, placebo-controlled, double-blind, clinical studies, including about 1,500 patients, from which 107 baseline characteristics were selected and 8 response criteria. the split set evaluation method (1,000 repeats) was used to estimate prediction accuracy, with the database randomly split into training and test subsets. logistic regression (lr), decision tree (dt), support vector machine (svm) and random forest (rf) models were then generated on the training subset and used to predict response in the test subset. prediction models were generated for placebo and tadalafil 5mg once daily receiver operating curve (roc) analysis was used to select optimal prediction models lying on the roc surface.#r##n#findings #r##n#international prostate symptom score (ipss) baseline group (mild/moderate vs. severe) for active treatment and placebo achieved the highest combined sensitivity and specificity of 70% and ~50% for all analyses, respectively. this was below the sensitivity and specificity threshold of 80% that would enable reliable allocation of an individual patient to either the responder or non-responder group#r##n#conclusions #r##n#this extensive clinical data mining study in luts-bph did not identify baseline clinical or demographic characteristics that were sufficiently predictive of an individual patient response to placebo or once daily tadalafil 5mg. however, the study reaffirms the efficacy of tadalalfil 5mg once daily in the treatment of luts-bph in the majority of patients and the importance of evaluating individual patient need in selecting the most appropriate treatment.
0012b7b2-af87-4b00-adba-75935267b9bf device identification from mixture of measurable characteristics in this paper, a novel framework that realizes identification of power mosfets for enhancing their security is proposed. selecting mixture of measurable device parameters of mosfets as a feature vector, individual devices are robustly distinguished. in the experiments using a 11-parameter feature vector with machine learning techniques of random forest and support vector machine (svm) as example classifiers, 70 commercial quality sic power mosfets were successfully distinguished at 98.1% accuracy.
0012c8cd-a095-44c8-ad04-49bcbbc3d264 machine learning for profiling network traffic 
0013772c-3f7f-40c3-8e06-1028c61e7eae based on the improved k - means algorithm of tianjin port traffic flow characteristic analysis mastering the characteristics of port ships traffic flow is the premise to plan, operate, and control waterway and navigation reasonably. therefore, analysis of traffic flow characteristic of port ships is necessary and extremely important. because vessel traffic flow data acquisition methods are diverse, they contain flawed and incorrect data. to solve the problem, we made use of the improved k-means algorithm to prove the raw data of sequence samples. we applied data mining clustering analysis method to the tianjin port ships traffic flow data characteristic for analysis. the results show that some valuable information is obtained by the proposed method which can also provide decision support for maritime safety administration.
0013863d-fdd4-47ee-a411-2408cbd5581e obsolescence risk optimization using fuzzy logic and genetic algorithm component obsolescence is a fact causing high values of losses and management costs. it always happens when some components of a complex system have the life time less than the expected life time for the whole system. artificial intelligence fields could help managers with component obsolescence problem. there have been lots of researches focusing on key factors of component obsolescence risk but there is a gap in the way of choosing low risk components from the view of business intelligence. in this paper an innovative framework is presented using genetic algorithm and fuzzy logic that chooses the best components among the available alternatives. the result would be the best mixture of components with lowest component obsolescence risk. obsolescence impact, obsolescence happening probability and the obsolescence time of each available component are considered as key factors of obsolescence risk management.
00140616-6f2b-41b1-a715-a30b227e5a5e prediction of the best eor method by artificial intelligence in this article, five type of enhanced oil recovery (eor) methods and seven of the most important reservoir parameter get noticed. also, data from more than 200 oil fields from all around the world collected where at least one of the eor methods has been applied. it has been tried via adaptive network-based fuzzy interface system (anfis) along clustering and normalizing techniques to design a model proficient to foresight and compare behavior of the five eor methods in case of cumulative oil production based on seven input parameters. designed model applied to one of the iranian oil fields and results have been discussed.
001420c9-ef3c-4a44-ad37-0be96ce744a0 multi-objective optimization of a hybrid model for network traffic classification by combining machine learning techniques 
0015185b-efb7-46c0-a4a8-4228c32387d2 a study of improving the performance of mining multi-valued and multi-labeled data nowadays data mining algorithms are successfully applying to analyze the real data in our life to provide useful suggestion. since some available real data is multi-valued and multi-labeled, researchers have focused their attention on developing approaches to mine multi-valued and multi- labeled data in recent years. unfortunately, there are no algorithms can discretize multi-valued and multi-labeled data to improve the performance of data mining. in this paper, we proposed a novel approach to solve this problem. our approach is based on a statistical-based discretization metric and the simulated annealing search algorithm. experimental results show that our approach can effectively improve the performance of the-state-of-art multi-valued and multi-labeled classification algorithm.
00151e79-456a-414d-95f7-d96ed3a71a65 using negotiation techniques as time-restricted scheduling policies on intelligent agents tasks scheduling policies for real-time systems are generally not very flexible due to the time restrictions they have to fulfill. nowadays, research lines to apply artificial intelligence techniques to real-time systems are becoming more relevant, because they can be used to soften tasks scheduling. in this work, we present a proposal in this line. that is, to apply negotiation techniques to optimize real-time systems decisions by increasing and improving the available information to schedule the tasks of an intelligent agent working in a real-time environment. to implement our proposal, we have used an agent working in a hard real-time environment such as   (a real-time intelligence system). finally, we show some results obtained of including such methods in an   agent.
0015b95d-7f02-4959-a730-45806a981d9a unsupervised relation extraction by massive clustering the goal of information extraction is to automatically generate structured pieces of information from the relevant information contained in text documents. machine learning techniques have been applied to reduce the cost of information extraction system adaptation. however, elements of human supervision strongly bias the learning process. unsupervised learning approaches can avoid these biases. in this paper, we propose an unsupervised approach to learning for relation detection, based on the use of massive clustering ensembles. the results obtained on the ace relation mention detection task outperform in terms of f1 score by 5 points the state of the art of unsupervised techniques for this evaluation framework, in addition to being simpler and more flexible.
0016e953-7980-4e23-a87d-883456416892 reducing side effects of hiding sensitive itemsets in privacy preserving data mining data mining is traditionally adopted to retrieve and analyze knowledge from large amounts of data. private or confidential data may be sanitized or suppressed before it is shared or published in public. privacy preserving data mining (ppdm) has thus become an important issue in recent years. the most general way of ppdm is to sanitize the database to hide the sensitive information. in this paper, a novel hiding-missing-artificial utility (hmau) algorithm is proposed to hide sensitive itemsets through transaction deletion. the transaction with the maximal ratio of sensitive to nonsensitive one is thus selected to be entirely deleted. three side effects of hiding failures, missing itemsets, and artificial itemsets are considered to evaluate whether the transactions are required to be deleted for hiding sensitive itemsets. three weights are also assigned as the importance to three factors, which can be set according to the requirement of users. experiments are then conducted to show the performance of the proposed algorithm in execution time, number of deleted transactions, and number of side effects.
0017a0a1-1988-41f4-84ae-831abc9d288d actors: the stage is set the software systems being developed by artificial intelligence researchers are no different in many respects from the systems being developed by the business and scientific communities. they are large, intricate systems often very difficult to implement if they must also be understandable, reliable and maintainable. the ai community has been developing their own ideas to deal with the construction of such systems; ideas whose application is not restricted to the ai domain. this paper introduces the concept of actors, discusses their properties, and describes how actor systems and languages are being used in application areas outside ai. 17 references.
00184f4a-69a2-4fcd-b274-79922a3d33e2 modelling the intelligence phenomenon the paper presents an analysis of the intelligence phenomenon from the point of view of the new artificial intelligence approach. the results of the analysis highlight that intelligence is related to the stability, modelling capacity and perturbation management of the agent. the structure of the paper is as follows. it begins with an introduction to the ai approaches, paradigms and knowledge representation. the main part of the paper focuses on an analysis of the intelligence phenomenon. the paper ends with conclusions and a discussion of future work.
0018aaca-e660-4c71-99f5-dae32b02e41f fuzzy temporal rules for mobile robot guidance in dynamic environments the paper describes a fuzzy control system for the avoidance of moving objects by a robot. the objects move with no type of restriction, varying their velocity and making turns. due to the complex nature of this movement, it is necessary to realize temporal reasoning with the aim of estimating the trend of the moving object. a new paradigm of fuzzy temporal reasoning, which we call fuzzy temporal rules (ftrs), is used for this control task. the control system has over 117 rules, which reflects the complexity of the problem to be tackled. the controller has been subjected to an exhaustive validation process and examples are shown of the results obtained.
0018e91d-dcb8-4999-a5a9-d0d96d4509bb new ways of worldmaking: the alterne platform for vr art we introduce a novel approach to the creation of virtual reality art installations, which supports the design of alternative worlds, in which laws of physics can be redefined to induce new user experiences. to implement this concept of "alternative reality", we have used artificial intelligence techniques to support the definition of the virtual environment behaviour, an approach inspired by qualitative reasoning systems. besides the redefinition of physical laws, we have developed mechanisms for eliciting causal relations between events, as causality plays an important part in users' perception of virtual worlds. our pilot installation is a cavetm-like system incorporating a state-of-the-art computer game engine as visualisation software, which has-been ported to this immersive display. the event-based system underlying the game engine is used to bypass the native physics engine and replace it with our alternative reality software. a first prototype has been fully implemented, the alternative reality modules totaling over 100,000 lines of c++ code. we present early results obtained with this approach, illustrated with examples taken from two artistic briefs, developed by digital artists associated to this research.
0018f6a9-3e67-414c-9f8a-d7783133af1e estimating female labor force participation through statistical and machine learning methods: a comparison 
001a5ddd-8a7f-4da9-a7c9-371d219d985f real-time diagnosis system using incremental emerging pattern mining currently, as a effort to reduce a rate of death by cardiovascular diseases, a lot of researches have been studied regarding real-time diagnosis system. so, we implement a prototype which is contained of stream data processor and incremental data mining module for automatic diagnosis of cardiovascular diseases. in the prototype, (i)ecg signal data which is transported from body-attached sensor is collected and pre-processed, and (ii)diagnosis features of the bio-signal data are extracted. and the patients are automatically diagnosed using the incremental emerging pattern mining module, then (iii)the diagnosis result is provided for the doctor in charge of the patients via a web application in order to manage the medical history of each patient. so, the prototype is able to diagnose and predict patient state on real-time, automatically.
001ac3d9-b77c-4bd6-849b-23abcb50dab3 reexamining database keyword search: beyond performance as an active research field, database keyword search (kws) has put much emphasis on the performance issues, due to its high computational cost. however, a closer examination on kws reveals other interesting aspects worth noting. in this paper, we examine kws from a broader perspective, particularly from its relationship with data mining. freed from syntax-related considerations, kws users now have better opportunities to explore the data in the way as they wish, and such exploration may reveal useful hindsight for what to be done in data mining. recently we have conducted our kws research from this unique perspective. we propose a software environment which offers a dual-mode approach to explore kws: the database mode allows the implementation of database kws directly by incorporating various kws algorithms, while the xml mode converts the database contents to an xml document on which kws is conducted. the dual mode approach not only has the potential of achieving integrated kws on both structured and semistructured data, but also facilitates query relaxation by incorporating ontologies in the xml mode. the software environment still allows us to observe performance related issues of kws; but more importantly, it offers a freehand approach for users to explore the data, thus has the potential of aiding data mining. component design and experimental studies are described.
001aef93-49b7-46b5-b519-e73ec874d688 {fully-automatic bayesian piecewise sparse linear models} piecewise linear models (plms) have been widely used in many enterprise machine learning problems, which assign linear experts to individual partitions on feature spaces and express whole models as patches of local experts. this paper addresses simultaneous model selection issues of plms; partition structure determination and feature selection of individual experts. our contributions are mainly three-fold. first, we extend factorized asymptotic bayesian (f ab) inference for hierarchical mixture
001b1e6b-26ff-4cf7-8c54-8adab2d87d99 forecasting nonstationary time series based on hilbert-huang transform and machine learning we propose a modification of the adaptive approach to time series forecasting. on the first stage, the original signal is decomposed with respect to a special empirical adaptive orthogonal basis, and the hilbert's integral transform is applied. on the second stage, the resulting orthogonal functions and their instantaneous amplitudes are used as input variables for the machine learning unit that employs a hybrid genetic algorithm to train an artificial neural network and a regressive model based on support vector machines. the efficiency of the proposed approach is demonstrated on real data coming from nord pool spot and australian national energy market.
001bdc31-01ab-41e4-bd3e-eef11b0d9498 review of syn-flooding attack detection mechanism denial of service (dos) is a security threat which compromises the confidentiality of information stored in local area networks (lans) due to unauthorized access by spoofed ip addresses. syn flooding is a type of dos which is harmful to network as the flooding of packets may delay other users from accessing the server and in severe cases, the server may need to be shut down, wasting valuable resources, especially in critical real-time services such as in e-commerce and the medical field. the objective of this paper is to review the state-of-the art of detection mechanisms for syn flooding. the detection schemes for syn flooding attacks have been classified broadly into three categories  detection schemes based on the router data structure, detection schemes based on statistical analysis of the packet flow and detection schemes based on artificial intelligence. the advantages and disadvantages for various detection schemes under each category have been critically examined. the performance measures of the categories have also been compared.
001c3c2e-ae13-4cd7-a7ea-4685447e765c geometric lattice structure of covering and its application to attribute reduction through matroids the reduction of covering decision systems is an important problem in data mining, and covering-based rough sets serve as an efficient technique to process the problem. geometric lattices have been widely used in many fields, especially greedy algorithm design which plays an important role in the reduction problems. therefore, it is meaningful to combine coverings with geometric lattices to solve the optimization problems. in this paper, we obtain geometric lattices from coverings through matroids and then apply them to the issue of attribute reduction. first, a geometric lattice structure of a covering is constructed through transversal matroids. then its atoms are studied and used to describe the lattice. second, considering that all the closed sets of a finite matroid form a geometric lattice, we propose a dependence space through matroids and study the attribute reduction issues of the space, which realizes the application of geometric lattices to attribute reduction. furthermore, a special type of information system is taken as an example to illustrate the application. in a word, this work points out an interesting view, namely, geometric lattice, to study the attribute reduction issues of information systems.
001c6d67-248c-402c-b02c-2cd127ef9aef voice-based search processing architecture for completing search queries by using artificial intelligence based schemes to infer search intentions of users. partial queries are completed dynamically in real time. additionally, search aliasing can also be employed. custom tuning can be performed based on at least query inputs in the form of text, graffiti, images, handwriting, voice, audio, and video signals. natural language processing occurs, along with handwriting recognition and slang recognition. the system includes a classifier that receives a partial query as input, accesses a query database based on contents of the query input, and infers an intended search goal from query information stored on the query database. a query formulation engine receives search information associated with the intended search goal and generates a completed formal query for execution.
001ca2af-eae8-4775-8a81-fd1829612f65 cost-sensitive alternating decision trees for record linkage record linkage (rl) is the task of identifying two or more records referring to the same entity (e.g., a person, a company, etc.). rl models can be based on cost sensitive alternating decision trees (adtree), an algorithm that uniquely combines boosting and decision trees algorithms to create shorter and easier-to-interpret linking rules. these models can be naturally trained to operate at industrial precision/recall operating points, and the shorter output rules are so clear that it can effectively explain its decisions to non-technical users via score aggregation or visualization. the models significantly outperform other baselines on the desired industrial operating points, and the improved understanding of the model's decisions led to faster debugging and feature development cycles.
001d9064-93d9-48fa-ba28-aea2d76d89fd approaches for conversion of high phosphorus hot metal to steel for flat products consistent production of low phosphorus steel (0.015%) from high hot metal phosphorus (~0.230%) in bof steelmaking is a technologically challenging task. the problem gets compounded if steel is to be tapped at high temperature (1700c).this paper deals with the continuing efforts and experiences of producing low phosphorus steel at steel melting shop no. 2. two approaches are mentioned. first approach being optimization of slag chilling process near the blow end which has enabled better control of steel phosphorus in high temperature heats. other approach delves into an extensive data mining study to understand dephosphorization behaviour in bof. data mining throws few interesting findings based on which underlying mechanism of bof dephosphorization has been proposed.
0020e077-d006-4e1f-b7ec-49c1fefa17cb a feedback scheme for control of single electron transport: feedback scheme for control of single electron transport 
00213159-bf88-463c-833b-5bde1500d110 detecting flaws and intruders with visual data analysis the task of sifting through large amounts of data to find useful information spawned the field of data mining. most data mining approaches are based on machine-learning techniques, numerical analysis, or statistical modeling. they use human interaction and visualization only minimally. such automatic methods can miss some important features of the data. incorporating human perception into the data mining process through interactive visualization can help us better understand the complex behaviors of computer network systems. this article describes visual-analytics-based solutions and outlines a visual exploration process for log analysis. three log-file analysis applications demonstrate our approach's effectiveness in discovering flaws and intruders in network systems.
002264a0-1a34-4d9b-af17-9af68caba745 an efficient graph partition method for fault section estimation in large-scale power network in order to make fault section estimation (fse) in large scale power networks using a distributed artificial intelligence approach, we have to develop an efficient way to partition the large-scale power network into the desired number of connected sub-networks such that each sub-network should have balanced working burden in performing fse. in this paper, a new efficient multiple-way graph partition method is suggested for the partition task. the method consists of three basic steps. the first step is to form the weighted depth-first-search tree of the power network. the second step is to further partition the network into connected balanced sub-networks. the last step is an iterative process, which tries to minimize the number of the frontier nodes of the sub-networks in order to reduce the required interaction of the adjacent sub-networks. the proposed graph partition approach has been implemented with applications of sparse storage technique. it is further tested in the ieee 14-bus, 30-bus and 118-bus systems respectively. computer simulation results show that the proposed multiple-way graph partition approach is suitable for fse in large-scale power networks and is compared favorably with other graph partition methods suggested in references.
0022f090-e5d1-41f8-a18d-ca123e5971c4 automated natural language headline generation using discriminative machine learning models 
00231608-2174-4478-8bc4-5e8705c6a104 a resource query interface for network-aware applications development of portable network-aware applications demands an interface to the network that allows an application to obtain information about its execution environment. the paper motivates and describes the design of remos, an api that allows network-aware applications to obtain relevant information. the major challenges in defining a uniform interface are network heterogeneity, diversity in traffic requirements, variability of the information, and resource sharing in the network. remos addresses these issues with two abstraction levels, explicit management of resource sharing, and statistical measurements. the flows abstraction captures the communication between nodes, and the topologies abstraction provides a logical view of network connectivity. remos measurements are made at network level, and therefore information to manage sharing of resources is available. remos is designed to deliver best effort information to applications, and it explicitly adds statistical reliability and variability measures to the core information. the paper also presents preliminary results and experience with a prototype remos implementation for a high speed ip based network testbed.
0023c2b7-201d-4b83-a2a1-f69ce86ce31e multiclass fault diagnosis in gears using support vector machine algorithms based on frequency domain data as a dominant machine learning method, the support vector machine is known to have good generalization capability in its application of the multiclass machinefault classification utility. in this paper, an application of the svm in multiclass gearfault diagnosis has been studied when the gear vibration data in frequency domain averaged over a large number of samples is used. it is established that the svm classifier has excellent multiclass classification accuracy when the training data and testing data are at identical angular speeds. however, this method relies on the availability of both the training and testing data at that particular angular speed of the gear operation. but the training data may not always be available at all angular speeds of the gear. hence, two novel techniques, namely the interpolation and the extrapolation methods, have been proposed; these techniques that help the svm classifier perform multiclass gear fault diagnosis with noticeable accuracy, even in the absence of the training data at the testing angular speed. this method is based on interpolating and extrapolating the training data at angular speeds near the speeds of the test data. in this study effects of choice over different kernels and parameters of svm on its overall classification accuracy has been studied and optimum values for these are suggested. finally, the effect on length of training data and data density on the svm accuracy is also presented.
0023d0d5-5168-41a0-8b77-6ac81561ba10 parallel implementation of constraint solving many problems from artificial intelligence can be described as constraint satisfaction problems over finite domains (csp(fd)), that is, a solution is an assignment of a value to each problem variable such that a set of constraints is satisfied. arc-consistency algorithms remove inconsistent values from the set of values that can be assigned to a variable (its domain), thus reducing the search space. we have developed a parallelisation scheme of arc-consistency to be run on mimd multiprocessor. the set of constraints is divided into n partitions, which are executed in parallel on n processors. the parallelisation scheme has been implemented on a cray t3e multiprocessor with up to thirty-four processors. empirical results on speedup and behaviour are reported and discussed.
00246d42-a7af-4bb1-abf3-a00ece3efb39 comparative analysis of emotion estimation methods based on physiological measurements for real-time applications in order to improve intelligent human-computer interaction it is important to create a personalized adaptive emotion estimator that is able to learn over time emotional response idiosyncrasies of individual person and thus enhance estimation accuracy. this paper, with the aim of identifying preferable methods for such a concept, presents an experiment-based comparative study of seven feature reduction and seven machine learning methods commonly used for emotion estimation based on physiological signals. the analysis was performed on data obtained in an emotion elicitation experiment involving 14 participants. specific discrete emotions were targeted with stimuli from the international affective picture system database. the experiment was necessary to achieve the uniformity in the various aspects of emotion elicitation, data processing, feature calculation, self-reporting procedures and estimation evaluation, in order to avoid inconsistency problems that arise when results from studies that use different emotion-related databases are mutually compared. the results of the performed experiment indicate that the combination of a multilayer perceptron (mlp) with sequential floating forward selection (sffs) exhibited the highest accuracy in discrete emotion classification based on physiological features calculated from ecg, respiration, skin conductance and skin temperature. using leave-one-session-out crossvalidation method, 60.3% accuracy in classification of 5 discrete emotions (sadness, disgust, fear, happiness and neutral) was obtained. in order to identify which methods may be the most suitable for real-time estimator adaptation, execution and learning times of emotion estimators were also comparatively analyzed. based on this analysis, preferred feature reduction method for real-time estimator adaptation was minimum redundancy - maximum relevance (mrmr), which was the fastest approach in terms of combined execution and learning time, as well as the second best in accuracy, after sffs. in combination with mrmr, highest accuracies were achieved by k-nearest neighbor (knn) and mlp with negligible difference (50.33% versus 50.54%); however, mrmr+knn is preferable option for real-time estimator adaptation due to considerably lower combined execution and learning time of knn versus mlp.
0026f180-d081-44d1-9ab0-c9c1916866da a comparison of rule matching methods used in aq15 and lers this paper focuses on a performance comparison of two rule matching (classification) methods, used in data mining systems aq15 and lers. all rule sets used in our experiments were induced by the lers (learning from examples using rough sets) system from ten typical input data sets. then these rule sets were truncated using three different criteria: t-weight, u-weight and the strongest rule. the truncation process was performed using six different cut-off values for t-weight, six different cut-off values for u-weight and using the strongest rule option. hence for each of the input rule files thirteen truncated rule sets were created. performance was measured by a classification error rate. the objective of this study was to determine the best overall method of classification and the best truncation option.
0027930e-abb8-464a-a19f-849c67985520 early diagnosis and its benefits in sepsis blood purification treatment sepsis is a progressive medical condition characterized as an uncontrolled inflammatory response, which is the leading cause of death in non-coronary intensive care units in the united states. in sepsis treatment, accurate and timely diagnosis is essential for allowing physicians to design appropriate therapeutic strategies at early stages, when therapies are usually the most effective and the least costly. to make an adequate diagnosis, physicians usually rely on manual inspection of a large amount of complex, high-dimensional longitudinal data. we use our recently published data mining method for extracting patterns from such data and show that these patterns can be used to assist physicians in providing early diagnosis. in conducted experiments, we showed that combination of early diagnosis and blood purification therapy can rescue more patients (52%) than standard approach for blood purification therapy (32%). we also propose a hybrid therapy model that combines strengths of early and standard approaches and further improves the percentage of rescued patients. finally, by correctly classifying 98% of patients who didn't need treatment, msd method provides opportunity to reduce the total cost of treatments.
0027eca5-a55f-4430-b1dd-38e8b387ca43 feature selection for nonlinear models with extreme learning machines in the context of feature selection, there is a trade-off between the number of selected features and the generalisation error. two plots may help to summarise feature selection: the feature selection path and the sparsity-error trade-off curve. the feature selection path shows the best feature subset for each subset size, whereas the sparsity-error trade-off curve shows the corresponding generalisation errors. these graphical tools may help experts to choose suitable feature subsets and extract useful domain knowledge. in order to obtain these tools, extreme learning machines are used here, since they are fast to train and an estimate of their generalisation error can easily be obtained using the press statistics. an algorithm is introduced, which adds an additional layer to standard extreme learning machines in order to optimise the subset of selected features. experimental results illustrate the quality of the presented method.
00288140-2eae-49e8-a941-d3ee4e546ca8 individual differences in dispositional expressiveness: development and validation of the emotional expressivity scale. although emotional expressivity figures prominently in several theories of psychological and physical functioning, limitations of currently available measurement techniques impede precise and economical testing of these theories. the 17-item emotional expressivity scale (ees) was designed as a self-report measure of the extent to which people outwardly display their emotions. reliability studies showed the ees to be an internally consistent and stable individual-difference measure. validational studies established initial convergent and discriminant validities, a moderate relationship between self-rated and other-rated expression, and correspondence between self-report and laboratorymeasured expressiveness using both college student and community populations. the potential for the ees to promote and integrate findings across diverse areas of research is discussed. other peoples' emotional expressions hold a certain fascination for nearly everyone. news agencies always provide images of politicians' expressions on winning and losing elections. reports of court cases routinely mention the defendant's emotional expressions during the reading of the verdict. winning and losing locker-room photographs attempt to capture sports figures' expressive reactions. this level of fascination is probably supported by the belief that something unique and interesting is communicated by emotional expressionssomething that words may at times fail to express. as fritz perls (1969), the founder of gestalt therapy, put it "what we say is mostly either lies or bullshit. but the voice is there, the gesture, the posture, the facial expression" (p. 54). people vary in the extent to which they outwardly exhibit emotions, and these differences have long posed unique and interesting challenges to psychologists. indeed, emotional expressiveness has captured the attention of researchers interested in areas as diverse as nonverbal communication, psychopathology, personality, social psychology, and health psychology. this article reports on the development of a new self-report measure capturing the general disposition to outwardly express emotion. at the outset, it is worth addressing several crucial questions. can emotional expressiveness be defined operationally? is emo
0028da5f-238e-4fba-a292-1387f6aadbed algorithmic compositions based on discovered musical patterns computer music composition is the dream of computer music researchers. in this paper, a top-down approach is investigated to discover the rules of musical composition from given music objects and to create a new music object of which style is similar to the given music objects based on the discovered composition rules. the proposed approach utilizes the data mining techniques in order to discover the styled rules of music composition characterized by music structures, melody styles and motifs. a new music object is generated based on the discovered rules. to measure the effectiveness of the proposed approach in computer music composition, a method similar to the turing test was adopted to test the differences between the machine-generated and human-composed music. experimental results show that it is hard to distinguish between them. the other experiment showed that the style of generated music is similar to that of the given music objects.
002991f6-bd1b-4e29-925f-cf565da0505b the genome sequence of bacillus anthracis ames and comparison to closely related bacteria bacillus anthracis is an endospore-forming bacterium that causes inhalational anthrax 1 . key virulence genes are found on plasmids (extra-chromosomal, circular, double-stranded dna molecules) px01 (ref. 2) and px02 (ref. 3). to identify additional genes that might contribute to virulence, we analysed the complete sequence of the chromosome of b. anthracis ames (about 5.23 megabases). we found several chromosomally encoded proteins that may contribute to pathogenicity-including haemolysins, phospholipases and iron acquisition functions-and identified numerous surface proteins that might be important targets for vaccines and drugs. almost all these putative chromosomal virulence and surface proteins have homologues in bacillus cereus, highlighting the similarity of b. anthracis to near-neighbours that are not associated with anthrax 4 . by performing a comparative genome hybridization of 19 b. cereus and bacillus thuringiensis strains against a b. anthracis dna microarray, we confirmed the general similarity of chromosomal genes among this group of close relatives. however, we found that the gene sequences of pxo1 and px02 were more variable between strains, suggesting plasmid mobility in the group. the complete sequence of b. anthracis is a step towards a better understanding of anthrax pathogenesis.
0029b33e-2a88-44a6-a523-2885558ab0fa application of a hybrid wavelet feature selection method in the design of a self-paced brain interface system background#r##n#recently, successful applications of the discrete wavelet transform have been reported in brain interface (bi) systems with one or two eeg channels. for a multi-channel bi system, however, the high dimensionality of the generated wavelet features space poses a challenging problem.
0029f703-6817-43fc-b0ca-27d1747f0709 classifying pigmented skin lesions with machine learning methods we use a data set of 1619 pigmented skin lesions images from three categories (common nevi, dysplastic nevi, and melanoma) to investigate the performance of four machine learning methods on the problem of classifying lesion images. the methods used were k-nearest neighbors, logistic regression, artificial neural networks, and support vector machines. the data sets were used to train the algo rithms on the following tasks: to distinguish common nevi from dysplastic nevi and melanoma, and to distinguish melanoma from common nevi and dysplastic nevi. receiver operating characteristic curves were used to summarize the perfor mance of the models. three of the methods (logistic regression, artificial neural networks, and sup port vector machines) achieved very good results (area under curve values about 0.96) on the problem of distinguishing melanoma from common and dysplastic nevi, and good results (area under curve values about 0.82) for the problem of dis tinguishing common nevi from dysplastic nevi and melanoma. the performance of k-nearest neighbors models was about 3 percentage points worse than that of the other methods. these results show that for classification problems in the do main of pigmented skin lesions, excellent results can be achieved with several algorithms.
002a15b9-d340-4db1-9bc9-92063b049066 adaptive focused website segment crawler focused web crawler has become indispensable for vertical search engines that provide a search service for specialized datasets. these vertical search engines have to collect specific web pages in the web space, whereas search engines such as google and bing gather web pages from all over the world. the problem in focused crawling research is how to collect specific web pages with minimal computing resources. we previously addressed this problem by proposing a focused crawling strategy, which utilizes an ensemble machine learning classifier to find the group of relevant web pages, referred to as relevant website segment. in this paper, we enhance the proposed crawler as follows: 1) we increase the accuracy of predicting website segments, by preparing two predictors: a predictor learned by features extracted from relevant source website segments and another predictor learned by features from irrelevant ones. the idea is that there may exist different characteristics between these two types of source website segments. 2) we also propose a noisy data elimination method when updating the predictor incrementally during the crawling process. a preliminary experiment shows that our enhanced crawler outperforms a crawler that equips neither of these approaches by around 12%, at most.
002a936b-7c32-4304-ade8-14318b896e5c evolutionary learning of a fuzzy controller for wall-following behavior in mobile robotics the design of fuzzy controllers for the implementation of behaviors in mobile robotics is a complex and highly time-consuming task. the use of machine learning techniques such as evolutionary algorithms or artificial neural networks for the learning of these controllers allows to automate the design process. in this paper, the automated design of a fuzzy controller using genetic algorithms for the implementation of the wall-following behavior in a mobile robot is described. the algorithm is based on the iterative rule learning approach, and is characterized by three main points. first, learning has no restrictions neither in the number of membership functions, nor in their values. in the second place, the training set is composed of a set of examples uniformly distributed along the universe of discourse of the variables. this warrantees that the quality of the learned behavior does not depend on the environment, and also that the robot will be capable to face different situations. finally, the trade off between the number of rules and the quality/accuracy of the controller can be adjusted selecting the value of a parameter. once the knowledge base has been learned, a process for its reduction and tuning is applied, increasing the cooperation between rules and reducing its number.
002af2f5-6264-43f2-8d4c-c55cff6a2810 the state of machine learning methodology in software fault prediction the aim of this paper is to investigate the quality of methodology in software fault prediction studies using machine learning. over two hundred studies of fault prediction have been published in the last 10 years. there is evidence to suggest that the quality of methodology used in some of these studies does not allow us to have confidence in the predictions reported by them. we evaluate the machine learning methodology used in 21 fault prediction studies. all of these studies use nasa data sets. we score each study from 1 to 10 in terms of the quality of their machine learning methodology (e.g. whether or not studies report randomising their cross validation folds). only 10 out of the 21 studies scored 5 or more out of 10. furthermore 1 study scored only 1 out of 10. when we plot these scores over time there is no evidence that the quality of machine learning methodology is better in recent studies. our results suggest that there remains much to be done by both researchers and reviewers to improve the quality of machine learning methodology used in software fault prediction. we conclude that the results reported in some studies need to be treated with caution.
002beeb2-af12-454a-9fda-df274a2895a8 a multi-tier architecture for high-performance data mining 
002bf0de-7b70-4c2e-8b56-476b85a8d21b mining temporal medical data using adaptive fuzzy cognitive maps in this paper, we present our research on mining temporal medical data. in particular, we investigate the possibility of applying fuzzy cognitive maps (fcms) to discover temporal dependencies between medical concepts. we consider two types of simple concepts, i.e., medical interventions (such as prescribing drugs) and health effects expressed by changes of patients' conditions. the long-term goal of the research is the application of the discovered fcm to medical decision support and planning the therapy of patients. finally, we present the results of experiments performed on temporal diabetes data.
002d0100-bbfd-48ca-8900-b2eeb21a6281 design and development of neural bayesian approach for unpredictable stock exchange databases predicting stock market has been a well researched topic in the field of financial engineering. however, most methods suffer from serious drawback due to handling uncertain and missing continuous time-series data. we divide our study in two modules: data cleaning and decision making, first module includes preprocessing of stock market time series data mining which presents the extended form of extraction of both graphical structure and conditional probabilities of a bayesian belief networks (bbn) from a possible incomplete stock market time series databases. in second decision making module, we introduce linguistic rules-tree (lr-tree) which is the combination of fuzzy logics and decision tree but lr-tree may not be the best generalization due to over-fitting. consequently neuro-pruning method has been introduced for post pruning of lr-tree. in neuro-pruning, instead of absolutely removing nodes, we employ a back-propagation neural network to give weights to nodes according to their significance. after data cleaning and decision making modules we hope that, our proposed approaches will be able to handle attributes with differing costs, improving computational efficiency, outperforms error-based pruning, handle uncertain and missing continuous time-series data.
002e02fe-70d2-42d9-8c25-61b05ff1454b cell segmentation and classification by hierarchical supervised shape ranking while pathologists can readily elucidate disease-relevant information from tissue images, automated algorithms may fail to capture the intricate details of complex biological specimens. as histology patterns vary depending on different tissue types, it is typically necessary to adapt and optimize segmentation algorithms to specific applications. to address this, we present a supervised machine learning method we call support vector shape segmentation (svss) to enhance and improve more general segmentation methods by utilizing a cell shape ranking function. first, we pose shape segmentation as an optimization problem that maximizes shape similarity with respect to the specific shape classes. secondly, we propose a computationally efficient algorithm to solve the multi-scale segmentation problem in a minimum number of steps. the main advantage of the approach is that it naturally induces a ranking measure given the set of shape exemplars. we demonstrate large-scale quantitative and qualitative results on epithelial cells in a range of tissue types.
002e6763-82cf-437a-a7d0-0b04e8b40606 on the relationship between feature selection and classification accuracy dimensionality reduction and feature subset selection are two techniques for reducing the attribute space of a feature set, which is an important component of both supervised and unsupervised classification or regression problems. while in feature subset selection a subset of the original attributes is extracted, dimensionality reduction in general produces linear combinations of the original attribute set. in this paper we investigate the relationship between several attribute space reduction techniques and the resulting classification accuracy for two very different application areas. on the one hand, we consider e-mail filtering, where the feature space contains various properties of e-mail messages, and on the other hand, we consider drug discovery problems, where quantitative representations of molecular structures are encoded in terms of information-preserving descriptor values. subsets of the original attributes constructed by filter and wrapper techniques as well as subsets of linear combinations of the original attributes constructed by three different variants of the principle component analysis (pca) are compared in terms of the classification performance achieved with various machine learning algorithms as well as in terms of runtime performance. we successively reduce the size of the attribute sets and investigate the changes in the classification results. moreover, we explore the relationship between the variance captured in the linear combinations within pca and the resulting classification accuracy. the results show that the classification accuracy based on pca is highly sensitive to the type of data and that the variance captured the principal components is not necessarily a vital indicator for the classification performance.
002ea3df-864d-47d9-b2a2-4371e50ab6ea a multi-source integration framework for user occupation inference in social media systems with the rapid development of social media applications, lots of users are connected with friends online, and their daily life and opinions are recorded. social media provides us an unprecedented way to collect and analyze billions of users' information. proper user attribute identification or profile inference becomes increasingly attractive and feasible. however, the flourishing social records also pose great challenge in effective feature selection and integration for user profile inference, which is mainly caused by the text diversity and complex community structures. in this paper, we propose a comprehensive framework to infer the user occupation from his/her social activities recorded in the micro-blog system, which is a multi-source integration framework that combines both content and network information. we first identify some beneficial content features, and propose a machine learning classification model, named content model. we proceed to exploit the social network information, which tailors a community discovery based latent dimension solution to extract community-based feature, and utilizes the neighbor predictions for inference updating. extensive empirical studies are conducted on a large real-life micro-blog dataset. the experimental results demonstrate the superiority of our integrated model for the occupation inference task, verify the effect of homophily in user interaction records, and reveal different effects of heterogeneous interactive networks.
002f121d-37a0-4c78-823c-3d97bbef4eac mining system logs to learn error predictors: a case study of a telemetry system predicting system failures can be of great benefit to managers that get a better command over system performance. data that systems generate in the form of logs is a valuable source of information to predict system reliability. as such, there is an increasing demand of tools to mine logs and provide accurate predictions. however, interpreting information in logs poses some challenges. this study discusses how to effectively mining sequences of logs and provide correct predictions. the approach integrates different machine learning techniques to control for data brittleness, provide accuracy of model selection and validation, and increase robustness of classification results. we apply the proposed approach to log sequences of 25 different applications of a software system for telemetry and performance of cars. on this system, we discuss the ability of three well-known support vector machines - multilayer perceptron, radial basis function and linear kernels - to fit and predict defective log sequences. our results show that a good analysis strategy provides stable, accurate predictions. such strategy must at least require high fitting ability of models used for prediction. we demonstrate that such models give excellent predictions both on individual applications - e.g., 1 % false positive rate, 94 % true positive rate, and 95 % precision - and across system applications - on average, 9 % false positive rate, 78 % true positive rate, and 95 % precision. we also show that these results are similarly achieved for different degree of sequence defectiveness. to show how good are our results, we compare them with recent studies in system log analysis. we finally provide some recommendations that we draw reflecting on our study.
002f2cc0-f3d7-43c9-a545-cbb3bb3ef913 a proposed data mining methodology and its application to industrial procedures data mining is the process of discovering correlations, patterns, trends or relationships by searching through a large amount of data stored in repositories, corporate databases, and data warehouses. industrial procedures with the help of engineers, managers, and other specialists, comprise a broad field and have many tools and techniques in their problem-solving arsenal. the purpose of this study is to improve the effectiveness of these solutions through the application of data mining. to achieve this objective, an adaptation of the engineering design process is used to develop a methodology, specifically designed for industrial procedures operations. this paper concludes by describing some of the advantages and disadvantages of the application of data mining techniques and tools to industrial procedures; it mentions some possible problems or issues in its implementation; and finally, it provides recommendations for future research in the application of data mining to facilitate decisions relevant to industrial procedures.
002fd08d-f867-43b1-ac67-776a4fe6b590 plasmodb: exploring genomics and post-genomics data of the malaria parasite, plasmodium falciparum the recent completion of the genome sequence of plasmodium falciparum 3d7 provides the foundation for genome-wide analysis of the parasite. in addition to dna and gene sequence data, postgenomic methods including microarray-based transcript profiling and high-throughput proteomics are now accessible to plasmodium researchers. the plasmodium genome database ( ) was developed to provide rapid and convenient access to the terabytes of genomic-scale data now being generated around the world. all data are available in a relational framework, permitting convenient downloading, browsing, and analysis. combinatorial use of data analysis tools enables powerful data mining queries, such as combining gene and protein expression data to monitor changes through various life-cycle stages. functional predictions can be used to explore potential targets for antimalarial drug development. this report outlines the use of plasmodb to examine redox-active functions in plasmodium.
003016df-f5c4-4ee8-a49e-81bdb18202f4 a machine learning approach to fall detection algorithm using wearable sensor falls are the primary cause of accidents for the elderly in living environment. falls frequently cause fatal and non-fatal injuries that are associated with a large amount of medical costs. reduction hazards in living environment and doing exercise for training balance and muscle are the common strategies for fall prevention. but falls cannot be avoided completely; fall detection provides the alarm in time that can decrease the injuries or death caused by no rescue. we propose machine learning-based fall detection algorithm using multi-svm with linear, quadratic or polynomial kernel function, and k-nn classifier. eight kinds of falling postures and seven types of daily activities arranged in the experiment are used to explore the performance of the machine learning-based fall detection algorithm. the emulated falls were performed on a soft mat by ten healthy young subjects wearing protectors. the k-nearest neighbor method with 0.1 second window size has the highest accuracy, which is 96.26%. the results show that the proposed machine learning fall detection algorithm can fulfill the requirements of adaptability and flexibility for the individual differences.
00303f11-b084-4c2a-877e-dfc43144e8b1 on evaluation and training-set construction for duplicate detection a variety of experimental methodologies have been used to evaluate the accuracy of duplicate-detection systems. we advocate presenting precision-recall curves as the most informative evaluation methodology. we also discuss a number of issues that arise when evaluating and assembling training data for adaptive systems that use machine learning to tune themselves to specific applications. we consider several different application scenarios and experimentally examine the effectiveness of alternative methods of collecting training data under each scenario. we propose two new approaches to collecting training data called static-active learning and weaklylabeled non-duplicates, and present experimental results on their effectiveness.
0030b06b-12d6-4680-9acf-36c8ef1decca tri-training and data editing based semi-supervised clustering algorithm in this paper, a algorithm named de-tri-training semi-supervised k-means is proposed, which could get a seeds set of larger scale and less noise. in detail, prior to using the seeds set to initialize cluster centroids, the training process of a semi-supervised classification approach named tri-training is used to label unlabeled data and add them into the initial seeds set to enlarge the scale. meanwhile, to improve the quality of the enlarged seeds set, a nearest neighbor rule based data editing technique named depuration is introduced into tri-training process to eliminate and correct the mislabeled noise data in the enlarged seeds. experimental results show that the novel semi-supervised clustering algorithm could effectively improve the cluster centroids initialization and enhance clustering performance.
00324064-7fbc-41bf-8c3c-c95a03afdb39 unifying dependent clustering and disparate clustering for non-homogeneous data modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities. we present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints. unlike prior work that views constraints as boolean criteria, we present a formulation that allows constraints to be satisfied or violated in a smooth manner. this enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function. we present results on both synthetic data as well as several real-world datasets.
003274b8-81ad-44da-8ab9-78df35d5fb76 discrimination between alzheimer's disease and mild cognitive impairment using som and pso-svm in this study, an mri-based classification framework was proposed to distinguish the patients with ad and mci from normal participants by using multiple features and different classifiers. first, we extracted features (volume and shape) from mri data by using a series of image processing steps. subsequently, we applied principal component analysis (pca) to convert a set of features of possibly correlated variables into a smaller set of values of linearly uncorrelated variables, decreasing the dimensions of feature space. finally, we developed a novel data mining framework in combination with support vector machine (svm) and particle swarm optimization (pso) for the ad/mci classification. in order to compare the hybrid method with traditional classifier, two kinds of classifiers, that is, svm and a self-organizing map (som), were trained for patient classification. with the proposed framework, the classification accuracy is improved up to 82.35% and 77.78% in patients with ad and mci. the result achieved up to 94.12% and 88.89% in ad and mci by combining the volumetric features and shape features and using pca. the present results suggest that novel multivariate methods of pattern matching reach a clinically relevant accuracy for the a priori prediction of the progression from mci to ad.
0032bae2-b6b3-41bd-acc2-258174aac4b2 early warning system for approaching emergency vehicles the present disclosure is a method for alerting road users to the presence of an emergency vehicle. the method for alerting road user to the presence of an emergency vehicle may include receiving a short range signal, the signal including information regarding an emergency vehicle, decoding the signal, notifying a user, and transmitting the information regarding an emergency vehicle via a short range wireless signal. each vehicle may include a short range wireless signal transceiver which could create a daisy chain distribution mechanism to distribute the information regarding an emergency vehicle to a first vehicle than to a second vehicle quickly and efficiently.
00332ca1-523d-4028-ae2e-b3d149d12714 free-energy minimization and the dark-room problem recent years have seen the emergence of an important new fundamental theory of brain function. this theory brings information-theoretic, bayesian, neuroscientific, and machine learning approaches into a single framework whose overarching principle is the minimization of surprise (or, equivalently, the maximization of expectation). the most comprehensive such treatment is the free-energy minimization formulation due to karl friston (see e.g., friston and stephan, 2007; friston, 2010a,b  see also fiorillo, 2010; thornton, 2010). a recurrent puzzle raised by critics of these models is that biological systems do not seem to avoid surprises. we do not simply seek a dark, unchanging chamber, and stay there. this is the dark-room problem. here, we describe the problem and further unpack the issues to which it speaks. using the same format as the prolog of eddingtons space, time, and gravitation (eddington, 1920) we present our discussion as a conversation between: an information theorist (thornton), a physicist (friston), and a philosopher (clark).
00338ff8-cc94-425f-8d64-cf1ef73d9c41 an empirical study on data mining applications 
0033cafb-d5ee-4d5c-b27b-ef8572141552 malicioius software detection using multiple sequence alignment and data mining malware is currently a major threat to information and computer security, with the volume and growing diversity of its variants causing major problems to traditional security defenses. software patches and upgrades to anti-viral packages are typically released only after the malware's key characteristics have been identified through infection, by which time it may be too late to protect systems. multiple sequence analysis is widely used in bioinformatics for revealing the genetic diversity of organisms and annotating gene functions through the identification of common genetic regions. this paper adopts a new approach to the problem of malware recognition, which is to use multiple sequence alignment techniques from bioinformatics to align variable length computer viral and worm code so that core, invariant regions of the code occupy fixed positions in the alignment patterns. data mining (anns, symbolic rule extraction) can then be used to learn the critical features that help to determine into which class the aligned patterns fall. experimental results demonstrate the feasibility of our novel approach for identifying malware code through multiple sequence alignment followed by analysis by anns and symbolic rule extraction methods.
0033d636-d073-4b84-b666-a963225d72d8 a conceptual frame with two neural mechanisms to model selective visual attention processes an important problem in artificial intelligence (ai) is to find calculation procedures to save the semantic gap between the analytic formulations of the neuronal models and the concepts of the natural language used to describe the cognitive processes. in this work we explore a way of saving this gap for the case of the attentional processes, consisting in (1) proposing in first place a conceptual model of the attention double bottom-up/top-down organization, (2) proposing afterwards a neurophysiological model of the cortical and sub-cortical involved structures, (3) establishing the correspondences between the entities of (1) and (2), (4) operationalizing the model by using biologically inspired calculation mechanisms (algorithmic lateral inhibition and accumulative computation) formulated at symbolic level, and, (5) assessing the validity of the proposal by accommodating the works of the research team on diverse aspects of attention associated to visual surveillance tasks. the results obtained support in a reasonable way the validity of the proposal and enable its application in surveillance tasks different from the ones considered in this work. in particular, this is the case when linking the geometric descriptions of a scene with the corresponding activity level.
0033f92b-937f-4f18-8e5c-74ecec9ba420 formal validation of schema clustering for large information systems 
0033fc0c-a90d-4add-9262-d3af541772cf a two-level modelling approach to acquire functional design knowledge in mechanical engineering systems modelling as a means of knowledge acquisition has been proposed by some workers in artificial intelligence. in this paper the authors describe an approach using two-level knowledge modelling that employs the concepts of a functional model (extended febs model) and an object model to capture domain-specific functional design knowledge. the functional model serves as a basis for communication between domain experts and a knowledge engineer. the object model is used to bridge the gap between the functional model and an executable knowledge base. the approach is viewed as an important method for building a function-laden knowledge base that is useful for developing a knowledge-based functional design expert system. the proposed knowledge acquisition method is applied to an automatic assembly system for manufacturing electronic connectors to illustrate the general idea.
003440c0-161f-43b2-a745-5519d42903bb introduction of data mining and an analysis of data mining techniques 
0034ce3d-c608-4abf-9c83-22019850b358 data mining in web applications 
0034eb73-f50c-41e0-a957-3486ebb98a5c traditional chinese medicine treatments for upper respiratory tract infections/common colds in taiwan introduction#r##n#traditional chinese medicine (tcm) has been used to treat upper respiratory tract infections/common colds (urtis) in asian countries for over 2000 years. however, chinese medicine doctors (cmds) follow the traditional treatment rules to select or administer these diverse chinese medicine formulae. the main purpose of our study was to explore data on the frequency of medication and medication habits by cmds for the treatment of urtis with chinese herbs and chinese medicine formulae.#r##n#methods#r##n#the tcm treatments for patients consulting with an urtis were analyzed from the national health insurance research database using the appropriate codes from the international classification of diseases, ninth revision, clinical modification diagnoses for taiwan in 2009. a data mining and association rules, were used to analyze co-prescriptions of tcm for patients with urtis.#r##n#results#r##n#for 472,005 patients who sought the treatment of urtis, a total of 46,805 patients with urtis received tcm treatments, of these 29,052 patients sought both tcm and western medication treatments. of the urtis patients who had received a tcm treatment, 79% presented with an acute common cold, 9% had influenza, and 9% had acute upper respiratory infections. furthermore, 53.89% of the sample were aged between 20 and 49 years, and 62.84% were women, 3.56% of the patients used yin-qiao-san and 2.76% used jie-geng. yin-qiao-san and ma-xing-gan-shi-tang were the most commonly combinations of prescriptions for patients with urtis.#r##n#conclusions#r##n#the patients experiencing urtis were more likely to request tcm treatment if their symptoms were mild and they were women. the chinese medicine doctors treating urtis generally followed tcm theory. a coding system for tcm diagnostic classifications could improve evaluations of tcm treatments.
0035a5f1-9821-4708-9299-c7013f56b80c new programming languages for artificial intelligence research 
00360463-9a10-4ccb-a1e0-86bf27770daa open collaboration on hybrid video quality models - vqeg joint effort group hybrid several factors limit the advances on automatizing video quality measurement. modelling the human visual system requires multi- and interdisciplinary efforts. a joint effort may bridge the large gap between the knowledge required in conducting a psychophysical experiment on isolated visual stimuli to engineering a universal model for video quality estimation under real-time constraints. the verification and validation requires input reaching from professional content production to innovative machine learning algorithms. our paper aims at highlighting the complex interactions and the multitude of open questions as well as industrial requirements that led to the creation of the joint effort group in the video quality experts group. the paper will zoom in on the first activity, the creation of a hybrid video quality model.
00360aaa-843b-4a0a-adde-5ed7a54da947 soft computing techniques for reduced order modelling: review and application abstractas the mathematical procedure of system modelling often leads to a comprehensive description, which causes significant difficulty in both analysis and control synthesis, it is necessary to find lower order models, which maintain the dominant characteristics of the original system. in this paper, different soft computing (named as artificial intelligence (ai)) techniques are presented, applied, and analysed for model order reduction (mor) of multi time scale systems with the objective of substructure preservation. in addition to that, we investigate the firefly optimization technique for mor with substructure preservation. the analysis is concerned with the optimization approach and quality of method performance.
00361182-a31e-4dc7-8e78-a4c27f84a96e analyzing the balancing of error rates for multi-group classification this paper reports the relative performance of an experimental comparison of some well-known classification techniques such as classical statistical, artificial intelligence, mathematical programming (mp), and hybrid approaches. in particular, we examine the four-group, three-variable problem and the associated error rates for the four groups when each of the models is applied to various sets of simulated data. the data had varying characteristics such as multicollinearity, nonlinearity, sample proportions, etc. we concentrate on individual error rates for the four groups, i.e., we count the number of group 1 values classified into group 2, group 3, and group 4 and vice versa. the results indicate that in general not only are mp, k-nn, and hybrid approaches relatively better at overall classification but they also provide a much better balance between error rates for the top customer groups. the results also indicate that the mp and hybrid approaches provide relatively higher and stable classification accuracy under all the data characteristics.
003624cd-6b41-49ff-a367-0324d4cceaaf semantic annotation and classification in practice the web's evolution into a semantic web and the continuous increase in the amount of data published as linked data open up new opportunities for annotation and categorization systems to reuse these data as semantic knowledge bases. accordingly, information extraction systems use linked data to exploit semantic knowledge bases, which can be interconnected and structured to increase the precision and recall of annotation and categorization mechanisms. tellmefirst classifies and enriches textual documents written in english and italian. although various works present solutions for text annotation and classification, this article describes and studies the use case of a telecommunications operator that has adopted tellmefirst to add value to two services available to its users: friendtv and society.
00376a06-6aa1-423a-af69-50d40c885b9e immunoglobulin genes and their transcriptional control in teleosts. immunoglobulin (ig), which exists only in jawed vertebrates, is one of the most important molecules in adaptive immunity. in the last two decades, many teleost ig genes have been identified by in silico data mining from the enormous gene and est databases of many fish species. in this review, the organization of ig gene segments, the expressed ig isotypes and their transcriptional controls are discussed. the ig heavy chain (igh) locus in teleosts encodes the variable (v), the diversity (d), the joining (j) segments and three different isotypic constant (c) regions including c, c, and c/ genes, and is organized as a translocon type like the igh loci of higher vertebrates. in contrast, the ig light (l) chain locus is arranged in a multicluster or repeating set of vl, jl, and cl segments. the igl chains have four isotypes; two  l1/g and l3/f),  (l2) and . the transcription of igh genes in teleosts is regulated by a vh promoter and the e3 enhancer, which both function in a b cell-specific manner. the location of the igh locus, structure and transcriptional function of the e3 enhancer are important to our understanding of the evolutional changes that have occurred in the igh gene locus.
0037a442-9b90-4469-a505-ed070b22de4f automatic motion feature extraction with application to quantitative assessment of facial paralysis this paper presents a robust, objective, automated and quantitative assessment system for facial paralysis using artificial intelligence analysis of biomedical video data. facial feature localization and prescribed facial movements detection are discussed. optical flow is used to obtain the motion features in the relevant facial regions. radial basis function (rbf) neural network is applied to provide quantitative evaluation of facial paralysis based on the house-brackmann scale. the results from 197 videos of 87 subjects are encouraging with a mean squared error (mse) of 0.013 (training) and 0.0169 (testing).
0037b0b0-701c-4c46-81b7-ef4e04ee2771 genetic-algorithm-based learning this chapter describes a subarea of machine learning that is actively exploring the use of genetic algorithms as the key element in the design of robust learning strategies. after characterizing the kinds of learning problems motivating this approach, a brief overview of genetic algorithms is presented. three major approaches to using genetic algorithms for machine learning are described, and an example of their use in learning entire task programs is given. finally, an assessment of the strengths and weaknesses of this approach to machine learning is provided.
0037f0f2-a0b2-47cd-91d3-fc550236599f how the machine thinks: understanding opacity in machine learning algorithms this article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. these mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. in this article, i draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. the analysis in this article gets inside the algorithms themselves. i cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. i argue that recognizing the distinct forms of opacity that may be coming into play in a given application is key to determining which of a variety of technical and non-technical solutions could help to prevent harm. 
003875bb-899e-4da0-bae0-880d5a69273c intensity and range based features for object detection in mobile mapping data mobile mapping is used for asset management, change detection, surveying and dimensional analysis. there is a great desire to automate these processes given the very large amounts of data, especially when 3-d point cloud data is combined with co-registered imagery - termed 3-d images. one approach requires low-level feature extraction from the images and point cloud data followed by pattern recognition and machine learning techniques to recognise the various high level features (or objects) in the images. this paper covers low-level feature analysis and investigates a number of different feature extraction methods for their usefulness. the features of interest include those based on the bag of words concept in which many low-level features are used e.g. histograms of gradients, as well as those describing the saliency (how unusual a region of the image is). these mainly image based features have been adapted to deal with 3-d images. the performance of the various features are discussed for typical mobile mapping scenarios and recommendations made as to the best features to use.
003935de-59f3-45a8-8be1-8a3ba3c9288e distributed control on a model of mars rover spirit the achievement in vlsi technology has changed the old style single cpu integration into new concept of distributed control on multiple controllers which emphasis on the data communication between controllers. the main cpu sends command(s) and retrieves data/status from the slave controller(s) and the slave controller(s) act accordingly, as the result, two major impacts had occurred, the hardware design is much simpler, it can be designed into a much simpler basic unit and it can be duplicated throughout the system for any equivalent job. even though the jobs, may vary based on various functionalities, only minor additional hardware had been added to cover all the extra features. the major improvement is in the software development cycle, it has been cut drastically from months/year to just only week(s). this paper is a good example of an effort to implement the distributed control concept into a model of scaled-down mars rover spirit which in turn simplifies the overall design (both hardware and software) and allows the designer to easily add more software features such as: machine learning, artificial intelligence, etc. to the robot in the future.
0039c491-8bb4-4d7c-8022-fe354e5f401a using meta-mining to support data mining workflow planning and optimization knowledge discovery in databases is a complex process that involves many different data processing and learning operators. today's knowledge discovery support systems can contain several hundred operators. a major challenge is to assist the user in designing workflows which are not only valid but also - ideally - optimize some performance measure associated with the user goal. in this paper we present such a system. the system relies on a meta-mining module which analyses past data mining experiments and extracts meta-mining models which associate dataset characteristics with workflow descriptors in view of workflow performance optimization. the meta-mining model is used within a data mining workflow planner, to guide the planner during the workflow planning. we learn the meta-mining models using a similarity learning approach, and extract the workflow descriptors by mining the workflows for generalized relational patterns accounting also for domain knowledge provided by a data mining ontology. we evaluate the quality of the data mining workflows that the system produces on a collection of real world datasets coming from biology and show that it produces workflows that are significantly better than alternative methods that can only do workflow selection and not planning.
003a3f4a-c279-4547-b587-bbe7d7eb818e simple multiple noisy label utilization strategies with the outsourcing of small tasks becoming easier, it is possible to obtain non-expert/imperfect labels at low cost. with low-cost imperfect labeling, it is straightforward to collect multiple labels for the same data items. this paper addresses the strategies of utilizing these multiple labels for improving the performance of supervised learning, based on two basic ideas: majority voting and pair wise solutions. we show several interesting results based on our experiments. the soft majority voting strategies can reduce the bias and roughness, and improve the performance of the directed hard majority voting strategy. pair wise strategies can completely avoid the bias by having both sides (potential correct and incorrect/noisy information) considered (for binary classification). they have very good performance whenever there are a few or many labels available. however, it could also keep the noise. the improved variation that reduces the impact of the noisy information is recommended. all five strategies investigated are labeling quality agnostic strategies, and can be applied to real world applications directly. the experimental results show some of them perform better than or at least very close to the gnostic strategies.
003a8b2a-1f90-4f2a-8aa8-c3c56480756c design and analysis of intelligent system of electr o-energetic blocks motion management 1. abstract main goal of the present paper is to present assume d methodology of decision making processes virtuali zation on the basis of intelligent system of electro-energy power units motion management modeling (iseepumm). the paper is a continuation of the authors series of publications concerning small sc ale energy power units optimization, virtualization and management. the authors also conduct functional-object presentation of indi vidual systems modules, where decision making virtualization with reference to demand-supply changes of electricity markets is imp lied and activated on the basis of artificial intel ligence selected generators. 2. keywords: virtual decision making processes modeling, artificial intelligence, small scale electro- energy units motion management
003af1ce-0f7d-41f8-856d-999da57243d5 adaptive tools for the elderly: new devices to cope with age-induced cognitive disabilities we look at the issues and methodologies needed to develop, deploy, and evaluate situation-aware mobile computing devices that adapt to the needs of elder users based on observed or predicted user behavior and needs. this paper discusses how pervasive computing can help the aging population live independently for as long as possible. we believe that successfully applying technology to this problem will require careful studies of how the target population actually lives and what their actual needs are. we propose a combination of traditional laboratory studies and surveys, as well as the use of instrumented spaces and personal monitoring devices to measure how people behave, normally and while using proposed assistive devices. a key requirement is the development of ways to simultaneously monitor signals from the body, activities, and social interactions to provide a more complete view of individuals and their lives. some of the core research issues are machine learning to design devices learn from and adapt to user behavior, user-computer interaction to build devices and systems that support users in their tasks, mobile computing to support user and device mobility, mobility and data management to represent, access, update, and protect information, sensing devices that monitor human activity and finally rapid prototyping of services in a sensor-rich environment, in a scalable and secure manner.
003b516f-de8f-4b9b-9faa-f983f4b2ccc3 structural identifiability in low-rank matrix factorization in many signal processing and data mining applications, we need to approximate a given matrix y with a low-rank product yax. both matrices a and x are to be determined, but we assume that from the specifics of the application we have an important piece of a-priori knowledge: a must have zeros at certain positions.#r##n##r##n#in general, different ax factorizations approximate a given y equally well, so a fundamental question is whether the known zero pattern of a contributes to the uniqueness of the factorization. using the notion of structural rank, we present a combinatorial characterization of uniqueness up to diagonal scaling (subject to a mild non-degeneracy condition on the factors), called structural identifiability of the model.#r##n##r##n#next, we define an optimization problem that arises in the need for efficient experimental design. in this context, y contains sensor measurements over several time samples, x contains source signals over time samples and a contains the source-sensor mixing coefficients. our task is to monitor the signal sources with the cheapest subset of sensors, while maintaining structural identifiability. firstly, we show that this problem is np-hard. secondly, we present a mixed integer linear program for its exact solution together with two practical incremental approaches. we also propose a greedy approximation algorithm. finally, we perform computational experiments on simulated problem instances of various sizes.
003ba500-a15f-4d81-85c8-3f84ddbf279d a survey on heart disease diagnosis and prediction using naive bayes in data mining data mining is non trivial extraction of implicit data, previously not known, and imaginably useful information from data. data mining is an essential process where intelligent methods are applied in order to extract data patterns. using data mining we can evaluate patterns which we can use in future to take intelligent decisions and we can present the knowledge we extracted in better way. data mining refers to using a variety of techniques to identify information or decision making knowledge in the database and extracting these in a way that they can put to use in areas such as decision making, predictions, for valuable forecasting and computation. the healthcare industry collects huge amounts of healthcare data which, unfortunately, are not mined to discover hidden information, to take decisions effectively, to discover the relations that connect parameters in a database is the subject of data mining. as large amount of data is generated in medical organisations (hospitals, medical centers) but as this data is not properly used. there is a wealth of hidden information present in the datasets. this unused data can be converted into useful data. for this purpose we can use different data mining techniques. in this study, we are applying naive bayes data mining classifier technique which produces an optimal prediction model using minimum training set. data mining is the analysis step of the knowledge discovery in databases process (kdd). data mining involves use of techniques to find underlying structures and relationships in a large database. using medical profile such as age, sex, blood pressure and blood sugar we can easily predict the likelihood of patients getting heart disease. in this paper we have evaluated the performance of new classification approach that uses the experienced doctors knowledge to assign the weight to each attribute. more weight is assigned to the attribute having high impact on disease prediction.
003bc414-f9ca-4764-8219-62edeedf52f0 clustering remote rdf data using sparql update queries the emergence of large and distributed rdf data in the linked open data cloud calls for approaches to extract useful knowledge using machine learning techniques such as clustering. however, the massive size and remote nature of rdf data hinder traditional approaches that gather the datasets onto a centralized location for analysis. in this work, we show how to implement two representative clustering algorithms using update queries against the sparql endpoint of the rdf store. we compare the time complexity and the communication complexity of our algorithms with of those that require direct centralized access to the data and hence have to retrieve the entire rdf dataset from the remote location. we conduct experiments on a real social network dataset and report our preliminary findings.
003c26d8-8dde-4d11-a45c-6e6bc9c50ccd irris - an image registration, recognition, and inspection system this paper briefly describes irris-100 tm , a machine vision system shown in operation at four recent exhibitions sponsored by the society for manufacturing engineers. a gray level vision system combining artificial intelligence methods with statistical and structural pattern recognition techniques, irris-100 tm can rapidly determine the location and orientation of touching and overlapping objects that are randomly positioned within the camera's field of view, without special lighting, and without having to find connected components in the image. with the gray level image registered in a standard orientation in memory, customized algorithms can examine specific details in the image. based on a motorola 68000 processor, the system communicates location and orientation coordinates and other information for pick and place, de-palletizing, assembly, recognition, and inspection applications, over two rs232 ports and one 16 bit parallel port.
003ca9cd-f786-4ca0-a591-1e8a5c168e7c ideas on authenticating humanness in collaborative systems using ai-hard problems in perception and cognition collaborative applications including email, chat, file-sharing networks, blogs, and gaming are under constant threat of automated programs that are gaining access to, attacking, degrading, or otherwise disrupting the intended communications and interactions. thus, an important issue in collaborative systems security is how to verify that a user is a human, and not a computer attempting to access the system for malicious purposes. we propose and discuss several ai-hard examples from perception and cognition that may be useful for distinguishing between human-level intelligence and artificial intelligence.
003d1647-d9fb-466f-8328-866c897ae121 event-driven contrastive divergence: neural sampling foundations in a recent frontiers in neuroscience paper (neftci et al., 2014) we contributed an on-line learning rule, driven by spike-events in an integrate and fire (if) neural network, that emulates the learning performance of contrastive divergence (cd) in an equivalent restricted boltzmann machine (rbm) amenable to real-time implementation in spike-based neuromorphic systems. the event-driven cd framework assumes the foundations of neural sampling (buesing et al., 2011; maass, 2014) in mapping spike rates of a deterministic if network onto probabilities of a corresponding stochastic neural network. in neftci et al. (2014), we used a particular form of neural sampling previously analyzed in petrovici et al. (2013)1, although this connection was not made sufficiently clear in the published article. the purpose of this letter is to clarify this connection, and to raise the reader's awareness to the existence of various forms of neural sampling. we highlight the differences as well as strong connections across these various forms, and suggest applications of event-driven cd in a more general setting enabled by the broader interpretations of neural sampling.#r##n##r##n#in the bayesian view on neural information processing, the cognitive function of the brain arises from its ability to encode and combine probabilities describing its interactions with an uncertain world (doya et al., 2007). a recent neural sampling hypothesis has shed light on how probabilities may be encoded in neural circuits (fiser et al., 2010; berkes et al., 2011). in the neural sampling hypothesis, spikes are viewed as samples of a target probability distribution. from a modeling perspective, a key advantage of this view is that learning in spiking neural networks becomes more tractable than the alternative one, in which neurons encode probabilities, because one can borrow from well-established algorithms in machine learning (fiser et al., 2010) (see nessler et al., 2013 for a concrete example).#r##n##r##n#merolla et al. (2010) demonstrated a boltzmann machine using if neurons. in this model, spiking neurons integrate poisson-distributed spikes during a fixed time window set by a global rhythmic oscillation. a first-passage time analysis shows that the probability that a neuron spikes in the given time window follows a logistic sigmoid function consistent with a boltzmann distribution. the particular form of rhythmic oscillation ensures that, even when neurons are recurrently coupled, the network produces a sample of a boltzmann distribution for each oscillation cycle. merolla et al. (2010) also suggest an alternative, more biologically plausible forms of learning induced by rhythmic oscillations that resemble the role of theta oscillations across large neuronal ensembles. our event-driven cd rule is compatible with merolla et al.'s sampler because it would simply result in updating weights at every cycle of the rhythmic oscillation.#r##n##r##n#shortly after, buesing et al. (2011) proved that abstract neuron models consistent with the behavior of biological spiking neurons (jolivet et al., 2006) can perform markov chain monte carlo (mcmc) sampling of a boltzmann distribution. their sampler does not require global oscillations, although these could enable the sampling from multiple distributions within the same network (habenschuss et al., 2013). to demonstrate the performance of the sampler, a boltzmann machine was trained off-line using cd. learning in this model was further extended to on-line updates in a precursor of event-driven cd (pedroni et al., 2013).#r##n##r##n#an open question was whether neuron models that describe the biological processes of nerve cells endowed with deterministic action potential generation mechanisms can support stochastic sampling as described with the more abstract spiking forms in buesing et al. (2011). an answer to this question is relevant for understanding how neural sampling can be instantiated in biological neurons, but also for implementing neural samplers on low-power neuromorphic implementations of spiking neurons (indiveri et al., 2011). the stochastic nature of neural sampling suggests studying the behavior of neurons under noisy inputs. the diffusion model commonly referred to as the ornstein-uhlenbeck process (van kampen, 1992) has been the basis of a standard continuous-time stochastic neuron model since the first rigorous analysis of its behavior in capocelli and ricciardi (1971). petrovici et al. (2013) discuss these issues and provide a rigorous link between deterministic neuron models (leaky integrate-and-fire with conductance-based synapses) and stochastic network-level dynamics, as can be observed in vivo. in particular, they identify how the high-conductance state caused by poissonian background bombardment can provide the fast membrane reaction time required for precise sampling. they provide analytical derivations of the activation function at the single-cell level as well as for the synaptic interaction and investigate the convergence behavior of the sampled distribution at the network level.#r##n##r##n#o'connor et al. (2013) employ the siegert approximation of if neurons to compute cd updates. the siegert or diffusion approximation expresses the firing rate of an if neuron, as a function of input firing rates, under the assumption that all inputs are independent and poisson distributed. after learning, the parameters of the learned boltzmann machine are transferred to the equivalent network of if neurons. although the off-line cd learning in o'connor et al. (2013) operated using firing rates rather than spikes, in its basic form, it is functionally equivalent and compatible with event-driven cd under the condition that spike times are uncorrelated.#r##n##r##n#our work implements a biologically-inspired algorithm for the purposes of training boltzmann machines (neftci et al., 2014). we assumed a neuronal model consistent with biology and realizable in a neuromorphic implementation. petrovici et al. (2013) provided a deeper physical and mathematical interpretation of neural sampling. similarly to their approach, we considered the standard leaky if neuron stimulated by non-capacitively summed pre-synaptic inputs obeying poisson statistics.#r##n##r##n#the performance of event-driven cd on the mnist hand-written digit recognition task was robust to spike probabilities that deviate slightly from the boltzmann distribution, even though such distributions violate the assumptions of cd formulated for training rbms. this suggests that event-driven cd provides a general learning framework for biologically-inspired spiking rbms and is consistent with wide range of neural samplers.
003d32d8-4e15-41af-9694-d5eafe64de72 co-processor for evolutionary full decision tree induction in this paper a co-processor for the hardware aided decision tree induction using evolutionary approach (eftip) is proposed. eftip is used for hardware acceleration of the fitness evaluation task since this task is proven in the paper to be the execution time bottleneck. the eftip co-processor can significantly improve the execution time of a novel algorithm for the full decision tree induction using evolutionary approach (efti) when used to accelerate the fitness evaluation task. the comparison of the hw/sw efti implementation with the pure software implementation suggests that the proposed hw/sw architecture offers substantial dt induction time speedups for the selected benchmark datasets from the standard uci machine learning repository database.
003d8979-331f-43a8-83d6-8102dcb7d166 comparative analysis of safety performance indicators based on inductive loop detector data conflicts in traffic stream have been detected by different safety performance indicators. this study aims to empirically investigate the differences between different indicators in detecting rear-end conflicts and assessing the risk in an uninterrupted flow. micro-level data of a 24-hr traffic stream (including 6,657 vehicles) were captured using inductive loop detectors installed on a rural freeway section. different indicators (time headway (h), time to collision (ttc), proportion of stopping distance (psd), deceleration rate to avoid collision (drac) and stopping distance index (sdi)) were used to measure each car following event in a bivalent state (safe/unsafe). unsafe events associated with each indicator were detected and common unsafe events characterized by different indicators were identified. temporal distributions of rear-end collision risks associated with each indicator at 15-min intervals were also compared. finally, the 15-min risk values based on different indicators were categorized and compared across three levels (low, medium and high). data mining and statistical techniques showed that while sdi is the single most conservative indicator, drac and ttc detect a few risky events but very equal ones. in almost all conflicts associated with ttc, headway is still lower than the critical threshold. however, there exist considerable risky events based on headway which are still safe according to ttc. comparison of psd and ttc also declares that almost all conflicts associated with ttc are also risky according to psd.
003d8d5e-de0f-4daa-a8e5-d14d3e10ad61 a review on vision techniques applied to human behaviour analysis for ambient-assisted living human behaviour analysis (hba) is more and more being of interest for computer vision and artificial intelligence researchers. its main application areas, like video surveillance and ambient-assisted living (aal), have been in great demand in recent years. this paper provides a review on hba for aal and ageing in place purposes focusing specially on vision techniques. first, a clearly defined taxonomy is presented in order to classify the reviewed works, which are consequently presented following a bottom-up abstraction and complexity order. at the motion level, pose and gaze estimation as well as basic human movement recognition are covered. next, the mainly used action and activity recognition approaches are presented with examples of recent research works. increasing the degree of semantics and the time interval involved in the hba, finally the behaviour level is reached. furthermore, useful tools and datasets are analysed in order to provide help for initiating projects.
003e0579-5c2c-4ddc-b55e-4cf2c8fe42a1 the future of predictive models in radiation oncology: from extensive data mining to reliable modeling of the results 
003e9240-5767-46c0-89fd-2f9a355494ff cancer informatics by prototype networks in mass spectrometry objective: mass spectrometry has become a standard technique to analyze clinical samples in cancer research. the obtained spectrometric measurements reveal a lot of information of the clinical sample at the peptide and protein level. the spectra are high dimensional and, due to the small number of samples a sparse coverage of the population is very common. in clinical research the calculation and evaluation of classification models is important. for classical statistics this is achieved by hypothesis testing with respect to a chosen level of confidence. in clinical proteomics the application of statistical tests is limited due to the small number of samples and the high dimensionality of the data. typically soft methods from the field of machine learning are used to generate such models. however for these methods no or only few additional information about the safety of the model decision is available. in this contribution the spectral data are processed as functional data and conformal classifier models are generated. the obtained models allow the detection of potential biomarker candidates and provide confidence measures for the classification decision.#r##n##r##n#methods: first, wavelet-based techniques for the efficient processing and encoding of mass spectrometric measurements from clinical samples are presented. a prototype-based classifier is extended by a functional metric and combined with the concept of conformal prediction to classify the clinical proteomic spectra and to evaluate the results.#r##n##r##n#results: clinical proteomic data of a colorectal cancer and a lung cancer study are used to test the performance of the proposed algorithm. the prototype classifiers are evaluated with respect to prediction accuracy and the confidence of the classification decisions. the adapted metric parameters are analyzed and interpreted to find potential biomarker candidates.#r##n##r##n#conclusions: the proposed algorithm can be used to analyze functional data as obtained from clinical mass spectrometry, to find discriminating mass positions and to judge the confidence of the obtained classifications, providing robust and interpretable classification models.
003eee65-1e81-4163-9883-a55c3ee720fd lightlda: big topic models on modest computer clusters when building large-scale machine learning (ml) programs, such as massive topic models or deep neural networks with up to trillions of parameters and training examples, one usually assumes that such massive tasks can only be attempted with industrial-sized clusters with thousands of nodes, which are out of reach for most practitioners and academic researchers. we consider this challenge in the context of topic modeling on web-scale corpora, and show that with a modest cluster of as few as 8 machines, we can train a topic model with 1 million topics and a 1-million-word vocabulary (for a total of 1 trillion parameters), on a document collection with 200 billion tokens --- a scale not yet reported even with thousands of machines. our major contributions include: 1) a new, highly-efficient o(1) metropolis-hastings sampling algorithm, whose running cost is (surprisingly) agnostic of model size, and empirically converges nearly an order of magnitude more quickly than current state-of-the-art gibbs samplers; 2) a model-scheduling scheme to handle the big model challenge, where each worker machine schedules the fetch/use of sub-models as needed, resulting in a frugal use of limited memory capacity and network bandwidth; 3) a differential data-structure for model storage, which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory, while maintaining high inference speed. these contributions are built on top of the petuum open-source distributed ml framework, and we provide experimental evidence showing how this development puts massive data and models within reach on a small cluster, while still enjoying proportional time cost reductions with increasing cluster size.
003f25d0-1813-4f49-ac35-fecb75647b82 convolutional neural network based segmentation machine learning system refers to the one which automati- cally accumulates knowledge about new environments based on experi- ence to recognize complex patterns. this ability to learn from experience, analytical observation, and other means, results in a system that can im- prove its own speed and performance. in this work, convolutional neural network is used for learning how to segment images. convolutional neu- ral networks (cnn) extract features directly from pixel images with minimal preprocessing. it can even able to recognize a pattern which has not been presented before, provided it resembles one of the training patterns. after learning (from ground-truth image), cnn automatically generate a good affinity graph from raw sem images. this affinity graph can be then paired with any standard partitioning algorithm, such as n-cut, connected component to achieve improved segmentation. in this paper, we demonstrate the use of combined approach, where a convo- lutional neural network and connected component algorithm(cc) are used to segment sem images. f-score of this algorithm was found to be 78%.
003f6d5e-c49e-410d-a1ce-aa08c6db796f the effect of business intelligence on management accounting information system in today's business world, we are faced with high volumes of data. new developments in it provide organizations with effective and efficient access and storage of information. in any case, there is a long distance between the mass of data and its use. management accounting information system has changed as a key to success in today's business environment. in the field of management accounting, if the accounting information system is not capable of providing information to business managers timely and quickly, organizations' success will be threatened in the competitive environment. to cope with competitors and growth of long-term strategies, the accounting information system should benefit from business intelligence techniques to provide timely and effective financial information. the important competitive advantage against opponents and business competitors in the market is the most important reason to create intelligent systems. the purpose of business intelligence is to help control the flow and resources of business information within and around the organization. in this study, based on the research objectives, using a meta-analysis, some of the applied criteria and parameters of accounting information systems were examined based on business intelligence features. in addition, a model was proposed based on four categories of relationships and inferences, warning and reporting systems, and tools for effective analysis and decision-making. among the criteria in the literature review are group decision-making, optimization, integration, simulation, traffic reports, prototyping based on the original version, two-way argument process, awareness technology, informing on the content, fuzzificatio, data mining, data storage, real-time analysis process, establishing communication  channels, creating intelligent factors etc. therefore, the necessity to use a business intelligence-based model in management accounting information system is proposed.
004001fa-66fa-45ea-b6bb-f62613b47428 exploring the behaviour of base classifiers in credit scoring ensembles many techniques have been proposed for credit risk assessment, from statistical models to artificial intelligence methods. during the last few years, different approaches to classifier ensembles have successfully been applied to credit scoring problems, demonstrating to be more accurate than single prediction models. however, it is still a question what base classifiers should be employed in each ensemble in order to achieve the highest performance. accordingly, the present paper evaluates the performance of seven individual prediction techniques when used as members of five different ensemble methods. the ultimate aim of this study is to suggest appropriate classifiers for each ensemble approach in the context of credit scoring. the experimental results and statistical tests show that the c4.5 decision tree constitutes the best solution for most ensemble methods, closely followed by the multilayer perceptron neural network and logistic regression, whereas the nearest neighbour and the naive bayes classifiers appear to be significantly the worst.
004072be-441f-4862-bf92-a4a32beb4887 bexa: set covering vs. neural network knowledge acquisition-a comparative review machine learning approaches to knowledge acquisition usually employ a symbolic method based on search, heuristically guided through the concept space to avoid the combinatorial explosion of possible concept descriptions to be examined. neural networks, on the other hand usually employ gradient based minimization of a cost function to acquire classificational knowledge. this paper presents a new symbolic set covering algorithm for rule induction, reviews five learning paradigms and compares that to knowledge acquisition by a neural network classifier.
004130fd-2104-490e-898b-dbea9b115bfb signal data mining from wearable systems 
0041a72a-00ba-4eef-b6a3-84b0a7fb3935 a mapreduce-based distributed svm algorithm for automatic image annotation machine learning techniques have facilitated image retrieval by automatically classifying and annotating images with keywords. among them support vector machines (svms) have been used extensively due to their generalization properties. however, svm training is notably a computationally intensive process especially when the training dataset is large. this paper presents mrsmo, a mapreduce based distributed svm algorithm for automatic image annotation. the performance of the mrsmo algorithm is evaluated in an experimental environment. by partitioning the training dataset into smaller subsets and optimizing the partitioned subsets across a cluster of computers, the mrsmo algorithm reduces the training time significantly while maintaining a high level of accuracy in both binary and multiclass classifications.
0041e0df-25aa-4c1b-a49b-c731d5c6279e efficient minimization of higher order submodular functions using monotonic boolean functions submodular function minimization is a key problem in a wide variety of applications in machine learning, economics, game theory, computer vision and many others. the general solver has a complexity of o(n6 + n5l) where l is the time required to evaluate the function and n is the number of variables [22]. on the other hand, many useful applications in computer vision and machine learning applications are defined over a special subclasses of submodular functions in which that can be written as the sum of many submodular cost functions defined over cliques containing few variables. in such functions, the pseudo-boolean (or polynomial) representation [2] of these subclasses are of degree (or order, or clique size) k where k ii n. in this work, we develop efficient algorithms for the minimization of this useful subclass of submodular functions. to do this, we define novel mapping that transform submodular functions of order k into quadratic ones, which can be efficiently minimized in o(n3) time using a max-flow algorithm. the underlying idea is to use auxiliary variables to model the higher order terms and the transformation is found using a carefully constructed linear program. in particular, we model the auxiliary variables as monotonic boolean functions, allowing us to obtain a compact transformation using as few auxiliary variables as possible. specifically, we show that our approach for fourth order function requires only 2 auxiliary variables in contrast to 30 or more variables used in existing approaches. in the general case, we give an upper bound for the number or auxiliary variables required to transform a function of order k using dedekind number, which is substantially lower than the existing bound of 22k.
00435191-8af9-48e1-bf3f-6aa51f0c3ac7 a hybrid method of optimal data mining and artificial neural network for voltage stability assessment this paper proposes a new method for voltage stability assessment in power systems. the proposed method is based on a hybrid method of optimal data mining and an artificial neural network (ann). voltage stability is of main concern in power system operation and planning. in recent years, the deregulated power market brings about the uncertain events and increases the degree of uncertainty. as a result, power system operators are faced with more complicated power system operation and planning. to understand power system conditions appropriately, they need the feature extraction of power system conditions with an index. in this paper, a hybrid method of optimal data mining and an artificial neural network is proposed to estimate a voltage stability index and extract the rules. optimal data mining is based on optimizing the regression tree with respect to the splitting value of the splitting conditions. particle swarm optimization (pso) of swarm intelligence is used to optimize the combination of splitting values. as ann, the multi-layer perceptron (mlp) is employed to estimate the voltage stability index at each cluster. the proposed method makes use of data similarity of power system conditions to estimate the voltage stability index so that mlp becomes more accurate estimator. the effectiveness of the proposed method is demonstrated in a sample system.
00435aed-4c2d-49f8-883e-fb9f52e71c17 a novel visualization approach for data-mining-related classification classification and categorization are common tasks in data mining and knowledge discovery. visualizations of classification models can create understanding and trust in data mining models. however, existing visualizations are often complex or restricted to specific classifiers and attributes. in this work, we propose an intuitive visualization system to observe and understand classification processes and results. our system can handle multiple classes, nominal and numeric attributes, and supports all classifiers whose predictions can be interpreted as probabilities. we state that the possibility to observe the training process of a classifier boosts the understanding of classification results also for non-expert users. in combination with an intuitive visualization, we provide a system to generate in-depth understanding of classification processes and results. our simulations revealed that the system could support the user to better understand a classifier's decision, and to gain insights into classification processes.
0044432f-b2d6-4241-9d0c-1b6816ed4052 towards kernel density estimation over streaming data a variety of real-world applications heavily relies on the analysis of transient data streams. due to the rigid processing requirements of data streams, common analysis techniques as known from data mining are not applicable. a fundamental building block of many data mining and analysis approaches is density estimation. it provides a well-defined estimation of a continuous data distribution, a fact which makes its adaptation to data streams desirable. a convenient method for density estimation utilizes kernels. however, its computational complexity collides with the processing requirements of data streams. in this work, we present a new approach to this problem that combines linear processing cost with a constant amount of allocated memory. we even support a dynamic memory adaptation to changing system resources. our kernel density estimators over streaming data are related to m-kernels, a previously proposed technique, but substantially improve them in terms of accuracy as well as processing time. the results of an experimental study with synthetic and real-world data streams substantiate the efficiency and effectiveness of our approach as well as its superiority to m-kernels with respect to estimation quality and processing time.
00446224-80e2-4718-94e5-1ad1adf4cbd2 uncorrelated multiway discriminant analysis for motor imagery eeg classification motor imagery-based braincomputer interfaces (bcis) training has been proved to be an effective communication system between human brain and external devices. a practical problem in bci-based systems is how to correctly and efficiently identify and extract subject-specific features from the blurred scalp electroencephalography (eeg) and translate those features into device commands in order to control external devices. in real bci-based applications, we usually define frequency bands and channels configuration that related to brain activities beforehand. however, a steady configuration usually loses effects due to individual variability among different subjects in practical applications. in this study, a robust tensor-based method is proposed for a multiway discriminative subspace extraction from tensor-represented eeg data, which performs well in motor imagery eeg classification without the prior neurophysiologic knowledge like channels configuration and active frequency bands. motor imagery eeg patterns in spatial-spectral-temporal domain are detected directly from the multidimensional eeg, which may provide insights to the underlying cortical activity patterns. extensive experiment comparisons have been performed on a benchmark dataset from the famous bci competition iii as well as self-acquired data from healthy subjects and stroke patients. the experimental results demonstrate the superior performance of the proposed method over the contemporary methods.
0045030e-e58d-4c2a-84c5-2274d4d3ae4e determining model structure for neural models by network stripping abstract   currently, the backpropagation neural network (bpn) is the most widely used network paradigm for solving chemical engineering problems. despite its wide usage, there is no definite methodology for determining the network structure for a particular mapping application. the lack of a network procedure has resulted in a tendency to use networks much larger than needed. such neural models have excessive parameters or weights and often memorize the training data which causes difficulty in extrapolation to unseen data. hence it is important to use networks which have the simplest possible structure, i.e. use the minimum number of weights and nodes.  in this paper, a detailed method to strip a bpn to its essential weights and nodes is proposed. the algorithm, called the stripnet algorithm, results in a network of lesser complexity in terms of its interconnections. such networks reduce the risk of overfitting the data and have better generalization properties. the paper also explains how one can probe into such a stripped network and gain a deeper insight into the knowledge that has been captured. the stripnet algorithm provides a systematic procedure for determining the topology of the network for any application.
00450543-2b67-44ad-a9bc-5774358b5f4e clustering and visualization approaches for human cell cycle gene expression data analysis in this work a comprehensive multi-step machine learning data mining and data visualization framework is introduced. the different steps of the approach are: preprocessing, clustering, and visualization. a preprocessing based on a robust principal component analysis neural network for feature extraction of unevenly sampled data is used. then a probabilistic principal surfaces approach combined with an agglomerative procedure based on fisher's and negentropy information is applied for clustering and labeling purposes. furthermore, a multi-dimensional scaling approach for a 2-dimensional data visualization of the clustered and labeled data is used. the method, which provides a user-friendly visualization interface in both 2 and 3 dimensions, can work on noisy data with missing points, and represents an automatic procedure to get, with no a priori assumptions, the number of clusters present in the data. analysis and identification of genes periodically expressed in a human cancer cell line (hela) using cdna microarrays is carried out as test case.
00458ac0-8c07-4f66-9f1b-08532372d9cf an incremental mining algorithm for high utility itemsets association-rule mining, which is based on frequency values of items, is the most common topic in data mining. in real-world applications, customers may, however, buy many copies of products and each product may have different factors, such as profits and prices. only mining frequent itemsets in binary databases is thus not suitable for some applications. utility mining is thus presented to consider additional measures, such as profits or costs according to user preference. in the past, a two-phase mining algorithm was designed for fast discovering high utility itemsets from databases. when data come intermittently, the approach needs to process all the transactions in a batch way. in this paper, an incremental mining algorithm for efficiently mining high utility itemsets is proposed to handle the above situation. it is based on the concept of the fast-update (fup) approach, which was originally designed for association mining. the proposed approach first partitions itemsets into four parts according to whether they are high transaction-weighted utilization itemsets in the original database and in the newly inserted transactions. each part is then executed by its own procedure. experimental results also show that the proposed algorithm executes faster than the two-phase batch mining algorithm in the intermittent data environment
00463d10-9764-44d9-a6c0-dd6840ebe23f seeing the forest from the trees in two looks: matrix sketching by cascaded bilateral sampling matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions, which has important applications in optimization and machine learning. given a matrix a of size m by n, state-of-the-art randomized algorithms take o(m * n) time and space to obtain its low-rank decomposition. although quite useful, the need to store or manipulate the entire matrix makes it a computational bottleneck for truly large and dense inputs. can we sketch an m-by-n matrix in o(m + n) cost by accessing only a small fraction of its rows and columns, without knowing anything about the remaining data? in this paper, we propose the cascaded bilateral sampling (cabs) framework to solve this problem. we start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling. in particular, the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions. motivated by this analysis, we propose to first generate a pilot-sketch using simple random sampling, and then pursue more advanced, "follow-up" sampling on the pilot-sketch factors seeking maximal encoding powers. in this cascading process, the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step, thus theoretically guarantees the algorithmic boosting property. computationally, our framework only takes linear time and space, and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources. empirical evaluations on benchmark data fully demonstrate the potential of our methods in large scale matrix sketching and related areas.
004667d2-5ada-41cf-9677-03f7b437b90a semi-automatic wordnet based emotion dictionary construction this paper describes an algorithm for semi automatically creating an emotion dictionary using wordnet. the algorithm takes as input a set of seed words that have had emotion information assigned as well as wordnet sense information. from this list an initial dictionary is automatically created using the various relations found within wordnet. then, various correction stages are performed where parts of the dictionary are shown to the user for verification and additional information in the form of emotion polarity and probability information are assigned. using the proposed algorithm with a set of 549 seed words, an emotion dictionary containing over 13,000 wordnet senses was created in just under 7 person-hours of time. to evaluate the created dictionary its usefulness in improving the performance of affect and sentiment classification was examined. classification was performed using support vector machines and a baseline non-machine learning dictionary based algorithm. the results showed that the error rate is reduced when using the dictionary over when not using the dictionary.
0046dde1-4c95-42c7-b1ca-3db18b3db7b7 computation, rhythm, and cerebral slow waves the trajectories of data and modelling from formerly divergent research efforts now seem to be converging to an unexpected region of the phase space of neuroscience. computational network theory and simulation assume that temporal rhythm may be a significant parameter for the successful organization of nonlinear analog computation effected by hierarchical sets of biological neurons and of nonbiological circuitry alike. for neurobiology, the apparently chaotic rhythms of cerebral compound field potentialsthe electroencephalogram (eeg) and slow waves of event related brain potentials (erbp)have long been a phenomenological embarrassment, of only marginal clinical utility. but recent data from molecular biophysics, nonlinear dynamics, artificial intelligence, and scalp-conducted human electrocorticography suggest a possible functional role in the serial gating of neural network computations for the familiar theta-alpha-beta rhythms of the eeg clinic.
00471703-8dca-4800-ba10-740b3980e7a7 a survey of context classfication for intelligent systems research for ambient intelligence isyrami (intelligent systems research for ambient intelligence) proposed by ist [1] is an artificial intelligence oriented methodology and architecture for the development of ambient intelligence (ami) systems. the isyrami architecture considers the following four modules: data/information/knowledge acquisition; data/information/knowledge storage, conversion, and handling; intelligent reasoning; and decision support/intelligent actuation. also, dr. hoon ko had presented about isyrami sf, which was involved security model to isyrami in icwmc2009 [2], isyrami is consists of four modules, that is context allocator, context analyzer, context collector and context detector. because there are various and many contexts in ambient intelligence environment, contexts are needed to classify according to a characteristic and a purpose. therefore, we studied contexts classification that will be generated from isyrami.
0047d322-7c71-4692-a040-6a0efc46042d xml and knowledge based process model reuse and management in business intelligence system as a kind of data-driven decision support systems, business intelligence tools focus too much on data. in order to provide the business intelligence system with the ability of process-driven decision making, we introduce the concept of business process management to the current business intelligence system. with the implementation of case-base reasoning and rule-base reasoning technology, the process models can be built and managed efficiently. in this paper we also provide a strategy for data mining experience reuse. process models in our system are all defined based on xml.
0047f260-acb5-46ac-a207-1d5fd0cb4ea8 a preconditioned conjugate gradient algorithm for generank with application to microarray data mining the problem of identifying key genes is of fundamental importance in biology and medicine. the generank model explores connectivity data to produce a prioritization of the genes in a microarray experiment that is less susceptible to variation caused by experimental noise than the one based on expression levels alone. the generank algorithm amounts to solving an unsymmetric linear system. however, when the matrix in question is very large, the generank algorithm is inefficient and even can be infeasible. on the other hand, the adjacency matrix is symmetric in the generank model, while the original generank algorithm fails to exploit the symmetric structure of the problem in question. in this paper, we discover that the generank problem can be rewritten as a symmetric positive definite linear system, and propose a preconditioned conjugate gradient algorithm to solve it. numerical experiments support our theoretical results, and show superiority of the novel algorithm.
00489dc9-50ad-4ddf-938d-c76401d0d773 association rules network: definition and applications the role of data mining is to search the space of candidate hypotheses to offer solutions, whereas the role of statistics is to validate the hypotheses offered by the data-mining process. in this paper we propose association rules networks (arns) as a structure for synthesizing, pruning, and analyzing a collection of association rules to construct candidate hypotheses. from a knowledge discovery perspective, arns allow for a goal-centric, context-driven analysis of the output of association rules algorithms. from a mathematical perspective, arns are instances of backward-directed hypergraphs. using two extensive case studies, we show how arns and statistical theory can be combined to generate and test hypotheses. copyright  2009 wiley periodicals, inc. statistical analysis and data mining 1: 260-279, 2009
0048f089-ee8b-4d2e-abe7-ba13c86cbbb1 possible detection of pancreatic cancer by plasma protein profiling. the survival rate of pancreatic cancer patients is the lowest among those with common solid tumors, and early detection is one of the most feasible means of improving outcomes. we compared plasma proteomes between pancreatic cancer patients and sex- and age-matched healthy controls using surface-enhanced laser desorption/ionization coupled with hybrid quadrupole time-of-flight mass spectrometry. proteomic spectra were generated from a total of 245 plasma samples obtained from two institutes. a discriminating proteomic pattern was extracted from a training cohort (71 pancreatic cancer patients and 71 healthy controls) using a support vector machine learning algorithm and was applied to two validation cohorts. we recognized a set of four mass peaks at 8,766, 17,272, 28,080, and 14,779 m/z, whose mean intensities differed significantly (mann-whitney u test, p <0 .01), as most accurately discriminating cancer patients from healthy controls in the training cohort [sensitivity of 97.2% (69 of 71), specificity of 94.4% (67 of 71), and area under the curve value of 0.978]. this set discriminated cancer patients in the first validation cohort with a sensitivity of 90.9% (30 of 33) and a specificity of 91.1% (41 of 45), and its discriminating capacity was further validated in an independent cohort at a second institution. when combined with ca19-9, 100% (29 of 29 patients) of pancreatic cancers, including early-stage (stages i and ii) tumors, were detected. although a multi-institutional large-scale study will be necessary to confirm clinical significance, the biomarker set identified in this study may be applicable to using plasma samples to diagnose pancreatic cancer. (cancer res 2005; 65(22): 10613-22)
0048f284-18f7-49fc-8e40-ad26f16a5a2f relationships between depth and number of misclassifications for decision trees this paper describes a new tool for the study of relationships between depth and number of misclassifications for decision trees. in addition to the algorithm the paper also presents the results of experiments with three datasets from uci machine learning repository [3].
0048f783-773c-4010-b8c0-4ecaadfbc8a2 a hybrid approach to the profile creation and intrusion detection anomaly detection involves characterizing the behaviors of individuals or systems and recognizing behavior that is outside the norm. this paper describes some preliminary results concerning the robustness and generalization capabilities of machine learning methods in creating user profiles based on the selection and subsequent classification of command line arguments. we base our method on the belief that legitimate users can be classified into categories based on the percentage of commands they use in a specified period. the hybrid approach we employ begins with the application of expert rules to reduce the dimensionality of the data, followed by an initial clustering of the data and subsequent refinement of the cluster locations using a competitive network called learning vector quantization. since learning vector quantization is a nearest neighbor classifier, and new record presented to the network that lies outside a specified distance is classified as a masquerader. thus, this system does not require anomalous records to be included in the training set.
00491bda-aedf-4041-9607-e05de9e41828 feature selection based on rough sets and particle swarm optimization we propose a new feature selection strategy based on rough sets and particle swarm optimization (pso). rough sets have been used as a feature selection method with much success, but current hill-climbing rough set approaches to feature selection are inadequate at finding optimal reductions as no perfect heuristic can guarantee optimality. on the other hand, complete searches are not feasible for even medium-sized datasets. so, stochastic approaches provide a promising feature selection mechanism. like genetic algorithms, pso is a new evolutionary computation technique, in which each potential solution is seen as a particle with a certain velocity flying through the problem space. the particle swarms find optimal regions of the complex search space through the interaction of individuals in the population. pso is attractive for feature selection in that particle swarms will discover best feature combinations as they fly within the subset space. compared with gas, pso does not need complex operators such as crossover and mutation, it requires only primitive and simple mathematical operators, and is computationally inexpensive in terms of both memory and runtime. experimentation is carried out, using uci data, which compares the proposed algorithm with a ga-based approach and other deterministic rough set reduction algorithms. the results show that pso is efficient for rough set-based feature selection.
0049586f-444e-422b-b3af-7ca7cadf4152 semi-supervised learning in the field of machine learning, semi-supervised learning (ssl) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). interest in ssl has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. this first comprehensive overview of ssl presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. semi-supervised learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. the core of the book is the presentation of ssl methods, organized according to algorithmic strategies. after an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. the book then discusses ssl applications and offers guidelines for ssl practitioners by analyzing the results of extensive benchmark experiments. finally, the book looks at interesting directions for ssl research. the book closes with a discussion of the relationship between semi-supervised learning and transduction. adaptive computation and machine learning series
0049ed58-bbc2-4068-bcd9-cffda57a41d4 adaptive probabilistic neural network-based crane type selection system due to the central role of cranes in construction operations, specialists in the construction industries have cooperated in the development of structured methods and software for crane selection. most of these software tools are for crane model selection, and integrated systems that handle both crane type and model selection are not readily available. this paper presents the crane type selection features of intellicranes, a prototype integrated crane selection tool that assists in both crane type and crane model selection based on a set of inputs describing the construction operation under consideration. by using historical data and advanced artificial intelligence computing tools such as artificial neural networks, intellicranes automates crane type selection. crane type and crane model selection are seamlessly integrated in a comprehensive crane selection tool, and consistency in the selection of cranes for similar situations is increased.
0049fd67-fbb4-4ba8-92c3-899f1c2f4310 multiple tests for wind turbine fault detection and score fusion using two- level multidimensional scaling (mds) wind is an important renewable energy source. the energy and economic return from building wind farms justify the expensive investments in doing so. however, without an effective monitoring system, under-performing or faulty turbines will cause a huge loss in revenue. early detection of such failures help prevent these undesired working conditions. we develop three tests on power curve, rotor speed curve, pitch angle curve of individual turbine. in each test, multiple states are defined to distinguish different working conditions, including complete shut-downs, under-performing states, abnormally frequent default states, as well as normal working states. these three tests are combined to reach a final conclusion, which is more effective than any single test. through extensive data mining of historical data and verification from farm operators, some state combinations are discovered to be strong indicators of spindle failures, lightning strikes, anemometer faults, etc, for fault detection. in each individual test, and in the score fusion of these tests, we apply multidimensional scaling (mds) to reduce the high dimensional feature space into a 3-dimensional visualization, from which it is easier to discover turbine working information. this approach gains a qualitative understanding of turbine performance status to detect faults, and also provides explanations on what has happened for detailed diagnostics. the state-of-the-art scada (supervisory control and data acquisition) system in industry can only answer the question whether there are abnormal working states, and our evaluation of multiple states in multiple tests is also promising for diagnostics. in the future, these tests can be readily incorporated in a bayesian network for intelligent analysis and decision support.
004a1e37-c2e3-4504-a0b5-8a1f35fc6637 falling detection system based on machine learning as falling is the most important issue that faces elderly people all over the world, this paper proposes a detection system for falling based on machine learning (ml). in the proposed system, a dataset of videos containing falling actions has been utilized via dividing each video into many shots that are consequently being converted into gray-level images. then, for detecting the moving objects in videos, the foreground is firstly detected, then noise and shadow are deleted to detect the moving object. finally, a number of features, including aspect ratio and falling angle, are extracted and a number of classifiers are being applied in order to detect the occurrence of falling. experimental results, using 10-fold cross validation, shown that the proposed falling detection approach based on linear discriminant analysis (lda) classification algorithm has outperformed both support vector machines (svms) and knearest neighbor (knn) classification algorithms via achieving falling detection with accuracy of 96.59 %.
004a4a73-0b92-4622-b2b8-54636243bd49 searching for patterns in long sequences searching for sequential patterns is a popular field in data mining. the richest such patterns are episodes, depicted by directed acyclic graphs (dags), which allow us to discover partial orders among the events making up a pattern. in this thesis, we broaden the traditional concepts used in episode mining in three separate ways. first, rather than stopping at discovering episodes themselves, we search for association rules between them. additionally, we propose three different ways to measure the quality of each such rule. second, we redefine episodes to allow a node in the corresponding dag to carry multiple labels, thus allowing us to discover, and depict, patterns consisting of events that happen simultaneously. third, we introduce a new interestingness measure for parallel episodes, or itemsets, in a sequence. hereby, we tackle the limitation of traditional approaches, which, for reasons of efficiency, insist that smaller episodes should be more interesting than larger ones. our method allows us to drop this unintuitive requirement, and we investigate various ways to ensure that the new measure can be computed efficiently. we experimentally demonstrate that our contributions all have their respective merits, and conclude the thesis with a discussion of possible future extensions of our work.
004aad9c-023a-44ab-86d6-d48d3feef5e6 detecting criminal networks: sna models are compared to proprietary models criminal networks have been an area of interest for public safety and intelligence community as well as social network analysis and data mining community. existing literature shows that offender demographics and crime features are not taken into account to identify their possible links to find out criminal networks. four crime data specific proprietary group detection models (gdm, ogdm, sodm, and comdm) have been developed based on these crime data features. these specific criminal network detection models are compared more common baseline sna group detection algorithms. it is intended to find out, whether these four crime data specific group detection models can perform better than widely used k-cores and n-clique algorithms. two datasets which contain various real criminal networks are used as experimental testbeds.
004b0804-9182-427c-8b46-154edfc055ad similarity of feature selection methods we empirically investigated the similarity among feature selection methods.extensive experiments were carried out across high dimensional classification tasks.we obtained useful insight into the pattern of agreement of eight popular methods. in the past two decades, the dimensionality of datasets involved in machine learning and data mining applications has increased explosively. therefore, feature selection has become a necessary step to make the analysis more manageable and to extract useful knowledge about a given domain. a large variety of feature selection techniques are available in literature, and their comparative analysis is a very difficult task. so far, few studies have investigated, from a theoretical and/or experimental point of view, the degree of similarity/dissimilarity among the available techniques, namely the extent to which they tend to produce similar results within specific application contexts. this kind of similarity analysis is of crucial importance when two or more methods are combined in an ensemble fashion: indeed the ensemble paradigm is beneficial only if the involved methods are capable of giving different and complementary representations of the considered domain. this paper gives a contribution in this direction by proposing an empirical approach to evaluate the degree of consistency among the outputs of different selection algorithms in the context of high dimensional classification tasks. leveraging on a proper similarity index, we systematically compared the feature subsets selected by eight popular selection methods, representatives of different selection approaches, and derived a similarity trend for feature subsets of increasing size. through an extensive experimentation involving sixteen datasets from three challenging domains (internet advertisements, text categorization and micro-array data classification), we obtained useful insight into the pattern of agreement of the considered methods. in particular, our results revealed how multivariate selection approaches systematically produce feature subsets that overlap to a small extent with those selected by the other methods.
004b910e-2128-4c63-ab75-dafb3e85789a usage reporting on recorded lectures using educational data mining this study analyses the interactions of students with the recorded lectures. we report on an analysis of students' use of recorded lectures at two universities in the netherlands. the data logged by the lecture capture system (lcs) is used and combined with collected survey data. we describe the process of data pre-processing and analysis of the resulting full dataset and then focus on the usage for the course with the most learner sessions. we found discrepancies as well as similarities between students' verbal reports and actual usage as logged by the recorded lecture servers. the analysis shows that recorded lectures are viewed to prepare for exams and assignments. the data suggests that students who do this have a significantly higher chance of passing the exams. given the discrepancies between verbal reports and actual usage, research should no longer rely on verbal reports alone.
004cb14b-59b4-4fca-b620-c779bf33e080 approximate classification using conceptual clustering the ability of a system to classify objects (physical or abstract) accurately and approximately is essential for performing such artificial intelligence tasks as natural language processing, inductive reasoning, and building decision support systems and expert systems. this paper introduces an approach to data analysis which will allow us to determine whether a set of objects is precisely describable, approximately describable, or non-describable. this approach also provides a conceptual description of the set whenever it is precisely or approximately describable. finally, this data analysis approach is used as a stepping stone to study foundations of knowledge representation.
004e50d3-cb0f-481a-a010-3892ffaaa153 a development of fuzzy encoding and decoding through fuzzy clustering fuzzy clustering has emerged as a fundamental technique of information granulation. in this study, we introduce and discuss multivariable encoding and decoding mechanisms (referred altogether as a reconstruction problem) expressed in the language of fuzzy sets and fuzzy relations. the underlying performance index associated with the problem helps quantify a reconstruction error that arises when transforming a numeric datum through fuzzy sets (relations) and then reconstructing it into an original numeric format. the clustering platform considered in this study concerns the well-known algorithm of fuzzy c-means (fcm). the main design aspects deal with the relationships between the number of clusters versus the reconstruction properties and the resulting reconstruction error. the impact of the fuzzification coefficient on the reconstruction quality is investigated. this finding is of interest, given the fact that predominantly all applications involving fcm use the value of the fuzzification coefficient equal to 2. in light of the completed experiments, we demonstrate that this selection may not be experimentally legitimate. we also carry out a comparative analysis of the reconstruction properties of the boolean decoding that is induced by the fuzzy partition. experimental investigations involve selected machine learning data.
004ea006-b097-4bea-b827-c331c4e9f4a1 on classifier behavior in the presence of mislabeling noise machine learning algorithms perform differently in settings with varying levels of training set mislabeling noise. therefore, the choice of the right algorithm for a particular learning problem is crucial. the contribution of this paper is towards two, dual problems: first, comparing algorithm behavior; and second, choosing learning algorithms for noisy settings. we present the sigmoid rule framework, which can be used to choose the most appropriate learning algorithm depending on the properties of noise in a classification problem. the framework uses an existing model of the expected performance of learning algorithms as a sigmoid function of the signal-to-noise ratio in the training instances. we study the characteristics of the sigmoid function using five representative non-sequential classifiers, namely, naive bayes, knn, svm, a decision tree classifier, and a rule-based classifier, and three widely used sequential classifiers based on hidden markov models, conditional random fields and recursive neural networks. based on the sigmoid parameters we define a set of intuitive criteria that are useful for comparing the behavior of learning algorithms in the presence of noise. furthermore, we show that there is a connection between these parameters and the characteristics of the underlying dataset, showing that we can estimate an expected performance over a dataset regardless of the underlying algorithm. the framework is applicable to concept drift scenarios, including modeling user behavior over time, and mining of noisy time series of evolving nature.
004ede9f-1e10-422f-a5d8-014b60b1b89c a novel approach for mining frequent itemsets: apriorimin the step of mining frequent itemsets in database is the essential step and most expensive in the process of mining association rules in data mining task, many algorithms of mining frequent itemsets have been proposed to improve the performance of apriori algorithm. in this paper, we have introduced an optimization in the phase of generation pruning of candidates by a new strategy for the calculation of frequent itemsets based on approximate values of supports exact the itemsets. we have evaluated our algorithm apriorimin against three popular frequent itemsets mining algorithms  apriori and fp-growth, close using two data sets with a variety of minimum support.
004f501d-fa7b-4898-8d18-001605ed7560 towards network-aware data mining distributed data mining algorithms executing on a shared network of workstations often suffer from unpredictable performance problems due to limited network resources that are being shared. we show that data mining algorithms, which have an approximate nature, can adapt to network-resource constraints. we argue that existing network monitoring and quality of service mechanisms are insufficient to support the needs of such applications. we then discuss the mechanisms (systems support) needed to support such network-aware applications. finally, we describe the current status (work-in-progress) of the system under development and discuss some general issues involved in developing resource-aware data mining algorithms.
005004d8-efdc-4a2e-a978-8e1c502befaf efficient clustering of databases induced by local patterns many large organizations have multiple large databases as they transact from multiple branches. most of the previous pieces of work are based on a single database. thus, it is necessary to study data mining on multiple databases. in this paper, we propose two measures of similarity between a pair of databases. also, we propose an algorithm for clustering a set of databases. efficiency of the clustering process has been improved using the following strategies: reducing execution time of clustering algorithm, using more appropriate similarity measure, and storing frequent itemsets space efficiently.
005019a0-663b-4ed7-b2d3-376f36f4232c on the use of security metrics based on intrusion prevention system event data: an empirical analysis with the increasing number of attacks on the internet, a primary concern for organizations is the protection of their network. to do so, organizations install security devices such as intrusion prevention systems to monitor network traffic. however, data that are collected by these devices are often imperfect. the contribution of this paper is to try to define some practical metrics based on imperfect data collected by an intrusion prevention system. since attacks greatly differ, we propose to group the attacks into several attack type groups. we then define a set of metrics for each attack type group. we introduce an approach that consists in analyzing the evolution of these metrics per attack type group by focusing on outliers in order to give an insight into an organizationpsilas security. the method is assessed for an organization of about 40,000 computers. the results were encouraging: outliers could be related to security issues that, in some cases, had not been previously flagged.
00508bdf-21cd-47b9-b73b-b96e910c887f multimedia information technology and the annotation of video the state of the art in multimedia information technology has not progressed to the point where a single solution is available to meet all reasonable needs of documentalists and users of video archives. in general, we do not have an optimistic view of the usability of new technology in this domain, but digitization and digital power can be expected to cause a small revolution in the area of video archiving. the volume of data leads to two views of the future: on the pessimistic side, overload of data will cause lack of annotation capacity, and on the optimistic side, there will be enough data from which to learn selected concepts that can be deployed to support automatic annotation. at the threshold of this interesting era, we make an attempt to describe the state of the art in technology. we sample the progress in text, sound, and image processing, as well as in machine learning.
0050f96c-a3f3-4eed-ab1b-99f786b47487 a new approach for discovering frequent pattern from transactional database frequent pattern mining is a heavily researched area in the field of data mining with wide range of applications. finding a frequent pattern (or items) play essentials role in data mining. efficient algorithm to discover frequent patterns is essential in data mining research. a number of research works have been published that presenting new algorithm or improvements on existing algorithm to solve data mining problem efficiently. in that apriori algorithm is the first algorithm proposed in this field. by the time of change or improvement in apriori algorithm, which compressed large database in to small tree data structure like fp tree, can tree and cp tree have been discovered. in cp tree, like fp tree it contains frequent and non frequent items at the mining time. so item required to extract frequent pattern is more. in this paper i propose a new novel tree structure - extension of cp tree that extract all frequent pattern from transactional database using cp-mine algorithm. so at the mining time it contains only frequent patterns. in my proposed algorithm it mine frequent item set from our proposed tree structure by using pruning tree and marked value technique. this tree structure constructs compact prefix tree structure with one database scan and it provide same mining performance as fp growth technique by efficient tree restructuring process. my proposed tree structure support interactice mining and incremental mining without rescan the original database.
0052b66a-d98f-410e-8627-11084344b52d from machine learning to natural product derivatives that selectively activate transcription factor ppar advanced kernel-based machine learning methods enable the identification of innovative bioactive compounds with minimal experimental effort. comparative virtual screening revealed that nonlinear models of the underlying structureactivity relationship are necessary for successful compound picking. in a proof-of-concept study a novel truxillic acid derivative was found to selectively activate transcription factor ppar.
0052f5f5-73c3-402c-a8fb-cb59794da08b setwise comparison: consistent, scalable, continuum labels for computer vision a growing number of domains, including affect recognition and movement analysis, require a single, real number ground truth label capturing some property of a video clip. we term this the provision of continuum labels. unfortunately, there is often an uncacceptable trade-off between label consistency and the efficiency of the labelling process with current tools. we present a novel interaction technique, setwise comparison, which leverages the intrinsic human capability for consistent relative judgements and the trueskill algorithm to solve this problem. we describe sortable, a system demonstrating this technique. we conducted a real-world study where clinicians labelled videos of patients with multiple sclerosis for the assess ms computer vision system. in assessing the efficiency-consistency trade-off of setwise versus pairwise comparison, we demonstrated that not only is setwise comparison more efficient, but it also elicits more consistent labels. we further consider how our findings relate to the interactive machine learning literature.
00534618-274d-43c1-8dce-13ee1e7207dc user-driven development of text mining resources for cancer risk assessment one of the most neglected areas of biomedical text mining (tm) is the development of systems based on carefully assessed user needs. we investigate the needs of an important task yet to be tackled by tm --- cancer risk assessment (cra) --- and take the first step towards the development of tm for the task: identifying and organizing the scientific evidence required for cra in a taxonomy. the taxonomy is based on expert annotation of 1297 medline abstracts. we report promising results with inter-annotator agreement tests and automatic classification experiments, and a user test which demonstrates that the resources we have built are well-defined, accurate, and applicable to a real-world cra scenario. we discuss extending and refining the taxonomy further via manual and machine learning approaches, and the subsequent steps required to develop tm for the needs of cra.
0053a7a1-8bc1-4084-b545-50c303cd97c7 chapter 9  public health surveillance: predictive analytics and big data recent advances in artificial intelligence are providing an unprecedented ability of medical research and clinical organizations to collect and analyze data that is broader in scope and more exacting in detail. new technologies are providing for revolutionary advances, improving our daily lives in health care. however, with any emerging technological advance, there is a potential for abuse of privacy and reduced quality of medical care. this chapter will discuss the process of collecting, analyzing, and applying data, using examples from clinical best practices, as well as the specific review of the durkheim project. closing the discussion, will be a technical analysis of advancing analytic systems.
005405a1-1a14-4395-af08-021c8f5c33d4 evaluations of deep convolutional neural networks for automatic identification of malaria infected cells this paper studied automatic identification of malaria infected cells using deep learning methods. we used whole slide images of thin blood stains to compile an dataset of malaria-infected red blood cells and non-infected cells, as labeled by a group of four pathologists. we evaluated three types of well-known convolutional neural networks, including the lenet, alexnet and googlenet. simulation results showed that all these deep convolution neural networks achieved classification accuracies of over 95%, higher than the accuracy of about 92% attainable by using the support vector machine method. moreover, the deep learning methods have the advantage of being able to automatically learn the features from the input data, thereby requiring minimal inputs from human experts for automated malaria diagnosis.
0054acfe-e599-4091-bab1-4f1f8213ebcc scaling up machine learning: scalable parallelization of automatic speech recognition 
0054c704-93d6-4bb1-8743-40d1ce2600ed analysis of causative attacks against svms learning from data streams machine learning algorithms have been proven to be vulnerable to a special type of attack in which an active adversary manipulates the training data of the algorithm in order to reach some desired goal. although this type of attack has been proven in previous work, it has not been examined in the context of a data stream, and no work has been done to study a targeted version of the attack. furthermore, current literature does not provide any metrics that allow a system to detect these attack while they are happening. in this work, we examine the targeted version of this attack on a support vector machine(svm) that is learning from a data stream, and examine the impact that this attack has on current metrics that are used to evaluate a models performance. we then propose a new metric for detecting these attacks, and compare its performance against current metrics.
0055982b-df4d-4041-b0ac-44ce2b3a2288 interactive distributed data access in a grid environment new interactive applications for the grid environment will profit from the use of distributed computing techniques like the message passing interface (mpi). the data these applications access should also be distributed for the sake of performance. we describe a method implemented in the european crossgrid project for enabling access to distributed data sources, for use in high energy physics and meteorology data mining applications.
0056df2e-eb30-42af-aa6e-640f69127fc8 predicting clinical response to anticancer drugs using an ex vivo platform that captures tumour heterogeneity predicting clinical response to anticancer drugs remains a major challenge in cancer treatment. emerging reports indicate that the tumour microenvironment and heterogeneity can limit the predictive power of current biomarker-guided strategies for chemotherapy. here we report the engineering of personalized tumour ecosystems that contextually conserve the tumour heterogeneity, and phenocopy the tumour microenvironment using tumour explants maintained in defined tumour grade-matched matrix support and autologous patient serum. the functional response of tumour ecosystems, engineered from 109 patients, to anticancer drugs, together with the corresponding clinical outcomes, is used to train a machine learning algorithm; the learned model is then applied to predict the clinical response in an independent validation group of 55 patients, where we achieve 100% sensitivity in predictions while keeping specificity in a desired high range. the tumour ecosystem and algorithm, together termed the canscript technology, can emerge as a powerful platform for enabling personalized medicine.
0057d416-7b31-47a5-bc49-c1c84a0da44c disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks the recognition of disease and chemical named entities in scientific articles is a very important subtask in information extraction in the biomedical domain. due to the diversity and complexity of disease names, the recognition of named entities of diseases is rather tougher than those of chemical names. although there are some remarkable chemical named entity recognition systems available online such as chemspot and tmchem, the publicly available recognition systems of disease named entities are rare. this article presents a system for disease named entity recognition (dner) and normalization. first, two separate dner models are developed. one is based on conditional random fields model with a rule-based post-processing module. the other one is based on the bidirectional recurrent neural networks. then the named entities recognized by each of the dner model are fed into a support vector machine classifier for combining results. finally, each recognized disease named entity is normalized to a medical subject heading disease name by using a vector space model based method. experimental results show that using 1000 pubmed abstracts for training, our proposed system achieves an f1-measure of 0.8428 at the mention level and 0.7804 at the concept level, respectively, on the testing data of the chemical-disease relation task in biocreative v.
0057e08f-f770-42ed-97e9-cc376d776c36 use of artificial neural network in data mining for weather forecasting knowledge of weather data or climate data in a region is essential for business, society, agriculture and energy applications. the main aim of this paper is to overview on data mining process for weather data and to study on weather data using data mining technique like clustering technique. by using this technique we can acquire.weather data and can find the hidden patterns inside the large dataset so as to transfer the retrieved information into usable knowledge for classification and prediction of climate condition. we discussed how to use a data mining technique to analyze the metrological data like weather data.. a variety of data mining tools and techniques are available in the industry, but they have been used in a very limited way for meteorological data. in this paper, a neural network-based algorithm for predicting the atmosphere for a future time and a given location is presented. we have used back propagation neural (bpn) network for initial modelling. the results obtained by bpn model are fed to a hopfield network. the performance of our proposed ann-based method (bpn and hopfield network based combined approach) tested on 3 years weather data set comprising 15000 records containing attributes like temperature, humidity and wind speed. the prediction error is found to be very less and the learning converges very sharply. the main focus of this paper is based on predictive data mining by which we can extract interesting (non-trivial, implicit, previously unknown and potentially useful) patterns or knowledge from huge amount of meteorological data.
0058051c-e496-4c4a-a2aa-b462db4e3e51 neural network and principal component analyses of highly variable myocardial mechanical waveforms derived from echocardiographic ultrasound images we introduce a new type of data for classification of regional segments of myocardium. we have analyzed strain measurements taken throughout the cardiac cycle from the echocardiograms of pigs. classifications by both principal component analysis (pca) and by neural network (nn) are combined for a data mining operation. differences in strain waveforms between normal and diseased myocardium may further elucidate the corresponding changes in physiology. altered functioning of the heart muscle is reflected by strain, and objective computer analysis should aid in the diagnosis of ischemia. we hypothesize that the entire strain waveform over one heart cycle can be classified to functionally determine whether or not a myocardial region is perfused.
005834ed-21fc-4f44-babb-71f09490c07f sitelayout modeling: how can artificial intelligence help? using the sitelayout task as an example to compare existing practices and tools used in industry and research environments, this paper puts artificial intelligence (ai) modeling techniques in perspective. the sitelayout task is characterized, field practice is described, and a thorough review of available tools for layout product modeling (including physical models and computeraided design tools) and process modeling (using ai as well as operations research methods) is presented. a rationale is provided for why many such tools have failed to gain widespread use in the construction industry. comparing the capabilities as well as the data and knowledge needs of computer programs with those of construction practitioners reveals a large discrepancy, which is also apparent when fitting the layout literature in a comprehensive table. this paper argues that aibased systems can reduce this discrepancy by better matching model capabilities with industry needs, and, therefore, suggests that such models will bec...
0058969d-3b8a-42b6-9c9e-95e3db4e9b32 parsimonious kernel extreme learning machine in primal via cholesky factorization recently, extreme learning machine (elm) has become a popular topic in machine learning community. by replacing the so-called elm feature mappings with the nonlinear mappings induced by kernel functions, two kernel elms, i.e., p-kelm and d-kelm, are obtained from primal and dual perspectives, respectively. unfortunately, both p-kelm and d-kelm possess the dense solutions in direct proportion to the number of training data. to this end, a constructive algorithm for p-kelm (ccp-kelm) is first proposed by virtue of cholesky factorization, in which the training data incurring the largest reductions on the objective function are recruited as significant vectors. to reduce its training cost further, pccp-kelm is then obtained with the application of a probabilistic speedup scheme into ccp-kelm. corresponding to ccp-kelm, a destructive p-kelm (cdp-kelm) is presented using a partial cholesky factorization strategy, where the training data incurring the smallest reductions on the objective function after their removals are pruned from the current set of significant vectors. finally, to verify the efficacy and feasibility of the proposed algorithms in this paper, experiments on both small and large benchmark data sets are investigated.
0058b581-9746-4ed0-b7a2-0f0b43d20228 chapter 2  solving problems in industry publisher summary#r##n#this chapter explores various business problems that are common to many industries and that can be addressed with a strategy based on data mining. the chapter further identifies and discusses several cross-industry solutions, where data mining plays a central role. the solutions discussed in this chapter are customer acquisition, customer retention, response modelling, fraud detection, cross-selling, new product line development, survey analysis, credit storing, warranty analysis, and defect analysis. understanding such common data mining scenarios is a beginning for identifying uses of data mining in individual application domains. this chapter also highlights several industries and their particular uses of data mining. each of these industries can apply the cross-industry solutions, cited and tailored to their own domain-specific needs. this chapter also reveals that data mining's benefits can be leveraged by companies both big and small, from large financial institutions to local car dealerships, those with millions of customers and those with hundreds, and those with scientific as well as manufacturing process data analysis needs.
00596e7c-1536-45aa-95cb-18e38437d2f9 knowledge discovery, data mining and hybrid systems 
005b7dac-1819-429b-9fcc-069dbfdcddce eye movement prediction and variability on natural video data sets we here study the predictability of eye movements when viewing high-resolution natural videos. we use three recently published gaze data sets that contain a wide range of footage, from scenes of almost still-life character to professionally made, fast-paced advertisements and movie trailers. inter-subject gaze variability differs significantly between data sets, with variability being lowest for the professional movies. we then evaluate three state-of-the-art saliency models on these data sets. a model that is based on the invariants of the structure tensor and that combines very generic, sparse video representations with machine learning techniques outperforms the two reference models; performance is further improved for two data sets when the model is extended to a perceptually inspired colour space. finally, a combined analysis of gaze variability and predictability shows that eye movements on the professionally made movies are the most coherent (due to implicit gaze-guidance strategies of the movie directors), yet the least predictable (presumably due to the frequent cuts). our results highlight the need for standardized benchmarks to comparatively evaluate eye movement prediction algorithms.
005bdee2-09af-4208-a57e-8cf821dba9b9 laser desorption/ionization mass spectrometry fingerprinting of complex hydrocarbon mixtures: application to crude oils using data mining techniques crude oil fingerprints were obtained from four crude oils by laser desorption/ionization mass spectrometry (ldi-ms) using a silver nitrate cationization reagent. replicate analyses produced spectral data with a large number of features for each sample (>11000m/z values) which were statistically analyzed to extract useful information for their differentiation. individual characteristic features from the data set were identified by a false discovery rate based feature selection procedure based on the analysis of variance models. the selected features were, in turn, evaluated using classification models. a substantially reduced set of 23 features was obtained through this procedure. one oil sample containing a high ratio of saturated/aromatic hydrocarbon content was easily distinguished from the others using this reduced set. the other three samples were more difficult to distinguish by ldi-ms using a silver cationization reagent; however, a minimal number of significant features were still identified for this purpose. focus is placed on presenting this multivariate statistical method as a rapid and simple analytical procedure for classifying and distinguishing complex mixtures.
005ccbff-3a9b-4dd4-ae09-65f9244bf6ec simulated crowd: towards a synthetic culture for engaging a learner in culture-dependent nonverbal interaction difficulties in living in a different culture are caused by different patterns of thinking, feeling and potential actions. a good way to experience cultural immersion is to walk in a crowd. this paper proposes a simulated crowd as a novel tool for allowing people to practice culture-specific nonverbal communication behaviors. we present a conceptual framework of a simulated crowd using an immersive interactive environment. we discuss technical challenges concerning a simulated crowd, including realtime eye gaze recognition in a dynamic moving situation, sensing of nonverbal behaviors using multiple range sensors, and behavior generation based on novel temporal data mining algorithms.
005cd346-257a-471b-b052-388492d66696 applications of data mining techniques in telecom churn prediction in this competitive world, business becomes highly saturated. especially, the field of telecommunication faces complex challenges due to a number of vibrant competitive service providers. therefore it has become very difficult for them to retain existing customers. since the cost of acquiring new customers is much higher than the cost of retaining the existing customers, it is the time for the telecom industries to take necessary steps to retain the customers to stabilize their market value. this paper explores the application of data mining techniques in predicting likely churners and the impact of attribute selection on identifying the churn. it also compares the efficiency of decision tree and neural network classifiers and lists their performances.
005d77ba-da16-4eda-9916-991f38a90095 using persistent homology and dynamical distances to analyze protein binding. persistent homology captures the evolution of topological features of a model as a parameter changes. the most commonly used summary statistics of persistent homology are the barcode and the persistence diagram. another summary statistic, the persistence landscape, was recently introduced by bubenik. it is a functional summary, so it is easy to calculate sample means and variances, and it is straightforward to construct various test statistics. implementing a permutation test we detect conformational changes between closed and open forms of the maltose-binding protein, a large biomolecule consisting of 370 amino acid residues. furthermore, persistence landscapes can be applied to machine learning methods. a hyperplane from a support vector machine shows the clear separation between the closed and open proteins conformations. moreover, because our approach captures dynamical properties of the protein our results may help in identifying residues susceptible to ligand binding; we show that the majority of active site residues and allosteric pathway residues are located in the vicinity of the most persistent loop in the corresponding filtered vietoris-rips complex. this finding was not observed in the classical anisotropic network model.
005dd88f-ea5d-478c-a8c8-0cc2b078dcdc quantifying knowledge base inconsistency via fixpoint semantics inconsistency and its handling are very important in the real worldand in the fields of computer science and artificial intelligence. when dealingwith inconsistency in a knowledge base (kb), there is a whole host of deeperissues we need to contend with in order to develop rational and robust intelligentsystems. in this paper, we focus our attention on one of the issues in copingwith kb inconsistency: how to measure the information content and thesignificance of inconsistency in a kb. our approach is based on a fixpoint semanticsfor kb. the approach reflects each inconsistent set of rules in the leastfixpoint of a kb and then measures the inconsistency in the context of the leastfixpoint for the kb. compared with the existing results, our approach has someunique benefits.
005f2e42-d4be-45de-85a1-6e4f671c48ed an intrinsic subsequence decomposition algorithm for network intrusion detection the problem of network intrusion detection is an active research issue. based on the techniques of sequence data mining, we propose a completely new approach based on intrinsic subsequence to detect intrusions in the network connection data. an intrinsic subsequence means that all items in it are always present together as a whole in the sequence. the total number of an intrinsic subsequence appeared in a sequence is referred to as absolute support. the intrinsic subsequences with approximate absolute support form a layer. a sequence is supposed to be composed of a set of intrinsic subsequences. and the anomalies are always shown as a composition of some unusual intrinsic subsequences. the abnormal sequence can be detected by decomposing the sequence into a number of layers and finding the differences of the corresponding layers between the normal and suspect sequence data. an original algorithm for intrusion detection by using the idea of decomposition is proposed. the experiments on the data sets of kdd 99 illuminate the utility and efficiency of our new approach.
005f4991-6925-4ab2-8e5d-adee7920ca61 quantifying information and contradiction in propositional logic through test actions 
005fcde8-dc00-494a-a6fd-f692390c1ef3 open social and xacml based group authorization framework in a data-driven science collaborative framework, access authorization is a vital component to facilitate the management of the collective data and computing resources shared by researchers from geographically distributed locations. but traditional virtual organization based access control frameworks are not suitable for self-organizing, ad-hoc and opportunistic scientific collaborations, in which scientists can easily set up group-oriented authorization rules across the administrative domains to share their resources by flexible and effective access control. using the emerging oauth2.0 protocol and xacml framework, this paper introduces a novel open social based access control framework to support ad-hoc team formation and user-controlled resource sharing. to verify the effectiveness of our authorization framework, we develop a infant birth-defect data and data mining resource-sharing application. our experiences demonstrate that the proposed framework is a very promising approach to resource sharing in cross-domain network environments.
005fce0e-787f-4c75-b000-39cb8d9d91b9 automatically correcting bias in speaker recognition systems in this paper we present a general machine learning framework for score bias reduction and analysis in speaker recognition systems. the general principle is to learn a meta-system using recognition systems' errors, given the training and testing conditions in which they occurred. in the context of speaker recognition, the proposed method is able to reduce the bias introduced in scores due to a variety of factors such as channel mismatch, additive noise, gender mismatch, different speaking styles, etc. moreover, this framework enables a deep understanding of the origins of score bias in any system, which will support an optimized system redesign. preliminary results obtained with several state-of-the-art systems showed considerable improvement in original performance, in addition to identifying sources of system bias.
006012dd-1d33-43be-ba20-a5ac34b6dec1 a data mining based analysis of nmap operating system fingerprint database an operating system (os) fingerprint database is used by nmap to identify oses performing tcp/ip (transmission control pro- tocol/internet protocol) stack identification. each entry in nmap os fingerprint database (nmap-os-db) represents an os. using data mining techniques, we propose three new forms of representation of nmap-os-db that can express how operating systems are similar among them accord- ing to their tcp/ip stack implementation. this approach can improve the capability of identifying devices running unknown oses. other ap- plications are also presented.
00601db9-9366-49af-b09f-19e1af6ec446 synthesizing high-frequency rules from different data sources many large organizations have multiple data sources, such as different branches of an interstate company. while putting all data together from different sources might amass a huge database for centralized processing, mining association rules at different data sources and forwarding the rules (rather than the original raw data) to the centralized company headquarter provides a feasible way to deal with multiple data source problems. in the meanwhile, the association rules at each data source may be required for that data source in the first instance, so association analysis at each data source is also important and useful. however, the forwarded rules from different data sources may be too many for the centralized company headquarter to use. this paper presents a weighting model for synthesizing high-frequency association rules from different data sources. there are two reasons to focus on high-frequency rules. first, a centralized company headquarter is interested in high-frequency rules because they are supported by most of its branches for corporate profitability. second, high-frequency rules have larger chances to become valid rules in the union of all data sources. in order to extract high-frequency rules efficiently, a procedure of rule selection is also constructed to enhance the weighting model by coping with low-frequency rules. experimental results show that our proposed weighting model is efficient and effective.
00607727-542a-43de-85c8-7e77060753ee using multivariate adaptive regression splines (mars) to develop crash modification factors for urban freeway interchange influence areas crash modification factors (cmfs) are used to measure the safety impacts of changes in specific geometric characteristics. their development has gained much interest following the adoption of cmfs by the recently released highway safety manual (hsm) and safetyanalyst tool in the united states. this paper describes a study to develop cmfs for interchange influence areas on urban freeways in the state of florida. despite the very different traffic and geometric conditions that exist in interchange influence areas, most previous studies have not separated them from the rest of the freeway system in their analyses. in this study, a promising data mining method known as multivariate adaptive regression splines (mars) was applied to develop cmfs for median width and inside and outside shoulder widths for "total" and "fatal and injury" (fi) crashes. in addition, cmfs were also developed for the two most frequent crash types, i.e., rear-end and sideswipe. mars is characterized by its ability to accommodate the nonlinearity in crash predictors and to allow the impact of more than one geometric variable to be simultaneously considered. the methodology further implements crash predictions from the model to identify changes in geometric design features. four years of crashes from 2007 to 2010 were used in the analysis and the results showed that mars's prediction capability and goodness-of-fit statistics outperformed those of the negative binomial model. the influential variables identified included the outside and inside shoulder widths, median width, lane width, traffic volume, and shoulder type. it was deduced that a 2-ft increase in the outside and inside shoulders (from 10ft to 12ft) reduces fi crashes by 10% and 33%, respectively. further, a 42-ft reduction in the median width (from 64ft to 22ft) increases the rear-end, total, and fi crashes by 473%, 263%, and 223%, respectively. language: en
0060af9c-09e2-4e3f-979c-ad60a0ca2b6f regression modeling in back-propagation and projection pursuit learning we study and compare two types of connectionist learning methods for model-free regression problems: 1) the backpropagation learning (bpl); and 2) the projection pursuit learning (ppl) emerged in recent years in the statistical estimation literature. both the bpl and the ppl are based on projections of the data in directions determined from interconnection weights. however, unlike the use of fixed nonlinear activations (usually sigmoidal) for the hidden neurons in bpl, the ppl systematically approximates the unknown nonlinear activations. moreover, the bpl estimates all the weights simultaneously at each iteration, while the ppl estimates the weights cyclically (neuron-by-neuron and layer-by-layer) at each iteration. although the bpl and the ppl have comparable training speed when based on a gauss-newton optimization algorithm, the ppl proves more parsimonious in that the ppl requires a fewer hidden neurons to approximate the true function. to further improve the statistical performance of the ppl, an orthogonal polynomial approximation is used in place of the supersmoother method originally proposed for nonlinear activation approximation in the ppl. >
006155b4-92ff-42d3-957f-9920d3f0b44f advances in technology are changing the future of medicine. in this october, we are having the most important allergy festival in asia-pacific region - the joint congress of asia pacific association of allergy, asthma, and clinical immunology (apaaaci) and asia pacific association of pediatric allergy, respiratory, and immunology in kuala lumpur, malaysia (october 1720, 2016, http://www.apaaaci-kl2016.org). the joint congress will be hosted by the malaysian society of allergy and immunology. "the era of allergy: local and global insights and intervention" is the main theme which will make the congress scientifically meaningful. there will be special sessions such as commemorating 50 years since discovery of ige.#r##n##r##n#it is remarkable that new international classification of diseases (icd)-11 beta phase structure "allergic and hypersensitivity conditions" section has been constructed as a result of a detailed and careful action plan based on scientific evidences for the necessity of changes and collaboration with the world health organization icd-11 revision governance [1]. apaaaci is supporting this international collaboration with the world allergy organization, the american academy of allergy asthma and immunology, the european academy of allergy and clinical immunology, the latin american society of allergy, asthma and immunology, and the american college of allergy asthma and immunology [2,3]. readers of this issue will find an original article on the mapping of icd-10 allergic and hypersensitivity conditions in the icd-11 beta phase structure [1].#r##n##r##n#readers of this issue will also find a review article on allergen-specific immunotherapy in pediatric allergic asthma [4] and an original article on the mixed house dust mite and weed pollen extract immunotherapy [5]. immunotherapy with a history of more than 100 years still works for allergy patients.#r##n##r##n#as radiocontrast media (rcm) media induced hypersensitivity is an unpredictable adverse drug reaction, prevention of the recurrence is one of the key components of the management [6]. avoidance is the best if possible. when you order computed tomography scans using rcm, it is important to find out those who had a history of rcm induced hypersensitivity reactions. a clinical decision supporting system embedded in the electronic medical record system may help [7]. if it is impossible to avoid the use of rcm, the preventive measures are used, for example, 50 mg of oral prednisolone at 13 hours, 7 hours and an hour before injection of rcm, and 4 mg of chlorpheniramine at an hour before the injection if a patient has previous history of severe immediate type rcm hypersensitivity [6]. what about mild cases such as simple urticaria? in this issue, lee et al. [8] report the efficacy of single premedication with antihistamines for rcm hypersensitivity.#r##n##r##n#advances in technology are changing the future of medicine. clinical decision supporting systems have been developed to help clinicians make informed management decisions [7,9]. the platform of the clinical decision supporting system has been evolved from simple algorithms to artificial intelligence with neural networking systems. most recently watson, the ibm supercomputer, has collaborated with several cancer care providers to help clinicians make informed treatment decisions [10]. in this issue, kim et al. [11] from korea report the feasibility of a smartphone application based action plan and monitoring in asthma. according to this article, the patients were satisfied with the information from the smartphone application that they've got right on time. it also showed some beneficial effects such as enhanced adherence. this application was developed by a university hospital. as described in the article [11], a study showed most of applications among 103 applications provided inaccurate information but only 32 applications provided strategies for asthma control. thus the role of physicians is very important in the development of medical applications. collaborations between medicine and computer or information science are essential.#r##n##r##n#readers will find an interesting study of a birth cohort from singapore on the relationship between all fevers or fever after vaccination and atopy or atopic diseases at 18 and 36 months of age [12] and a study on the elderly asthma from thailand [13].#r##n##r##n#this issue of asia pacific allergy also contains an educational case report on allergic bronchopulmonary aspergillosis [14] and a unique case of marking nut anaphylaxis [15].
006303c5-df9c-479b-87f9-2a72d67bcca9 interactive modeling software for supervisory control of industrial processes with increased complexity engineering systems are now becoming more and more complex and increasingly dependent on automation, giving rise to a fear that even the smallest error in the system may cause a disaster. among various factors that contribute to the increased system complexity are the intrinsic non-linearity within the system and the scale enlargement. it is understood that even simple non-linearity may lead to complex phenomena such as non-linear oscillation, bifurcation and chaos that are hard to model, predict or control. scale enlargement leads to well-known problems in supervisory control like complicated operators' situation awareness, data overload during system malfunction, etc. in this paper, an interactive modeling method would be proposed to support the extraction of meaningful patterns from large amount of plant data using data mining techniques and the latest grey-box modeling approach. this method would be demonstrated over a simple cstr system.
00630ea1-2442-4ff9-8b5f-8c325d17b0f1 an information-theoretic approach for setting the optimal number of decision trees in random forests data classification is a process within the data mining and machine learning field which aims at annotating all instances of a dataset by so-called class labels. this involves in creating a model from a training set of data instances which are already labeled, possibly being this model also used to define the class of data instances which are not classified already. a successful way of performing the classification process is provided by the algorithm random forests (rf), which is itself a type of ensemble-based classifier. an ensemble-based classifier increases the accuracy of the class label assigned to a data instance by using a set of classifiers that are modeled on different, but possibly overlapping, instance sets, and then combining the so-obtained intermediate classification results. to this end, rf particularly makes use of a number of decision trees to classify an instance, then taking the majority of votes from these trees as the final classifier. the latter one is a critical task of algorithm rf, which heavily impacts on the accuracy of the final classifier. in this paper, we propose a variation of algorithm rf, namely adjusting one of the two parameters that rf takes, the number of decision trees, dependant on a meaningful relation between the dataset predictive power rating and the number of trees itself, with the goal of improving accuracy and performance of the algorithm. this is finally demonstrated by our comprehensive experimental evaluation on several clean datasets.
0063b640-385d-4136-adea-1a82c5b97c27 dropout: a simple way to prevent neural networks from overfitting deep neural nets with a large number of parameters are very powerful machine learning systems. however, overfitting is a serious problem in such networks. large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. dropout is a technique for addressing this problem. the key idea is to randomly drop units (along with their connections) from the neural network during training. this prevents units from co-adapting too much. during training, dropout samples from an exponential number of different "thinned" networks. at test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. this significantly reduces overfitting and gives major improvements over other regularization methods. we show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.
0063d10b-2fbf-47b6-8484-d8b20fbf7bec value, cost, and sharing: open issues in constrained clustering clustering is an important tool for data mining, since it can identify major patterns or trends without any supervision (labeled data). over the past five years, semi-supervised (constrained) clustering methods have become very popular. these methods began with incorporating pairwise constraints and have developed into more general methods that can learn appropriate distance metrics. however, several important open questions have arisen about which constraints are most useful, how they can be actively acquired, and when and how they should be propagated to neighboring points. this position paper describes these open questions and suggests future directions for constrained clustering research.
00641d76-87a1-4a3b-9d27-47a7a3d3f899 distributed stochastic optimization for deep learning (thesis) we study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. we propose a new distributed stochastic optimization method called elastic averaging sgd (easgd). we analyze the convergence rate of the easgd method in the synchronous scenario and compare its stability condition with the existing admm method in the round-robin scheme. an asynchronous and momentum variant of the easgd method is applied to train deep convolutional neural networks for image classification on the cifar and imagenet datasets. our approach accelerates the training and furthermore achieves better test accuracy. it also requires a much smaller amount of communication than other common baseline approaches such as the downpour method. #r##n#we then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch sgd, the momentum sgd, and the easgd methods. we find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. we also find a surprising connection between the momentum sgd and the easgd method with a negative moving average rate. a non-convex case is also studied to understand when easgd can get trapped by a saddle point. #r##n#finally, we scale up the easgd method by using a tree structured network topology. we show empirically its advantage and challenge. we also establish a connection between the easgd and the downpour method with the classical jacobi and the gauss-seidel method, thus unifying a class of distributed stochastic optimization methods.
00642405-0dd2-4349-bb90-41e3aaa9cd79 the framework of knowledge discovery in data scattering using graph mining over the years, frequent itemset discovery algorithms have been used to solve various interesting problems. as data mining techniques are being increasingly applied to non- traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. an alternate way of modeling the objects in these data sets, is to use a graph to mode the database objects. within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs. in this paper we present a computationally efficient algorithm for finding all frequent subgraphs in large graph databases. we evaluated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset. the empirical results show that our algorithm scales linearly with the number of input transactions and it is able to discover frequent subgraphs from a set of graph transactions reasonably fast, even though we have to deal with computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not necessary for traditional frequent itemset discovery.
0064c5ee-998e-481c-a0a8-1797f641cf89 efficient temporal reasoning with uncertainty automated planning is an active area within artificial intelligence. with the help of computers we can quickly find good plans in complicated problem domains, such as planning for search and rescue ...
00650238-745e-4059-9189-0b3351e81f1d a novel rbf training algorithm for short-term electric load forecasting and comparative studies because of their excellent scheduling capabilities, artificial neural networks (anns) are becoming popular in short-term electric power system forecasting, which is essential for ensuring both efficient and reliable operations and full exploitation of electrical energy trading as well. for such a reason, this paper investigates the effectiveness of some of the newest designed algorithms in machine learning to train typical radial basis function (rbf) networks for 24-h electric load forecasting: support vector regression (svr), extreme learning machines (elms), decay rbf neural networks (drnns), improves second order, and error correction, drawing some conclusions useful for practical implementations.
0065e2e9-66b2-4545-bd96-fa8e98ca59ad computing the tree of life: leveraging the power of desktop and service grids the trend in life sciences research, particularly in molecular evolutionary systematics, is toward larger data sets and ever-more detailed evolutionary models, which can generate substantial computational loads. over the past several years we have developed a grid computing system aimed at providing researchers the computational power needed to complete such analyses in a timely manner. our grid system, known as the lattice project, was the first to combine two models of grid computing - the service model, which mainly federates large institutional hpc resources, and the desktop model, which harnesses the power of pcs volunteered by the general public. recently we have developed a "science portal" style web interface that makes it easier than ever for phylogenetic analyses to be completed using garli, a popular program that uses a maximum likelihood method to infer the evolutionary history of organisms on the basis of genetic sequence data. this paper describes our approach to scheduling thousands of garli jobs with diverse requirements to heterogeneous grid resources, which include volunteer computers running boinc software. a key component of this system provides a priori garli runtime estimates using machine learning with random forests.
00665602-0643-4a85-ab21-8bcc934a7e1b efficient local algorithms for distributed data mining in large scale peer to peer environments: a deterministic approach 
0066cc45-de4e-46f3-a455-09a8527e9638 a practical and effective sampling selection strategy for large scale deduplication the data deduplication task has attracted a considerable amount of attention from the research community in order to provide effective and efficient solutions. the information provided by the user to tune the deduplication process is usually represented by a set of manually labeled pairs. in very large datasets, producing this kind of labeled set is a daunting task since it requires an expert to select and label a large number of informative pairs. in this article, we propose a two-stage sampling selection strategy (t3s) that selects a reduced set of pairs to tune the deduplication process in large datasets. t3s selects the most representative pairs by following two stages. in the first stage, we propose a strategy to produce balanced subsets of candidate pairs for labeling. in the second stage, an active selection is incrementally invoked to remove the redundant pairs in the subsets created in the first stage in order to produce an even smaller and more informative training set. this training set is effectively used both to identify where the most ambiguous pairs lie and to configure the classification approaches. our evaluation shows that t3s is able to reduce the labeling effort substantially while achieving a competitive or superior matching quality when compared with state-of-the-art deduplication methods in large datasets.
00675cde-e4a4-4f70-9ede-d8ddffa6d68a a comparison between data mining methods in anticipation of financial distress and improvement of anticipation through combination of methods - ti journals abstract: nowadays, many studies have been conducted on corporate financial distress anticipation using data mining techniques. artificial neural networks, support vector machine and decision tree algorithms are three current methods for data mining to anticipate corporate financial distress. this study compares the anticipation accuracy of these three methods in anticipation of corporate financial distress. the effect of combining these three methods is also studied through relative majority voting method to improve anticipation of corporate financial distress. #r##n#statistical population of this study includes 100 sound companies and 100 distressed companies, active in tehran stock exchange market between 2005 and 2011, which were studied for the two years of t and t-1. findings of the study show that decision tree with 95.95% accuracy for the year t and artificial neural network with 89.92% accuracy for the year t-1 have the highest level of efficiency to anticipate corporate financial distress. the results also show that combination of relative majority voting with 93.89% accuracy in the year t and 89.89% accuracy in the yea t-1 is able to anticipate corporate financial distress.
006794f9-ab98-451a-afa0-06b2765ee43a telehealth innovations in health education and training abstract telehealth applications are increasingly important in many areas of health education and training. in addition, they will play a vital role in biomedical research and research training by facilitating remote collaborations and providing access to expensive/remote instrumentation. in order to fulfill their true potential to leverage education, training, and research activities, innovations in telehealth applications should be fostered across a range of technology fronts, including online, on-demand computational models for simulation; simplified interfaces for software and hardware; software frameworks for simulations; portable telepresence systems; artificial intelligence applications to be applied when simulated human patients are not options; and the development of more simulator applications. this article presents the results of discussion on potential areas of future development, barries to overcome, and suggestions to translate the promise of telehealth applications into a transformed enviro...
0067e7bb-58d6-4b52-b50b-1c42f4a75cfc a parallel algorithm to compute data synopsis business intelligence systems are based on traditional olap, data mining, and approximate query processing. generally, these activities allow to extract information and knowledge from large volumes of data and to support decisional makers as concerns strategic choices to be taken in order to improve the business processes of the information system. among these, only approximate query processing deals with the issue of reducing response time, as it aims to provide fast query answers affected with a tolerable quantity of error. however, this kind of processing needs to pre-compute a synopsis of the data stored in the data warehouse. in this paper, a parallel algorithm for the computation of data synopses is presented.
00685459-e3f9-418b-9c74-35c700f242fc object oriented models vs. data analysis  is this the right alternative? we review the role of mathematics from a historical and a conceptual perspective in the light of modern data science.
00686a4c-9370-453a-a25e-6f8415bb3dcb extensions to metric based model selection metric-based methods have recently been introduced for model selection and regularization, often yielding very significant improvements over the alternatives tried (including cross-validation). all these methods require unlabeled data over which to compare functions and detect gross differences in behavior away from the training points. we introduce three new extensions of the metric model selection methods and apply them to feature selection. the first extension takes advantage of the particular case of time-series data in which the task involves prediction with a horizon h. the idea is to use at t the h unlabeled examples that precede t for model selection. the second extension takes advantage of the different error distributions of cross-validation and the metric methods: cross-validation tends to have a larger variance and is unbiased. a hybrid combining the two model selection methods is rarely beaten by any of the two methods. the third extension deals with the case when unlabeled data is not available at all, using an estimated input density. experiments are described to study these extensions in the context of capacity control and feature subset selection.
00691a5c-214b-45a0-b6f2-e15151bfffde towards a near infrared spectroscopy-based estimation of operator attentional state. given the critical risks to public health and safety that can involve lapses in attention (e.g., through implication in workplace accidents), researchers have sought to develop cognitive-state tracking technologies, capable of alerting individuals engaged in cognitively demanding tasks of potentially dangerous decrements in their levels of attention. the purpose of the present study was to address this issue through an investigation of the reliability of optical measures of cortical correlates of attention in conjunction with machine learning techniques to distinguish between states of full attention and states characterized by reduced attention capacity during a sustained attention task. seven subjects engaged in a 30 minutes duration sustained attention reaction time task with near infrared spectroscopy (nirs) monitoring over the prefrontal and the right parietal areas. nirs signals from the first 10 minutes of the task were considered as characterizing the full attention class, while the nirs signals from the last 10 minutes of the task were considered as characterizing the attention decrement class. a two-class support vector machine algorithm was exploited to distinguish between the two levels of attention using appropriate nirs-derived signal features. attention decrement occurred during the task as revealed by the significant increase in reaction time in the last 10 compared to the first 10 minutes of the task (p<.05). the results demonstrate relatively good classification accuracy, ranging from 65 to 90%. the highest classification accuracy results were obtained when exploiting the oxyhemoglobin signals (i.e., from 77 to 89%, depending on the cortical area considered) rather than the deoxyhemoglobin signals (i.e., from 65 to 66%). moreover, the classification accuracy increased to 90% when using signals from the right parietal area rather than from the prefrontal cortex. the results support the feasibility of developing cognitive tracking technologies using nirs and machine learning techniques.
00697336-aa76-4b8f-9348-508e9af685c8 microrna expression profiling in prostate cancer micrornas (mirna) are small, endogenously expressed noncoding rnas that negatively regulate expression of protein-coding genes at the translational level. accumulating evidence, such as aberrant expression of mirnas, suggests that they are involved in the development of cancer. they have been identified in various tumor types, showing that different sets of mirnas are usually deregulated in different cancers. to identify the mirna signature specific for prostate cancer, mirna expression profiling of 6 prostate cancer cell lines, 9 prostate cancer xenografts samples, 4 benign prostatic hyperplasia (bph), and 9 prostate carcinoma samples was carried out by using an oligonucleotide array hybridization method. differential expression of 51 individual mirnas between benign tumors and carcinoma tumors was detected, 37 of them showing down-regulation and 14 up-regulation in carcinoma samples, thus identifying those mirnas that could be significant in prostate cancer development and/or growth. there was a significant trend ( p = 0.029) between the expression of mirnas and mirna locus copy number determined by array comparative genomic hybridization, indicating that genetic aberrations may target mirnas. hierarchical clustering of the tumor samples by their mirna expression accurately separated the carcinomas from the bph samples and also further classified the carcinoma tumors according to their androgen dependence (hormone naive versus hormone refractory), indicating the potential of mirnas as a novel diagnostic and prognostic tool for prostate cancer. [cancer res 2007;67(13):61305]
006a4cbd-fcc0-4e21-a9fa-69eb750bf816 geospatial visualization using google maps: a case study on conference presenters the integration of web services with gis has recently attracted much attention. free web mapping services providers such as google maps and microsoft virtual earth have become very popular. they use large distributed systems for information sharing on the internet based on geospatial location. also they provide their web apis for professional developers. in this paper, a web mapping application based on conference presenters' geographic locations using google maps is presented. it aims to demonstrate advanced features that current web mapping services can provide for location-based decision making. first system overview is presented. then geographic location mapping methodologies including data mining, map feature generation and georss conversion are addressed. finally geospatial visualization of the conference presenters as a case study is demonstrated.
006ab39a-f581-4f3d-8706-5b897e67e18a an improved fuzzy based approach to impute missing values in dna microarray gene expression data with collaborative filtering dna microarray experiments normally generate gene expression profiles in the form of high dimensional matrices. it may happen that dna microarray gene expression values contain many missing values within its data due to several reasons like image disruption, hybridization error, dust, moderate resolution etc. it will be very unfortunate if these missing values affect the performance of subsequent statistical and machine learning experiments significantly. there exist various missing value estimation algorithms. in this work we have proposed a modification to the existing imputation approach named as collaborative filtering based on rough-set theory (cfbrst) [10]. this proposed approach (cfbrstfdv) uses fuzzy difference vector (fdv) along with rough set based collaborative filtering that analyzes historical interactions and helps to estimate the missing values. this is a suggestion based system that works on the principle of how suggestion of items or products arrive to an individual while using fb, twitter or looking for books in amazon. we have applied our proposed algorithm on two benchmark dataset spellman & tumor cell (gds2932) and the experiments show that the modified approach, cfbrstfdv, outperforms the other existing state-of-the art methods as far as rmse measures are concerned, particularly when we increase the number of missing values.
006b199b-fe26-4a79-a79f-33d21eabbe6f one-class support vector machines: analysis of outlier detection for wireless sensor networks in harsh environments machine learning, like its various applications, has received a great interest in outlier detection in wireless sensor networks. support vector machines (svm) are a special type of machine learning techniques which are computationally inexpensive and provide a sparse solution. this work presents a detailed analysis of various formulations of one-class svms, like, hyper-plane, hyper-sphere, quarter-sphere and hyper-ellipsoidal. these formulations are used to separate the normal data from anomalous data. various techniques based on these formulations have been analyzed in terms of a number of characteristics for harsh environments. these characteristics include input data type, spatio-temporal and attribute correlations, user specified thresholds, outlier types, outlier identification(event/error), outlier degree, susceptibility to dynamic topology, non-stationarity and inhomogeneity. a tabular description of improvement and feasibility of various techniques for deployment in the harsh environments has also been presented.
006b23ef-b21e-4a5b-b74b-4e6796b7f1c0 extracting subjective and objective evaluative expressions from the web there are various opinions on the web, and analyzing them is an important task. although many previous studies focused on analyzing subjective evaluative expressions, objective evaluative expressions which describe positive or negative facts are also informative information. in this paper, we study extraction and classification of subjective and objective evaluative expressions on japanese web documents using machine learning and evaluative word dictionaries.
006bc093-ed6b-49bf-bca9-8d65b79a7b1e assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease relation (cdr) task. manually curating chemicals, diseases and their relationships is significantly important to biomedical research, but it is plagued by its high cost and the rapid growth of the biomedical literature. in recent years, there has been a growing interest in developing computational approaches for automatic chemical-disease relation (cdr) extraction. despite these attempts, the lack of a comprehensive benchmarking dataset has limited the comparison of different techniques in order to assess and advance the current state-of-the-art. to this end, we organized a challenge task through biocreative v to automatically extract cdrs from the literature. we designed two challenge tasks: disease named entity recognition (dner) and chemicalinduced disease (cid) relation extraction. to assist system development and assessment, we created a large annotated text corpus that consisted of human annotations of chemicals, diseases and their interactions from 1500 pubmed articles. 34 teams worldwide participated in the cdr task: 16 (dner) and 18 (cid). the best systems achieved an f-score of 86.46% for the dner taska result that approaches the human inter-annotator agreement (0.8875)and an f-score of 57.03% for the cid task, the highest results ever reported for such tasks. when combining team results via machine learning, the ensemble system was able to further improve over the best team results by achieving 88.89% and 62.80% in f-score for the dner and cid task, respectively. additionally, another novel aspect of our evaluation is to test each participating systems ability to return real-time results: the average response time for each teams dner and cid web service systems were 5.6 and 9.3 s, respectively. most teams used hybrid systems for their submissions based on machining learning. given the level of participation
006c09b3-c836-42bb-bbf1-94745b9fde47 the omssapercolator: an automated tool to validate omssa results protein identification by ms/ms is an important technique in proteome studies. the open mass spectrometry search algorithm (omssa) is an open-source search engine that can be used to identify ms/ms spectra acquired in these experiments. here, we present a software tool, termed omssapercolator, which interfaces omssa with percolator, a post-search machine learning method for rescoring database search results. we demonstrate that it outperforms the standard omssa scoring scheme, and provides reliable significant measurements. omssapercolator is programmed using java and can be readily used as a standalone tool or integrated into existing data analysis pipelines. omssapercolator is freely available and can be downloaded at http://sourceforge.net/projects/omssapercolator/.
006c84a7-3adc-4823-8305-3f82855fa3f5 optimization of process conditions in casting aluminum matrix composites via interconnection of artificial neurons and progressive solutions a genetic algorithm is a machine learning technique that was inspired by the analogy of biological evolution which generates solutions by repeatedly mutating and recombining parts of the best currently known solutions. in order to model and optimize the properties of a356 matrix composites, a finite element method (fem) with artificial neural network based genetic algorithm (ann-ga) model was developed. the tribological and mechanical properties of the aluminum matrix composite were also experimentally investigated. the results verified the accuracy of the proposed model to find the optimal process conditions in aluminum matrix composite materials.
006ce46d-d9a2-44e9-8586-f9d2a45cb065 self-organising maps in document classification: a comparison with six machine learning methods this paper focuses on the use of self-organising maps, also known as kohonen maps, for the classification task of text documents. the aim is to effectively and automatically classify documents to separate classes based on their topics. the classification with self-organising map was tested with three data sets and the results were then compared to those of six well known baseline methods: k-means clustering, ward's clustering, k nearest neighbour searching, discriminant analysis, naive bayes classifier and classification tree. the self-organising map proved to be yielding the highest accuracies of tested unsupervised methods in classification of the reuters news collection and the spanish clef 2003 news collection, and comparable accuracies against some of the supervised methods in all three data sets.
006ed317-4314-4629-afda-f6ee43c3b5d4 an ethnocentric look at the law and technology interface we often conduct ethnocentric research, observing everyday citizen end-users in their own environments to assess their access, technological and policy concerns. our recent research has revealed that many end-user concerns are on the interface of law and technology. such interface issues also have become apparent throughout our own interactions with software and the web, as well as within our specific research into ecommerce; link analysis and data mining; and the semantic web applied to egovernment and homeland security. among the relevant areas we now address are information security and integrity; barriers (intentional or otherwise) to access; security-privacy trade-offs; the need for user education; and importance of cooperation among stakeholders, policymakers and experts in technology-related law. we illustrate that in many policy areas of evolving law related to technology there may be no general decision procedure; many decisions can only be made case-by-case. we strongly support ethnocentric studies by practitioners of software engineering, experts in technology law and policymakers. this can identify specific critical deployment issues that might otherwise be overlooked within traditional research venues. we believe that many end-user concerns on the law and technology interface, once recognized, can be alleviated by software engineering professionals.
006fb0e6-d2bf-460e-8821-80255981582b prosody prediction from linguistically enriched documents based on a machine learning approach one of the main aspects in text-to-speech synthesis is the successful prediction of prosodic events. in this work we deal with the prediction of prosodic phrase breaks, accent tones and boundary tones from a linguistically xml-based enriched input (sole-ml) produced by a natural language generator (nlg) system. we first extended the original specification of sole-ml in order for the nlg to produce a more spoken aware output providing evidence of stress and intonational focus. we then used a machine learning approach (cart) to statistically analyze documents as sequences of part-of-speech (pos), already given or new information, object-subject information and other domain features, in order to predict prosodic phrase breaks, accent tones and boundary tones. we applied this approach on a specific domain of greek descriptions of museum exhibits. an important task of this work was the optimization of the set of features used for training, after which the correlation between the observed and the predicted aforementioned prosodic elements became 97,72%, 96,77% and 100,00% respectively. the large amount (48.03%) of untagged text in the above corpus shows that the produced trained models can be applied to plain text of the same domain as well with success.
006fd2c9-a757-47ba-b861-569d925478a0 high-dimensional similarity joins many emerging data mining applications require a similarity join between points in a high-dimensional domain. we present a new algorithm that utilizes a new index structure, called the /spl epsi/ tree, for fast spatial similarity joins on high-dimensional points. this index structure reduces the number of neighboring leaf nodes that are considered for the join test, as well as the traversal cost of finding appropriate branches in the internal nodes. the storage cost for internal nodes is independent of the number of dimensions. hence, the proposed index structure scales to high-dimensional data. we analyze the cost of the join for the /spl epsi/ tree and the r-tree family, and show that the /spl epsi/ tree will perform better for high-dimensional joins. empirical evaluation, using synthetic and real-life data sets, shows that similarity join using the /spl epsi/ tree is twice to an order of magnitude faster than the r/sup +/ tree, with the performance gap increasing with the number of dimensions. we also discuss how some of the ideas of the /spl epsi/ tree can be applied to the r-tree family. these biased r-trees perform better than the corresponding traditional r-trees for high-dimensional similarity joins, but do not match the performance of the /spl epsi/ tree.
006fd90c-00b3-4e1c-ac54-375f4a0542ba serious games and gaming 
007018e6-4bfd-4451-aad3-bbadd52650e2 teaching a data mining course to mba students 
007047bd-9856-47e3-a288-b68bddf36f0f the winning robots from the 1993 robot competition the second annual robot competition and exhibition sponsored by the association for the advancement of artificial intelligence was held in washington d.c. on 13-15 july 1993 in conjunction with the eleventh national conference on artificial intelligence. this article describes the robots that placed first and second in each event and compares their strategies and their resulting successes and difficulties.
0073483c-387a-413d-842a-5138d20558d1 a method of two-stage clustering with constraints using agglomerative hierarchical algorithm and one-pass k-means the aim of this paper is to propose a new method of two-stage clustering with constraints using agglomerative hierarchical algorithm and one-pass k-means. an agglomerative hierarchical algorithm has a larger computational complexity than non-hierarchical algorithm. it takes much time to execute agglomerative hierarchical algorithm, and sometimes, agglomerative hierarchical algorithm cannot be executed. in order to handle a large-scale data by an agglomerative hierarchical algorithm, the present method is proposed. the method is divided into two stages. in the first stage, a method of one-pass k-means is carried out. the difference between k-means and one-pass k-means is that the former uses iterations, while the latter not. small clusters obtained from this stage are merged using agglomerative hierarchical algorithm in the second stage. in order to improve correctness of clustering, pairwise constraints are included. to show effectiveness of the proposed method, numerical examples are given.
0073735e-a92f-4a80-b196-9252ce686427 neuro-fuzzy systems modeling tools for bacterial growth many techniques have been used in classification of bacterial growth-non-growth database are network based. this paper proposes adaptive neuro-fuzzy system for classifying the bacterial growth/non-growth and modeling the growth history. a brief description of the neuro-fuzzy intelligent systems scheme is proposed. the performance of neuro-fuzzy system is investigated for their quality and accuracy in classification of growth/no-growth state of a pathogenic escherichia coli r31 in response to temperature and water activity. a comparison with the most common used statistics and data mining classifiers was carried out. the neuro-fuzzy system classifier was found to do better than both linear/nonlinear regression and multilayer neural networks. results show bright future in implementing it in food science and medical industry.
00741eb9-cc11-42b2-bddf-bf3cb129276f automating image segmentation verification and validation by learning test oracles an image segmentation algorithm delineates (an) object(s) of interest in an image. its output is referred to as a segmentation. developing these algorithms is a manual, iterative process involving repetitive verification and validation tasks. this process is time-consuming and depends on the availability of experts, who may be a scarce resource (e.g., medical experts). we propose a framework referred to as image segmentation automated oracle (isao) that uses machine learning to construct an oracle, which can then be used to automatically verify the correctness of image segmentations, thus saving substantial resources and making the image segmentation verification and validation task significantly more efficient. the framework also gives informative feedback to the developer as the segmentation algorithm evolves and provides a systematic means of testing different parametric configurations of the algorithm. during the initial learning phase, segmentations from the first few (optimally two) versions of the segmentation algorithm are manually verified by experts. the similarity of successive segmentations of the same images is also measured in various ways. this information is then fed to a machine learning algorithm to construct a classifier that distinguishes between consistent and inconsistent segmentation pairs (as determined by an expert) based on the values of the similarity measures associated with each segmentation pair. once the accuracy of the classifier is deemed satisfactory to support a consistency determination, the classifier is then used to determine whether the segmentations that are produced by subsequent versions of the algorithm under test, are (in)consistent with already verified segmentations from previous versions. this information is then used to automatically draw conclusions about the correctness of the segmentations. we have successfully applied this approach to 3d segmentations of the cardiac left ventricle obtained from ct scans and have obtained promising results (accuracies of 95%). even though more experiments are needed to quantify the effectiveness of the approach in real-world applications, isao shows promise in increasing the quality and testing efficiency of image segmentation algorithms.
00746968-1e3a-4043-abbd-3007d9135880 rses and rseslib - a collection of tools for rough set computations rough set exploration system - a set of software tools featuring a library of methods and a graphical user interface is presented. methods, features and abilities of the implemented software are discussed and illustrated with a case study in data analysis.
0074d658-084b-4898-8670-f3c0d70e3d24 the sustainability of a community pharmacy intervention to improve the quality use of asthma medication summary what is known and objective: a previously published asthma intervention used a software application to data mine pharmacy dispensing records and generate a list of patients with potentially suboptimal management of their asthma; in particular, a high rate of provision of reliever medication. these patients were sent educational material from their community pharmacists and advised to seek a review of their asthma management from their general practitioner. the intervention resulted in a 3-fold improvement in the ratio of dispensed preventer medication (inhaled corticosteroids) to reliever medication (short-acting beta-2 agonists). this follow-up study aimed to determine the longterm effects of the intervention programme on the preventer-to-reliever (p:r) ratio. methods: the same data mining software was modified so that it could re-identify patients who were originally targeted for the intervention. community pharmacists who participated in the previous intervention installed the modified version of the software. the dispensing data were then de-identified, encrypted and transferred via the internet to a secure server. the follow-up dispensing data for all patients were compared with their pre- and post-intervention data collected originally. results and discussion: of the 1551 patients who were included in the original study, 718 (46ae3%) wereeligibletobeincludedinthefollow-upstudy. the improved p:r ratio was sustained for at least 12 months following the intervention (p <0 ae01). the sustained increase in the p:r ratio was attributed to significant decreases in the average daily usage of reliever medication (p <0 ae0001). what is new and conclusion: the follow-up study demonstrated a sustained improvement in the ratio of dispensed preventer medication to reliever medication for asthma. the intervention has the potential to show long-lasting and widespread improvements in asthma management, improved health outcomes for patients, and ultimately, a reduced burden on the health system.
0075bc83-54ae-468b-9914-2d32be6d2439 identification of host-microbe interaction factors in the genomes of soft rot-associated pathogens dickeya dadantii 3937 and pectobacterium carotovorum wpp14 with supervised machine learning background#r##n#a wealth of genome sequences has provided thousands of genes of unknown function, but identification of functions for the large numbers of hypothetical genes in phytopathogens remains a challenge that impacts all research on plant-microbe interactions. decades of research on the molecular basis of pathogenesis focused on a limited number of factors associated with long-known host-microbe interaction systems, providing limited direction into this challenge. computational approaches to identify virulence genes often rely on two strategies: searching for sequence similarity to known host-microbe interaction factors from other organisms, and identifying islands of genes that discriminate between pathogens of one type and closely related non-pathogens or pathogens of a different type. the former is limited to known genes, excluding vast collections of genes of unknown function found in every genome. the latter lacks specificity, since many genes in genomic islands have little to do with host-interaction.
0076285e-c6a7-4a30-b497-1ed2b296e775 pac-mdl bounds we point out that a number of standard sample complexity bounds (vc-dimension, pac-bayes, and others) are all related to the number of bits required to communicate the labels given the unlabeled data for a natural communication game. motivated by this observation, we give a general sample complexity bound based on this game that allows us to unify these different bounds in one common framework.
00766547-5b16-4774-87c0-3ee23637f04a uncertainty reasoning on fuzziness and randomness in challenged networks links in challenged networks, particularly in delay tolerance networks, are mostly disconnected due to the mobility, fallibility and scarcity of nodes. owing to its intermittent connectivity, there exists substantive uncertainty, in which fuzziness and randomness are the most important and fundamental. in this paper, in virtue of the relationship analysis between fuzziness and randomness, we discuss the uncertainty in challenged networks. an approach is proposed to describe the uncertainty reasoning by means of the intrinsically uncertainty on fuzzy reasoning and modeling the uncertain states on probability. certainty theory supports the protocol design in a flexible and adaptive manner. modeling associates fuzziness with randomness is a cross-discipline which provides a basis for the artificial intelligence of both logic and image thinking with uncertainty.
0078445a-1b60-4bd6-aa71-233081640c05 learning from annotated video: an initial study based on oyama karate tournament recordings the most of the up-to-date video data are amateur films consisted of an unbroken sequence of frames recorded from a single camera. in case of karate many even most important world tournaments do not have video broadcasting and the multimedia materials are mostly unprofessional. this type of recording is very difficult to analyze with automatic annotation methods. it is due the fact that a cameraman films the whole fight from stationary position. only with a help of experienced specialists it is possible to distinguish the karate techniques fighters perform. due to this fact annotations have to be done mostly manually. we have observed that none of the state of the art papers deal with problem of martial-arts annotated video analysis. it seems that this kind of data might be a very good source of knowledge about a kumite (a fight), a particular fighter and his or her strategies and abilities. in this paper we propose a novel video annotation method that enables both quantitative (numerical) and qualitative (categorical) features calculation. we also present and discuss example results we can obtain from those descriptions with popular data mining methods.
00796fa2-41ba-4f2b-bf57-c300334307b4 reconocedor de nombres de entidades para el espanol basado en expresiones regulares y aprendizaje automatico normal   0   21       false   false   false                 microsoftinternetexplorer4                     #r##n# /* style definitions */#r##n# table.msonormaltable#r##n##tab#{mso-style-name:"tabla normal";#r##n##tab#mso-tstyle-rowband-size:0;#r##n##tab#mso-tstyle-colband-size:0;#r##n##tab#mso-style-noshow:yes;#r##n##tab#mso-style-parent:"";#r##n##tab#mso-padding-alt:0pt 5.4pt 0pt 5.4pt;#r##n##tab#mso-para-margin:0pt;#r##n##tab#mso-para-margin-bottom:.0001pt;#r##n##tab#mso-pagination:widow-orphan;#r##n##tab#font-size:10.0pt;#r##n##tab#font-family:"times new roman";#r##n##tab#mso-ansi-language:#0400;#r##n##tab#mso-fareast-language:#0400;#r##n##tab#mso-bidi-language:#0400;}#r##n#      en este trabajo se presenta un reconocedor de nombres de entidades para documentos en espanol. este constituye la primera de una serie de herramientas computacionales desarrolladas con el objetivo de procesar los grandes volumenes de informacion textual disponibles en internet y extraer la informacion presente en ellos de manera automatica. en la herramienta se combinaron un conjunto de expresiones regulares y modelos de aprendizaje automatico. se realizaron experimentos sobre un conjunto de documentos compuesto por noticias de la prensa cubana, en los cuales se obtuvieron resultados satisfactorios.           in this work we present a named entity recognizer for spanish documents. this is the first of a series of computational tools developed in order to process the huge volumes of textual information available on the internet and to automatically extract the information contained in them. in this tool, a set of regular expressions and machine learning models were combined. experiments were carried out on a set of documents composed by news items from the cuban press, in which satisfactory results were achieved.
0079f25d-eea5-4e4d-9d39-91c6a75faa2a computational intelligent strategies to predict energy conservation benefits in excess air controlled gas-fired systems gas-fired systems and boilers are the noteworthy energy consumers in industries for energy production. the mostly applied factor in systems which contain boilers is the combustion efficiency. this parameter has widely applied to determine important information about exhaust gas such as co2 and o2. this study plays importance on applying the predictive techniques based on the adaptive neuro-fuzzy inference system (anfis), multi-layer perceptron artificial neural network (mlpann), support vector machine (svm), and simple correlation for predicting the combustion efficiency of natural gas at extensive range of excess air fraction and stack temperature rise. the developed tools can be of great assessment for engineers dealing with combustion to have a rapid check on combustion efficiency of natural gas at broad range of applications without the requirement of any unit as pilot plant. the levenbergmarquardt algorithm is employed to optimize the bias and weight values of the ann model. in addition, hybrid algorithm (coupling of back propagation and least square methods) is used to determine parameters of membership function in the anfis approach. hyper variables of the svm technique (i.e., c, k and e) are determined using tried and error procedures. to that end, a database including 72 data points has been collected from the energy management handbook. approximations are recognized to be in high approbation with reported data points. furthermore, potential of aforementioned models has been evaluated through statistical analyses. the developed models were examined using several data, and a reasonable match was attained showing a good potential for the proposed predictive tools in estimation of the combustion efficiency of natural gas at extensive range of excess air fraction and stack temperature rise.
007a60f7-5460-4bad-b38f-6b1d9722d0b8 content independent metadata production as a machine learning problem metadata provide a high-level description of digital library resources and represent the key to enable the discovery and selection of suitable resources. however the growth in size and diversity of digital collections makes manual metadata extraction an expensive task. this paper proposes a new content independent method to automatically generate metadata in order to characterize resources in a given learning objects repository. the key idea is to rely on few existing metadata to learn predictive models of metadata values. the proposed method is content independent and handles resources in different formats: text, image, video, java applet, etc.#r##n##r##n#two classical machine learning approaches are studied in this paper: in the first approach a supervised machine learning technique classify each value of a metadata field to be predicted according to the other a-priori filled metadata fields. the second approach used the fp-growth algorithm to discover relationships between the different metadata fields as association rules. experiments on two well-known educational data repositories show that both approaches can enhance metadata extraction and can even fill subjective metadata fields that are difficult to extract from the content of a resource, such as the difficulty of a resource.
007b3302-b72d-4a24-a379-bcc4129d2fcf towards ethical aspects on artificial intelligence this paper presents the role of ethics in developing artificial intelligence, and how the artificial intelligence could change our perspective, because artificial intelligence in fact is all around us. artificial intelligence is an important part of our life, but we are sure that the possibility of acquiring the domination of ai over the humanity is only a myth. during the time, the progress helps society but also brings a number of ethical problems. in the academic society, like in real life, the process of using different kind of power are complex, and even if it is about the robots, the computer or other artificial intelligence tools, the ethical problems are not only theoretical but also practical, it is not only a concept, but it is also a practical support for our life.
007c6b7d-6c05-4d92-bdda-923e7162267a a multi-objective cluster algorithm based on gep clustering is one of the main methods in data mining. many clustering algorithms have been proposed so far. among them, gep-cluster, a single-objective clustering algorithm, can automatically cluster with unknown clustering number. however, it is difficult for gep-cluster to find the high-quality solution in the limited search space. aiming at the problems, a multi-objective clustering algorithm based on gene expression programming, mogep-cluster, is proposed in this paper. to validate the effectiveness of mogep-cluster, a set of experiments are performed on 5 benchmark datasets. the experimental results show that mogep-cluster can find better solutions than gep-cluster.
007c729f-f257-4c1c-8604-17d24973495f bibliomining on north south university library data bibliomining is a process that is based on applying data mining techniques on vast amounts of library data to extract valuable behavioral patterns that would aid in decision making or greater efficiency of service [1]. much of this evolved from the more recognized, market basket analysis (mba), primarily used in shopping malls for analysis of product associations. the paper discusses various stages and techniques used to mine the north south university (nsu) library data and extract useful patterns among the borrowers. the findings and results of this work can be used to accomplish more efficient management and budget allocation of the library.
007ce5b2-c284-4b30-bf30-87d88443f925 research on gas explosion idss in coalmines based on cbr the coalmine gas hazard affected safety of china coalmine production most. as there were lots of complicated and uncertain factors during gas accidents emergency rescue and making decisions process, accurate decisions can not be drawn by mathematics models with present information monitored by the kj series monitoring systems such as kj90, so experiences of former questions were urgently required to help as assistant decision-making. accordingly, the construction of gas explosion intelligent decision support systems (idss) based on an artificial intelligence method cba (case-based reasoning) was put forward. the paper instructed the working flow of cbr as well as main structure of gas explosion early warning intelligent decision support systems, and then it designed representation methods of gas explosion cases, arithmetic of cases retrieval and cases learning process. it had great significance of preventing gas explosion accidents in coalmines as well as providing intelligent decision-making support for emergency rescue.
007d18b5-a1b4-4a72-b5a3-4d594d01be24 systems and methods for providing personal video services systems and methods for processing video are provided. video compression schemes are provided to reduce the number of bits required to store and transmit digital media in video conferencing or videoblogging applications. a photorealistic avatar representation of a video conference participant is created. the avatar representation can be based on portions of a video stream that depict the conference participant. a face detector is used to identify, track and classify the face. object models including density, structure, deformation, appearance and illumination models are created based on the detected face. an object based video compression algorithm, which uses machine learning face detection techniques, creates the photorealistic avatar representation from parameters derived from the density, structure, deformation, appearance and illumination models.
007d4be1-21a7-49eb-93ff-84a9417114ba error analysis of fisheye correction curve various types of image can be captured with fisheye lens, their wide field of view is particularly suited to a stereo vision. however, fisheye lens introduces distortion, this change makes the image difficult to identify. if a correction curve can be fitted to the distortion shape, the degree of distortion can be described, support vector machine is a popular machine learning method for classification and regression, support vector classification can find optimal interval of the different classified data, the classified curve is as the correction curve, the training data is obtained by corner detection, the center of the data is gained by hough transform. although ignoring the error in the process of the algorithm, but the error is still existed during the picture taken from a fisheye lens in different situation, for example angle or distance, then using the support vector regression to gain the error between the original data and predicted data, original data stem from training target which is also obtained by a fisheye lens and predicted data stem from another fisheye image.
007d4e56-b62a-4eb5-9909-f552826c3136 evolution of new warm using likert weight measures(lwm) summary the field of data mining draws upon several roots, including statistics, machine learning, databases and high performance computing. supplier selection is an important process which needs more expertise to select a supplier as the technology complexity has increased. frequently as there is a change in the market it will be better if flexibility is maintained. choosing the right method for supplier selection effectively leads to a reduction in purchase risk and increases the number of jit suppliers and tqm production. ahp is a widely accepted multi criteria decision making model, which is suitable for supplier selection process. but ahp is required high computation power. in order to reduce more computation power, in this paper we introduced a new model called likert weight meaure (lwm), which is considered to be a light weight supplier selection model. likert model is globally accepted scaling factor for psychometric feedback.
007d5d29-3edc-4c81-912d-193c80da1662 development of crash modification factors for changing lane width on roadway segments using generalized nonlinear models this study evaluates the effectiveness of changing lane width in reducing crashes on roadway segments. to consider nonlinear relationships between crash rate and lane width, the study develops generalized nonlinear models (gnms) using 3-years crash records and road geometry data collected for all roadway segments in florida. the study also estimates various crash modification factors (cmfs) for different ranges of lane width based on the results of the gnms. it was found that the crash rate was highest for 12-ft lane and lower for the lane width less than or greater than 12ft. gnms can extrapolate this nonlinear continuous effect of lane width and estimate the cmfs for any lane width, not only selected lane widths, unlike generalized linear models (glms) with categorical variables. the cmfs estimated using gnms reflect that crashes are less likely to occur for narrower lanes if the lane width is less than 12ft whereas crashes are less likely to occur for wider lanes if the lane width is greater than 12ft. however, these effects varied with the posted speed limits as the effect of interaction between lane width and speed limit was significant. the estimated cmfs show that crashes are less likely to occur for lane widths less than 12ft than the lane widths greater than 12ft if the speed limit is higher than or equal to 40mph. it was also found from the cmfs that crashes at higher severity levels (kabc and kab) are less likely to occur for lane widths greater or less than 12ft compared to 12-ft lane. the study demonstrates that the cmfs estimated using gnms clearly reflect variations in crashes with lane width, which cannot be captured by the cmfs estimated using glms. thus, it is recommended that if the relationship between crash rate and lane width is nonlinear, the cmfs are estimated using gnms. language: en
007d9d3d-0220-4d4a-9e5b-d872dc8a1eaf intellectual properties data mining over internet this research is based on needs from enterprises spread in each different area of expertise. no matter car manufacture, government, high-tech area, media/publication, research institute, academics and other areas, people have the desire of knowing what the top research and discovery today is. for sure, industrial company would not think the same as academic organizations while they both see the same report about latest intellectual properties. however, it is undeniable to conclude that knowledge of newest intellectual properties is so important for company that wants to dominate the market in next generation, academics that wants to win prizes in coming year and so on. and related researches in this area are many, such as 'sameer singh, maneesha singh, chid apte, petra perner' [1], 'xue li, shuliang wang, zhao yang dong'[2], 'petra perner, atsushi imiya' [3] and so on. this activity of research must have potential of helping any desires of data mining over internet in any area in the future.
007da002-17e2-4f60-b288-8e63ee1091df linguistic analysis of a blog from a murder-suicide. an analysis of the blog of a murder-suicide showed no similarities to previous analyses of the diary of a suicide. language: en
007ddc31-4eea-4e7e-aae0-1a0da4899a70 identifying and solving optimization problems on internet model design and implementation by means of computer technology and methods has been a focus of multiple researches in artificial intelligence, operations research, decision support and management systems at the last years. at the present paper, it is proposed a new approach based on application of systems approach, knowledge based systems, case based reasoning, and e-learning in model building. here, the problems associated to construction of optimization models by means of computer are discussed from theoretical and practical point of views. finally, a new hypermedia intelligent system on internet is proposed. it has as objectives to help learners (professional and students) in model building and implementation. taking into account this conception a new e-learning platform for operations research's teaching and learning was developed. the system has been implemented in net technology using c# computer language and web services. at the present time, it has been testing and improving at havana institute of technology.
007e364d-c992-4224-b978-26c6ca918e85 efficient parallel learning of hidden markov chain models on smps quad-core cpus have been a common desktop configuration for today's office. the increasing number of processors on a single chip opens new opportunity for parallel computing. our goal is to make use of the multi-core as well as multi-processor architectures to speed up large-scale data mining algorithms. in this paper, we present a general parallel learning framework, cut-and-stitch, for training hidden markov chain models. particularly, we propose two model-specific variants, cas-lds for learning linear dynamical systems (lds) and cas-hmm for learning hidden markov models (hmm). our main contribution is a novel method to handle the data dependencies due to the chain structure of hidden variables, so as to parallelize the em-based parameter learning algorithm. we implement cas-lds and cas-hmm using openmp on two supercomputers and a quad-core commercial desktop. the experimental results show that parallel algorithms using cut-and-stitch achieve comparable accuracy and almost linear speedups over the traditional serial version.
007e4eb3-7e2d-495b-a9eb-c5728b59be8c population-based incremental learning: a method for integrating genetic search based function optimization and competitive learning genetic algorithms (gas) are biologically motivated adaptive systems which have been used, with varying degrees of success, for function optimization. in this study, an abstraction of the basic genetic algorithm, the equilibrium genetic algorithm (ega), and the ga in turn, are reconsidered within the framework of competitive learning. this new perspective reveals a number of different possibilities for performance improvements. this paper explores population-based incremental learning (pbil), a method of combining the mechanisms of a generational genetic algorithm with simple competitive learning. the combination of these two methods reveals a tool which is far simpler than a ga, and which out-performs a ga on large set of optimization problems in terms of both speed and accuracy. this paper presents an empirical analysis of where the proposed technique will outperform genetic algorithms, and describes a class of problems in which a genetic algorithm may be able to perform better. extensions to this algorithm are discussed and analyzed. pbil and extensions are compared with a standard ga on twelve problems, including standard numerical optimization functions, traditional ga test suite problems, and np-complete problems.
007f0a5d-759c-4ac5-a57c-23844370b95e data mining techniques for crm the way in which companies interact with their customers has changed dramatically over the past few years. a customer's containing business is no longer guaranteed. as a result, companies have found that they need to understand their customers better, and to quickly respond to their wants and needs. in addition, the time frame in which these responses need to be made has been shrinking. it is no longer possible to wait until the signs of customer dissatisfaction are obvious before action must be taken. to succeed, companies must be proactive and anticipate what a customer desires. in this paper we are going to discuss the data mining techniques used in customer relationship management.
007f9fd9-ddc4-47cb-9d59-e24493660ee8 outrepasser les limites des techniques classiques de prise d'empreintes grace aux reseaux de neurones we present an application of artificial intelligence techniques to the field of information security. the problem of remote operating system (os) detection, also called os fingerprinting, is a crucial step of the penetration testing process, since the attacker (hacker or security professional) needs to know the os of the target host in order to choose the exploits that he will use. os detection is accomplished by passively sniffing network packets and actively sending test packets to the target host, to study specific variations in the host responses revealing information about its operating system. #r##n#the first fingerprinting implementations were based on the analysis of differences between tcp/ip stack implementations. the next generation focused the analysis on application layer data such as the dce rpc endpoint information. even though more information was analyzed, some variation of the "best fit" algorithm was still used to interpret this new information. our new approach involves an analysis of the composition of the information collected during the os identification process to identify key elements and their relations. to implement this approach, we have developed tools using neural networks and techniques from the field of statistics. these tools have been successfully integrated in a commercial software (core impact).
00804dd8-e372-4995-a1e2-859cd0609716 prevalence of heart failure signs and symptoms in a large primary care population identified through the use of text and data mining of the electronic health record abstract  background  the electronic health record (ehr) contains a tremendous amount of data that if appropriately detected can lead to earlier identification of disease states such as heart failure (hf). using a novel text and data analytic tool we explored the longitudinal ehr of over 50,000 primary care patients to identify the documentation of the signs and symptoms of hf in the years preceding its diagnosis.  methods and results  retrospective analysis consisted of 4,644 incident hf cases and 45,981 group-matched control subjects. documentation of framingham hf signs and symptoms within encounter notes were carried out with the use of a previously validated natural language processing procedure. a total of 892,805 affirmed criteria were documented over an average observation period of 3.4 years. among eventual hf cases, 85% had 1 criterion within 1 year before their hf diagnosis, as did 55% of control subjects. substantial variability in the prevalence of individual signs and symptoms were found in both case and control subjects.  conclusions  hf signs and symptoms are frequently documented in a primary care population as identified through automated text and data mining of ehrs. their frequent identification demonstrates the rich data available within ehrs that will allow for future work on automated criterion identification to help develop predictive models for hf.
0080b8a5-74df-4412-b95f-189fa30ad417 adam: detecting intrusions by data mining 
00810b4f-d2fd-4263-afa6-80092a9980a7 a set-theoretic predicate for semantics in natural and formal languages we present an axiomatic framework for semantics that can be applied to natural and formal languages. our main goal is to suggest a very simple mathematical model that describes fundamental cognitive aspects of the human brain and that can still be applied to artificial intelligence. one of our main results is a theorem that allows us to infer syntactical properties of a language out of its corresponding semantics. the role of pragmatics in semantics in our mathematical framework is also discussed.
0081650e-e63b-4942-9973-2492ea95428d from machine learning to machine reasoning a plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". this definition covers first-order logical inference or probabilistic inference. it also includes much simpler manipulations commonly used to build large learning systems. for instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. the resulting model answers a new question, that is, converting the image of a text page into a computer readable text.#r##n##r##n#this observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.
00822694-1388-456c-a323-aad076a9818e fuzzy rough sets for self-labelling: an exploratory analysis semi-supervised learning incorporates aspects of both supervised and unsupervised learning. in semi-supervised classification, only some data instances have associated class labels, while others are unlabelled. one particular group of semi-supervised classification approaches are those known as self-labelling techniques, which attempt to assign class labels to the unlabelled data instances. this is achieved by using the class predictions based upon the information of the labelled part of the data. in this paper, the applicability and suitability of fuzzy rough set theory for the task of self-labelling is investigated. an important preparatory experimental study is presented that evaluates how accurately different fuzzy rough set models can predict the classes of unlabelled data instances for semi-supervised classification. the predictions are made either by considering only the labelled data instances or by involving the unlabelled data instances as well. a stability analysis of the predictions also helps to provide further insight into the characteristics of the different fuzzy rough models. our study shows that the ordered weighted average based fuzzy rough model performs best in terms of both accuracy and stability. our conclusions offer a solid foundation and rationale that will allow the construction of a fuzzy rough self-labelling technique. they also provide an understanding of the applicability of fuzzy rough sets for the task of semi-supervised classification in general.
00842825-20c8-4a4a-a5ad-65d3340188da a heuristic model to predict earthworm biomass in agroecosystems based on selected management and soil properties earthworm burrows can be significant preferential flow paths for water and contaminants to move to subsurface drainage networks and groundwater. thus earthworm biomass could serve as an indicator of such transport potential, and therefore, inform risk assessments associated with water contamination resulting from land application of fertilizer amendments. in this study, we evaluated relationships and interactions between earthworm biomass, soil properties (bulk density, particle size, organic matter, surface residue), land management (crop type, tillage approach), and soil hydraulic properties (field saturated hydraulic conductivity and air-entry tension) for the purpose of building regionally based models to predict earthworm biomass. data were collected from 43 fields distributed throughout eastern ontario, canada. earthworm biomass was measured using hot mustard methods (early autumn) and  in situ  soil hydraulic properties were determined using pressure infiltrometers (late summer/early fall). classification and regression tree (cart) data mining techniques were used to develop tree-structured models to predict biomass from site environmental data. cart regression tree models had coefficients of determination between 0.50 (not including soil hydraulic properties) and 0.55 (including soil hydraulic properties). both regression trees split all earthworm biomass data ( n  = 243) into two groupings defined on the basis of tillage treatment. no-tilled field biomass averaged 192.1 g m 2  (s.d. = 71.5 g m 2 ), and biomass data for conventionally tilled sites subdivided into terminal groupings on the basis of higher surface residue cover (biomass average = 107.9 g m 2  (s.d. = 81.1 g m 2 ) and lower surface residue cover (62.4 g m 2  (s.d. = 54.6 g m 2 )) classes. soil physical and hydraulic data were not important predictors of biomass for tilled datasets; whereas they were more important for no-tilled datasets. for both regression trees, no-till biomass stratified into terminal biomass groupings defined on the basis of bulk density, clay content, and silt content; and for the model including soil hydraulic properties, additionally by soil air-entry tension and surface residue cover. however, bulk density was deemed in the model to be a proxy for years a field was in no-tillage; a positive relationship existed between bulk density and biomass. overall, the terminal tree groups with the highest average earthworm biomasses were for no-till soils with bulk densities >1.4 g cm 3  (longer term no-tillage). regression tree variance reductions associated with the  in situ  measurements of field saturated hydraulic conductivity and air-entry tension were insignificant or small. generally, empirical models predicting earthworm biomass at large spatial scales in agroecosystems using soils and land management information, should consider utilizing variables that express tillage practice, surface residue coverage, years in no-tillage, and soil particle size; however, variable interactions should be considered.
008449c0-ffba-41a2-83b8-3a8796fb4dc3 a metagenomic hybrid classifier for paediatric inflammatory bowel disease abstractinflammatory bowel disease (ibd) is a group of inflammatory diseases of the human colon and small intestine. ibd symptoms are non-specific; diagnosis can be delayed becausean invasive colonoscopy is required for confirmation. delayed diagnosis is linked to poor growth in children. imbalances in the human intestinal microbiome - the community of microorganismsthat reside in the human gut - are thought to contribute to the development of ibd. work done to date in classifying host health statuses from patterns in human micro biomes with supervised learning algorithms has focused on modelling what is present in the gut (i.e. a bacterial census) with the random forest algorithm. metagenomic shotgun sequencing is required to understand what is occurring in the gut (i.e. gene functions) and is often cost prohibitive for hundreds of samples. however, gene functions can be predicted with the phylogenetic investigation ofcommunities by reconstruction of unobserved states (picrust) software package, which could represent a valuable source of new features. in this paper we investigate feature relevanceacross the feature set with the boruta algorithm. we find that the majority of relevant features are from the predicted metagenome. support vector machines (svm) and multilayer perceptrons (mlp) are rarely used with macrobiotic datasets but offer several theoretical advantages. to determine if the new features and alternative algorithms are appropriate, we experiment with a range of machine learning and computational intelligence algorithms. with the best performing algorithms we also implement a conditional multiple classifier system that can identify ibd presence, ibd subtype, and ibd activity from a non-invasive stool sample.
0084b6a9-5986-4c23-becf-1f179aff0f76 detecting biotechnology industry's earnings management using bayesian network, principal component analysis, back propagation neural network, and decision tree the characteristic of long value chain, high-risk, high cost of research and development are belong to high knowledge based content in the biotech medical industry, and the reliability of biotechnology industry's financial statements and the earnings management behavior conducted by the management in their accrual manipulation have been a critical issue. in recent years, some studies have used the data mining technique to detect earnings management, with which the accuracy has therefore risen. as such, this study attempts to diagnose the detecting biotechnology industry earnings management by integrating suitable computing models, we first screened the earnings management variables with the principal component analysis (pca) and bayesian network (bn), followed by further constructing the integrated model with the back propagation neural network (bpn) and c5.0 (decision tree) to detect if a company's earnings were seriously manipulated. the empirical results show that combining the bn screening method with c5.0 decision tree has the best performance with an accuracy rate of 98.51%. from the rules set in the final additional testing of the study, it is also found that an enterprise's prior period discretionary accruals play an important role in affecting the serious degree of accrual earnings management.
0084bf6e-1328-4346-b5b9-0a34ed131cda intrusion detection by integrating boosting genetic fuzzy classifier and data mining criteria for rule pre-screening the purpose of the work described in this paper is to provide an intelligent intrusion detection system (iids) that uses two of the most popular data mining tasks, namely classification and association rules mining together for predicting different behaviors in networked computers. to achieve this, we propose a method based on iterative rule learning using a fuzzy rule-based genetic classifier. our approach is mainly composed of two phases. first, a large number of candidate rules are generated for each class using fuzzy association rules mining, and they are pre-screened using two rule evaluation criteria in order to reduce the fuzzy rule search space. candidate rules obtained after pre-screening are used in genetic fuzzy classifier to generate rules for the classes specified in iids: namely normal, prb-probe, dos-denial of service, u2r-user to root and r2l-remote to local. during the next stage, boosting genetic algorithm is employed for each class to find its fuzzy rules required to classify data each time a fuzzy rule is extracted and included in the system. boosting mechanism evaluates the weight of each data item to help the rule extraction mechanism focus more on data having relatively more weight, i.e., uncovered less by the rules extracted until the current iteration. each extracted fuzzy rule is assigned a weight. weighted fuzzy rules in each class are aggregated to find the vote of each class label for each data item.
008503ad-2572-44a0-9d5f-9593ebc4fee9 essentials of artificial intelligence since its publication, essentials of artificial intelligence has been#r##n#adopted at numerous universities and colleges offering introductory ai#r##n#courses at the graduate and undergraduate levels. based on the author's#r##n#course at stanford university, the book is an integrated, cohesive#r##n#introduction to the field. the author has a fresh, entertaining writing#r##n#style that combines clear presentations with humor and ai anecdotes. at the#r##n#same time, as an active ai researcher, he presents the material#r##n#authoritatively and with insight that reflects a contemporary, first hand#r##n#understanding of the field. pedagogically designed, this book offers a#r##n#range of exercises and examples.#r##n##r##n##r##n#table of contents#r##n##r##n#1 introduction: what is ai? #r##n#2 overview #r##n#3 blind search #r##n#4 heuristic search #r##n#5 adversary search #r##n#6 introduction to knowledge representation #r##n#7 predicate logic #r##n#8 first-order logic #r##n#9 putting logic to work: control of reasoning #r##n#10 assumption-based truth maintenance #r##n#11 nonmonotonic reasoning #r##n#12 probability #r##n#13 putting knowledge to work: frames and semantic nets #r##n#14 planning #r##n#15 learning #r##n#16 vision #r##n#17 nature language #r##n#18 expert systems #r##n#19 concluding remarks
00868c7d-72f1-4373-a6c7-96067cac1d7a the constrained weight space svm: learning with ranked features applying supervised learning methods to new classication tasks requires domain experts to label sucient training data for the classier to achieve acceptable performance. it is desirable to mitigate this annotation effort. to this end, a pertinent observation is that instance labels are often an indirect form of supervision; it may be more ecient to impart domain knowledge directly to the model in the form of labeled features. we present a novel classication model for ex
0086fa44-821b-4137-9fc7-bf231e2e6353 proactive intrusion detection machine learning systems are deployed in many adversarial conditions like intrusion detection, where a classifier has to decide whether a sequence of actions come from a legitimate user or not. however, the attacker, being an adversarial agent, could reverse engineer the classifier and successfully masquerade as a legitimate user. in this paper, we propose the notion of a proactive intrusion detection system (ids) that can counter such attacks by incorporating feedback into the process. a proactive ids influences the user's actions and observes them in different situations to decide whether the user is an intruder. we present a formal analysis of proactive intrusion detection and extend the adversarial relationship between the ids and the attacker to present a game theoretic analysis. finally, we present experimental results on real and synthetic data that confirm the predictions of the analysis.
00881641-b05c-4752-a078-3f3461700d98 regression based multi-tier resource provisioning for session slowdown guarantees autonomous management of a multi-tier internet service involves two critical and challenging tasks, one understanding its dynamic behavior when subjected to dynamic workload and second adaptive management of its resources to achieve performance guarantees. in this paper, we propose a statistical machine learning based approach to achieve session slowdown guarantees of a multi-tier internet service. session slowdown is the ratio of a session's total queueing delay to its total processing time. it is a compelling performance metric of session-based internet services because it directly measures user-perceived relative performance. however, there is no analytical model for session slowdown on multi-tier servers. we first conduct training to learn the statistical regression models that quantitatively capture an internet service's dynamic behavior as relationships between various service parameters. then, we propose a dynamic resource provisioning approach that utilizes the learned regression models to efficiently achieve session slowdown guarantees under varying workloads. the approach is based on the combination of extensive offline training and online monitoring of the internet service behavior. experiments using the industry standard tpc-w benchmark demonstrate the effectiveness and efficiency of the regression based dynamic resource provisioning approach in meeting the session slowdown guarantees of a multi-tier e-commerce application.
0088788a-2b38-47c0-8848-915d11916135 density estimation for spatial data streams in this paper we study the problem of estimating several types of spatial queries in a streaming environment. we propose a new approach, which we call local kernels, for computing density estimators by using local rather than global statistics on the data. the approach is easy to extend to an on-line setting, by maintaining a small random sample with a kd-tree-like structure on top of it. our structure dynamically adapts to changes in the locality of data and has small update time. experimental results show that the proposed algorithm returns good approximate results for a variety of data and query distributions. we also show that it is useful in off-line computations, as well.
008936e4-2984-4cbb-84e1-af9afcceffb2 improved snow depth retrieval by integrating microwave brightness temperature and visible/infrared reflectance the accuracy of snow depth retrieval by remote sensing depends heavily on the characteristics of the snow, and both passive microwave and visible/infrared sensors can contribute to the acquisition of this information. a method integrating these two remotely sensed data sets is presented in this study. snow depth retrieval is performed using microwave brightness temperature at 19 and 37 ghz from the special sensor microwave/imager (ssm/i) and the special sensor microwave image/sounder (ssmi/s), and visible/infrared surface reflectance from moderate resolution imaging spectroadiometer (modis) products. microwave brightness temperature provides information about the volume of snow pack, and visible/infrared surface reflectance can indicate snow presence and surface grain size. with these two remote sensing data sets, snow depth is retrieved by a nonlinear data mining technique, the modified sequential minimal optimization (smo) algorithm for support vector machine (svm) regression. the proposed method is tested by using 16,329 records of dry snow measured at 54 meteorological stations in xinjiang, china over an area of 1.6 million km2 from 2000 to 2009. the root mean square error (rmse), relative rmse and the correlation coefficient of our method are 6.21 cm, 0.64 and 0.87, respectively. these results are better than those obtained using only brightness temperature data (8.80 cm, 0.90 and 0.73), the traditional spectral polarization difference (spd) algorithm (15.07 cm, 1.54 and 0.58), a modified chang algorithm in westdc (9.80 cm, 1.00 and 0.62), or the multilayer perceptron classifier of artificial neural networks (ann) (9.23 cm, 0.94 and 0.72). the daily snow water equivalent (swe) retrieved by this method has an rmse of 8.05 mm and a correlation of 0.84, which are better than those of nasa nsidc (32.87 mm and 0.47) or globsnow (19.07 mm and 0.59). this study demonstrates that the combination of visible/infrared surface reflectance and microwave brightness temperature via an svm regression can provide a more accurate retrieval of snow depth.
00895d27-7618-47ff-9860-8701b6930736 modeling free-form handwriting gesture user authentication for android smartphones smartphones nowadays are customized to help users with their daily tasks such as storing important data or making transactions through the internet. with the sensitivity of the data involved, authentication mechanism such as fixed-text password, pin, or unlock patterns are used to safeguard these data against intruders. however, these mechanisms have the risk from security threats such as cracking or shoulder surfing. to enhance mobile and/or information security, this study aimed to develop a free-form handwriting gesture user authentication for smartphones. it also tried to discover the static and dynamic handwriting features that significantly influence the recognition of a legitimate user. the experiment was then conducted by asking thirty (30) individuals to draw or swipe using their fingertip their desired free-form security pattern ten (10) times. these patterns were then cleaned and processed, and extracted seven (7) static and eleven (11) dynamic handwriting features. by means of neural network classifier of the rapidminer data mining tool, these features were used to develop, validate, and test a model for user authentication. the model showed a very promising recognition rate of 96.67%. the model is further tested through a prototype, and it still gave a very satisfactory result.
0089d283-aab2-43fd-bf16-fb331aca7b58 adsorption and dissociation of water on relaxed alumina clusters: a first principles study using previous results for the equilibrium geometries of stoichiometric (al 2 o 3 ) n  clusters as models for non ideal alumina surfaces, we obtain, from ab-initio total energy lcao calculations, the corresponding relaxed structures of the complexes h 2 o-(al 2 o 3 ) n with n  7. depending on the initial position of the water molecule relative to the cluster site, the complex evolves to different equilibrium structures, with and without dissociation of h 2 o, whose energetic, bond lengths, and charge transfer trends are studied as the morphology and size of the initial cluster change. dissociation of h 2 o with the radical oh -  bound on top of an al atom and the proton h + bound to the second nearest neighbour o, is the dominant process for the reaction (al 2 o 3 ) n  + h 2 o with n = 4, in agreement with the one observed for the adsorption of h 2 o on the extended real surface.
0089f938-165c-4f56-9f48-1e528ed59042 android malware detection using category-based machine learning classifiers android malware growth has been increasing dramatically as well as the diversity and complicity of their developing techniques. machine learning techniques have been applied to detect malware by modeling patterns of static features and dynamic behaviors of malware. the accuracy rates of the machine learning classifiers differ depending on the quality of the features. we increase the quality of the features by relating between the apps' features and the features that are required to deliver its category's functionality. to measure the benign app references, the features of the top rated apps in a specific category are utilized to train a malware detection classifier for that given category. android apps stores such as google play organize apps into different categories. each category has its distinct functionalities which means the apps under a specific category are similar in their static and dynamic features. in other words, benign apps under a certain category tend to share a common set of features. on the contrary, malicious apps tend to have abnormal features, which are uncommon for the category that they belong to. this paper proposes category-based machine learning classifiers to enhance the performance of classification models at detecting malicious apps under a certain category. the intensive machine learning experiments proved that category-based classifiers report a remarkable higher average performance compared to non-category based.
0089fa79-8a04-44f1-8d56-c4a6950cf4a8 automated system for drilling operations classification using statistical features operations classification is one of the most needed tasks in the oil & gas industry. it provides the engineers with detailed information about what is happening on the rig site. in this paper we propose an approach to classify drilling operations automatically using machine learning techniques. this approach takes as input the sensors data in a specific time range, and predicts the drilling operation. our approach is simple but effective, where for each sensor data (channel) a list of statistical features will be extracted, then features selection algorithms will be used to select the most informative features, and finally, a classifier will be trained based on these features. in this paper many feature weighting and selection algorithms were tested to find which statistical measures clearly distinguish between many different rig operations. in addition, many classification techniques were employed to find the best one in terms of accuracy and speed. experimental evaluation with real data, from four different drilling scenarios, shows that our approach has the ability to extract and select the best features and build accurate classifiers. the performance of the classifiers was evaluated by using the cross-validation method.
008a7e4f-7972-4e67-bf19-4bc25d833253 qualitative spatial relationships cleaning for spatial data mining in this article, we investigate the problem of preparing qualitative spatial relations before implementing spatial data mining by checking consistency in a constraint network, which includes topological and cardinal directional relations between pairs of spatial objects. we aim to explore potential spatial relations and possible inconsistency among the data of relationships for enforcing the correctness of spatial data mining. this task is carried out through qualitative spatial reasoning method, specifically consistency checking. we try to lay the theoretical foundation for this kind of problem. instead of using conventional composition tables, we investigate the interactions between topological and cardinal directional relations with the aid of rules. these rules are shown to be sound, i.e. the deductions are logically correct. based on these rules, an improved constraint propagation algorithm is introduced to enforce the path consistency. an example is presented to show the utility of these rules.
008aa9ed-4b58-4d66-9bc5-b6a8ad0c9be2 prediction of fracture characteristics of high strength and ultra high strength concrete beams based on relevance vector machine this paper examines the applicability of relevance vector machine-based regression to predict fracture characteristics and failure load (pmax) of high strength and ultra high strength concrete beams. fracture characteristics include fracture energy (gf), critical stress intensity factor (kic) and critical crack tip opening displacement. characterization of mix and testing of beams of high strength and ultra high strength concrete have been described briefly. the procedure to compute gf, kic and ctodc has been outlined. relevance vector machine is a machine learning technique that uses bayesian inference to obtain parsimonious solutions for regression and classification. the relevance vector machine has an identical functional form to the support vector machine, but provides probabilistic classification and regression. relevance vector machine is based on a bayesian formulation of a linear model with an appropriate prior that results in a sparse representation. four relevance vector machine models have been developed using matlab software for training and prediction of pmax, kic, gf and ctodc. relevance vector machine models have been trained with about 70% of the total 87 datasets and tested with about 30% of the total datasets. it is observed that the predicted values from the relevance vector machine models are in good agreement with those of the experimental values.
008b6e88-d8b8-46d2-a44c-c333d53e9f56 domains, brains and evolution our aim in this paper is to do some conceptual spring-cleaning. several prominent evolutionary psychologists have argued that the human cognitive architecture consists in a large number of domain-specific features, rather than, as dissenters claim, a small number of domain-general features. the first difficulty here is that there exists no widely agreed-upon definition of domain. we show that evolutionary psychology has the resources for such a definition: a domain is defined as an adaptive problem, or a set of suitably related adaptive problems. adopting this definition, we proceed to introduce the distinction between data and algorithms, and to differentiate four conceptions of our cognitive architecture, only two of which, we argue, are viable: (a) general-purpose mechanisms operating on domain-specific information, and (b) special-purpose mechanisms operating on domain-specific information. typically, evolutionary psychologists argue in favour of (b), as against (a). following a defence of this position against a recent claim that the process of exaptation makes general-purpose mechanisms evolutionarily plausible, we consider the strongest of the evolutionary psychologists in-principle arguments for the evolutionary implausibility of general-purpose mechanisms. this argument is based on two requirements: that the human cognitive architecture must (i) be capable of solving all the adaptive problems faced by our ancestors, and (ii) have outperformed all competing designs. work in artificial intelligence suggests that although requirement (i) might be met by general-purpose mechanisms coupled with domain-specific information, requirement (ii) wont. nonetheless, we propose (tentatively) that relatively general-purpose mechanisms might result from the operation of multiple, simultaneous, systematically related selection pressures. an examination of this proposal, however, brings into sharp relief the fact that, in many evolutionary scenarios, it simply may not be possible to establish a robust distinction between domain-specific and domain-general features.
008c6924-ef5c-4f67-8b77-e92829ee9b28 madame bovary on the holodeck: immersive interactive storytelling in this paper, we describe a small-scale, yet complete, integration of a real-time immersive interactive storytelling system. while significant progress has been achieved in recent years on the individual component technologies of interactive storytelling, the main objective of this work is to investigate the concept of interactive storytelling in a fully immersive context. we describe each individual component of immersive interactive storytelling from a technical perspective. we have used a commercial game engine as a development environment, supporting real-time visualisation as well as the inclusion of artificial intelligence components controlling virtual actors. this visualisation engine has been ported to an immersive setting using dedicated software and hardware supporting real-time stereoscopic visualisation. the hardware platform is built around a 4-sided cave-like immersive display operated by a pc-cluster. the interactive storytelling engine is constituted by a planning system based on characters motivations and emotional states. the user can interact with the virtual world using multimodal interaction. we illustrate the system's behaviour on the implementation of excerpts from  madame bovary , a classic xix th  century novel, and demonstrate the ability for the user to play the role of one of the characters and influence the unfolding of the story by his actions.
008c81f8-d54b-43c7-b5de-9944b1a549e3 intruder detection using deep learning and association rule mining with the upsurge of internet popularity, nowadays there are millions of online transactions that are being processed per minute thus increasing the possibilities of intruder attacks over the recent times. there have been various intruder detection techniques such as using traditional machine learning based algorithms. these algorithms were widely used to identify and prevent intruder activities in the recent past. furthermore, multilayer neural networks[5] were also used in this regard to perform the detection. hence multi-layer neural networks inherit fundamental drawbacks due to its inability to perform training due the problems such as overfitting, etc. in contrast, deep learning algorithms were introduced to overcome these issues effectively. we propose a novel framework to perform intruder detection and analysis using deep learning nets and association rule mining. we utilize a recurrent network to predict intruder activities and fp-growth to perform the analysis. our results show the effectiveness of our framework in detail.
008cc0f3-47d2-4beb-91d8-d661b89e5ef2 parallel interval type-2 subsethood neural fuzzy inference system a subsethood based interval type-2 fuzzy neural evolutionary inference system.model is implemented on a parallel platform, it learns using differential evolution.this model is hybrid of type-1 and type-2 fuzzy sets.works excellent on function approx., time series prediction, control applications.this model handles uncertainty with lesser number of trainable parameters. neuro-fuzzy models are being increasingly employed in the domains like weather forecasting, stock market prediction, computational finance, control, planning, physics, economics and management, to name a few. these models enable one to predict system behavior in a more human-like manner than their crisp counterparts. in the present work, an interval type-2 neuro-fuzzy evolutionary subsethood based model has been proposed for its use in finding solutions to some well-known problems reported in the literature such as regression analysis, data mining and research problems relevant to expert and intelligent systems. a novel subsethood based interval type-2 fuzzy inference system, named as interval type-2 subsethood neural fuzzy inference system (it2sunfis) is proposed in the present work. mathematical modeling and empirical studies clearly bring out the efficacy of this model in a wide variety of practical problems such as truck backer-upper control, mackey-glass time-series prediction, narazaki-ralescu and bell function approximation. the simulation results demonstrate intelligent decision making capability of the proposed system based on the available data. the major contribution of this work lies in identifying subsethood as an efficient measure for finding correlation in interval type-2 fuzzy sets and applying this concept to a wide variety of problems pertaining to expert and intelligent systems. subsethood between two type-2 fuzzy sets is different from the commonly used sup-star methods. in the proposed model, this measure assists in providing better contrast between dissimilar objects. this method, coupled with the uncertainty handling capacity of type-2 fuzzy logic system, results in better trainability and improved performance of the system. the integration of subsethood with type-2 fuzzy logic system is a novel idea with several advantages, which is reported for the first time in this paper.
008cd09e-2710-492c-99f7-8d3acaaf58c9 meta methods for model sharing in personal information systems this article introduces a methodology for automatically organizing document collections into thematic categories for personal information management (pim) through collaborative sharing of machine learning models in an efficient and privacy-preserving way. our objective is to combine multiple independently learned models from several users to construct an advanced ensemble-based decision model by taking the knowledge of multiple users into account in a decentralized manner, for example, in a peer-to-peer overlay network. high accuracy of the corresponding supervised (classification) and unsupervised (clustering) methods is achieved by restrictively leaving out uncertain documents rather than assigning them to inappropriate topics or clusters with low confidence. we introduce a formal probabilistic model for the resulting ensemble based meta methods and explain how it can be used for constructing estimators and for goal-oriented tuning. comprehensive evaluation results on different reference data sets illustrate the viability of our approach.
008cdddc-7cf4-413a-86f8-7b9e450cb080 on the methodology for comparing learning algorithms: a case we explore several issues relevant to the benchmarking and comparison of machine learning algorithms. we illustrate those issues with a case study using the decision tree induction algorithms c4.5 (j. quinlan, 1993) and multiscale classification (msc) (a.p. bradley and b.c. lovell, 1994), multilayer perceptrons (mlp) and multivariable regression (mvr). then for a "real world" problem we compare estimates of the true error rates for each classifier, first on a single train and test partition, and then using cross validated subsampling techniques. the relevance of the /spl chi//sup 2/ test is then discussed in relation to comparing the classifier accuracies. the paper concludes by evaluating the performance of these four fundamentally different approaches to the solution of this regression problem. >
008d0acc-a61c-4ced-8746-9977cde99455 machine learning for atomic forces in a crystalline solid: transferability to various temperatures recently, machine learning has emerged as an alternative, powerful approach for predicting quantum-mechanical properties of molecules and solids. here, using kernel ridge regression and atomic fingerprints representing local environments of atoms, we trained a machine-learning model on a crystalline silicon system in order to directly predict the atomic forces at a wide range of temperatures. our idea is to construct a machine-learning model using a quantum-mechanical data set taken from canonical-ensemble simulations at a higher temperature, or an upper bound of the temperature range. with our model, the force prediction errors were about 2% or smaller with respect to the corresponding force ranges, in the temperature region between 300 and 1650 k. we also verified the applicability to a larger system, ensuring the transferability with respect to system size.
008d6ab4-6741-44be-b79e-e6422d93ee42 algorithms for locating faults on series compensated lines using neural network and deterministic methods this paper investigates a scheme to improve the reach measurement of distance relays and fault locators for series compensated power lines. a deterministic method and a feedforward neural network method have been implemented for online calculation of the voltage across a nonlinear capacitor installation. these techniques are compared and incorporated into a new relaying scheme which is independent of the series capacitor installation, operation of the capacitor protection, and the surrounding power system elements. the proposed scheme is simple and accurate and requires only local voltage and current at the bus. detailed testing using emtp has been done to show the benefits of the new adaptive scheme. the results demonstrate the suitability of the techniques for real world applications.
008def27-d198-4046-abf0-2a59bc874ae7 bilateral negotiation for agent-mediated electronic commerce one of the main issues in electronic commerce is the inclusion of the negotiation facilities commonly available in human client-vendor interaction in real world commerce. e-commerce negotiation processes have usually been modeled as self-interested multi-agent systems. in these systems buyers and sellers are represented by agents that have opposite demands and decide what to do on the flight, based on the available information. however, currently ecommerce systems, such as kasbah and magma [21], provide a small number of bilateral negotiation facilities. fortunately, some general negotiation models could be at first applied to e-commerce domain. this is typically the case of faratin's model, which can be seen as an extension of kasbah's. in this paper, we propose an original bilateral agent negotiation model, which extends faratin's one. we introduce various facilities, such as alternative product suggestion, ultimatum generation, local contract agreements, etc. these facilities intend to grant users with a more flexible e-commerce environment. we present our model formalization, including the knowledge base that determines agent behavior. some empirical validation is also presented to the case of computer purchase.
008e0e26-c84d-4b8b-9fff-2ff4481dc00c applying machine learning to chinese temporal relation resolution temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language. this information is often inferred from a variety of interactive grammatical and lexical cues, especially in chinese. for this purpose, inter-clause relations (temporal or otherwise) in a multiple-clause sentence play an important role. in this paper, a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a chinese multiple-clause sentence. the model makes use of the fact that events are represented in different temporal structures. it takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures. a set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution.
008e25d5-89d1-452b-bf9f-05dce536827c metabolic profiling of 1h nmr spectra in chronic kidney disease with local predictive modeling metabolic profiling, the study of changes in the concentration of the metabolites in the organism induced by biological differences within subpopulations, has to deal with a very large amount of complex data. it therefore requires the use of powerful data processing and machine learning methods. to overcome over-fitting, a common concern in metabolic profiling where the number of features is often much larger than the number of observations, many predictive analyses combined dimension reduction techniques with multivariate predictive linear modeling. moreover, they built a global model that identifies biomarkers predictive of the output of interest giving their overall trend variations. however, this fails to capture local biological phenomena underlying subgroups of subjects. more recently, local exploration methods based on decision trees approaches have been applied in metabolomics but they only explore random parts of the feature space. in this study, we used a supervised rule-mining algorithm that locally and exhaustively explores the feature space to predict chronic kidney disease (cdk) stages based on proton nuclear magnetic resonance (1h nmr) data. from the discriminant subregions obtained with this exploration, we extracted local features and learned a l2-regularized logistic regression (l2lr) classifier. we compared the resulting local predictive model with a standard one, combining classical univariate supervised feature selection techniques with a l2lr, and a model mixing both global and local features. results show that the local predictive model is more powerful in terms of predictive performance than the mixed and global models. additionally, it gives key insights into biological variations specific to subgroups of subjects.
008e3576-9e1b-4432-96de-39d713e1e160 proximal knowledge-based classification prior knowledge over general nonlinear sets is incorporated into proximal nonlinear kernel classification problems as linear equalities. the key tool in this incorporation is the conversion of general nonlinear prior knowledge implications into linear equalities in the classification variables without the need to kernelize these implications. these equalities are then included into a proximal nonlinear kernel classification formulation (g. fung and o. l. mangasarian, proximal support vector machine classifiers, in proceedings kdd-2001: knowledge discovery and data mining, f. provost and r. srikant (eds), san francisco, ca, new york, association for computing machinery) that is solvable as a system of linear equations. effectiveness of the proposed formulation is demonstrated on a number of publicly available classification datasets. nonlinear kernel classifiers for these datasets exhibit marked improvements upon the introduction of nonlinear prior knowledge compared with nonlinear kernel classifiers that do not utilize such knowledge. copyright  2009 wiley periodicals, inc. statistical analysis and data mining 1: 000-000, 2009
008f0f0a-b901-4293-b600-cdbc1af9ce28 generalized derivative based kernelized learning vector quantization we derive a novel derivative based version of kernelized generalized learning vector quantization (kglvq) as an effective, easy to interpret, prototype based and kernelized classifier. it is called d-kglvq and we provide generalization error bounds, experimental results on real world data, showing that d-kglvq is competitive with kglvq and the svm on uci data and additionally show that automatic parameter adaptation for the used kernels simplifies the learning.
008f1189-bc57-420f-a965-aa5508f7209d an empirical study on prediction of heart disease using classification data mining techniques in this research paper, the use of pattern recognition and data mining techniques into risk prediction models in the clinical domain of cardiovascular medicine is proposed. the data is to be modelled and classified by using classification data mining technique. some of the limitations of the conventional medical scoring systems are that there is a presence of intrinsic linear combinations of variables in the input set and hence they are not adept at modelling nonlinear complex interactions in medical domains. this limitation is handled in this research by use of classification models which can implicitly detect complex nonlinear relationships between dependent and independent variables as well as the ability to detect all possible interactions between predictor variables.
008f68b9-4277-4f47-bf66-db9d318ad157 lung cancer detection and analysis using data mining techniques, principal component analysis and artificial neural network the successful diagnosis of lung cancer disease in early time increases the percentage of patient survival. effective ways for predict and treat lung cancer remain challenges due to lack of effective ways of detection the lung nodules which causes by their arbitrariness in shape, size and texture. in this paper, image processing is used for image pre-processing, image segmentation and feature extraction. artificial neural network (ann) have been employed to learn extracted feature for nodule detection such as shape, size, volume.while principal component analysis were employed for multivariate data processing, it used to detect the complexity of interrelationships between diverse patient, disease and treatment variables.matlab have been used for all procedure in processing lung image and artificial neural network for train features extracted. xlstart software was used for principal component analysis. the lung cancer database which contains the images classify lung image into two kinds:1)normal with no nodule and 2)nodule image such as benign or malignant.therefore,by using the proposed method the accuracy obtained was 76%.
008fa323-20a8-450f-bbcb-7c41673db91d spatio temporal data mining framework for natural resource exploration technology has allowed for a substantial increase in success rate of identifying the presence of energy sources such as oil and natural gas. data mining, an emerging technology characterized by significantly advanced analytical tools, can contribute to this success rate as it has the potential to guide or at least assist opportunists in hydrocarbon prediction. to make a prediction about presence of hydrocarbon reserve beneath the surface of earth involves geological, geochemical, seismic and microbial prospecting. the test methods involved in the mentioned process need a great deal of cost and time. this project is aimed at developing decision support system to improve the process of hydrocarbon need evaluation and reserve detection by integrating the methods and tools from data mining and potential surface analysis to approach the problem from an interdisciplinary stance.#r##n#in the thesis, the world countries are classified with respect to sustainable energy development with the underlying assumption that hydrocarbon is the major source of energy all over the world. the addressed question is whether the hydrocarbon reserves in the world comply with its consumption? as a result, two possibilities arose to ensure energy sustainability: (1). to provide an optimal framework for improvement in hydrocarbon exploration process. (2). to provide a framework for improvement in hydrocarbon consumption. the study is about the aforementioned.#r##n#the presence of hydrocarbon reserves beneath earths surface is predicted on the basis of either (a). surface indicators or (b). beneath surface parameters. the surface indicators which are considered in this project may consist of geological and microbial indicators. in state of the art geological and microbial methodologies, the cost and time involved is in affordable. the research attempts to replace geological predictions with intelligent remote sensing and microbial indications with microbe data mining. but the existing techniques of data mining cannot produce desired accuracy if applied to surface indicators database. some data mining techniques for mining temporal spatial and non spatial data related to surface indicators of hydrocarbon reserves are proposed. the model includes the classification mechanism of world countries on the basis of sustainable hydrocarbon development and then extraction of useful patterns from surface indicators to predict hydrocarbon reserve in a time and cost effective manner. a series of empirical investigations have been made to evaluate the performance of proposed techniques using different and diverse databases that show the effectiveness of methodology.
00901d6e-a2d9-4a3a-ab6b-5ac933048dd9 learning decision strategies with genetic algorithms 
009129fa-646f-4d8d-8808-488cab9430b7 the comparisons of prognostic indexes using data mining techniques and cox regression analysis in the breast cancer data the purpose of this study is to determine new prognostic indexes for the differentiation of subgroups of breast cancer patients with the techniques of decision tree algorithms (c&rt, chaid, quest, id3, c4.5 and c5.0) and cox regression analysis for disease-free survival (dfs) in breast cancer patients. a retrospective analysis was performed in 381 breast cancer patients diagnosed. age, menopausal status, age of menarche, family history of cancer, histologic tumor type, quadrant of tumor, tumor size, estrogen and progesterone receptor status, histologic and nuclear grading, axillary nodal status, pericapsular involvement of lymph nodes, lymphovascular and perineural invasion, adjuvant radiotherapy, chemotherapy and hormonal therapy were assessed. based on these prognostic factors, new prognostic indexes for c&rt, chaid, quest, id3, c4.5 and c5.0 and cox regression were obtained. prognostic indexes showed a good degree of classification, which demonstrates that an improvement seems possible using standard risk factors. we obtained that c4.5 has a better performance than c&rt, chaid, quest, id3, c5.0 and cox regression to determine risk groups using random survival forests (rsf).
00916a29-733f-4928-822a-7353031c8eed the legal precedent in online dispute resolution the advances observed in the last years in telecommunication technologies rapidly brought along new ways of doing business. this new reality, however, has not been so rapidly followed by the entities responsible for dealing with the conflicts that arise from these interactions, now undertaken in an electronic format. traditional paper-based courts, designed for the industrial era, are now outdated. the answer to this problem may rely on the new tools that can be built using new artifacts from fields such as artificial intelligence. using these tools the parties can simulate outcomes, thus having a better notion of the possible consequences of a legal dispute, namely in terms of the best and worst alternative to negotiated agreements. in this paper, we present our agent-based architecture for such a tool, umcourt, placing special emphasis on a particular agent that, based on the concept of legal precedent, gives its users a set of possible outcomes of a case, based on the observation of past similar cases and learns new cases in order to enrich its knowledge base about the portuguese labor law.
0091773e-3055-481f-adc5-c08ca5727436 short-term forecasting of zonal and meridional wave energy flux in the bay of biscay using random forests three types of statistical models have been used to create up to 24h forecasts of the zonal and meridional components of wave energy flux levels at three directional buoys located in the bay of biscay. hourly observations of the mean wave period and the significant height covering the 19992012 period have been used for this purpose. additionally, data from the ocean (wam model) and atmospheric components of the era-interim reanalysis of the ecmwf have also been used. those data have been splitted into a training database (19992005) used to build the models, and a test database (20062012) reserved to test them and assess their performance. the models used have been built using three different techniques: analogues, random forests (a machine learning algorithm) and a combination of both. for evaluation purposes, the performance of the models at each location has been compared at a 95% confidence level with the simplest prediction-persistence of levels- and also with the nearest gridpoint wam forecasts. for forecasting horizons between 3 and roughly 16 hours at locations near the coast (where wave farms can be installed), among the statistical models, those built on random forests outperform the rest, including wam and persistence.
0091adb1-603f-4b0b-9f26-4c8a82a4fe8b maximum margin semi-supervised learning for structured variables many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency. recent progress in machine learning has mainly focused on supervised classification of such structured variables. in this paper, we investigate structured classification in a semi-supervised setting. we present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. unlike transductive algorithms, our formulation naturally extends to new test points.
0093572b-0247-49a5-94da-22b3e6da56f0 revolution in health and wellbeing we argue that recent technology developments hold great promises for health and wellbeing. in our view, recent advances of (1) smart tools and wearable sensors of diverse kinds, (2) data collection and data mining methods, (3) 3d visual recording and visual processing methods, (4) 3d models of the environment with robust physics engine, and last but not least, (5) new applications of human computing and crowdsourcing started the revolution. we are neither claiming nor excluding that human intelligence will be reached in some years from now, but make the above claim, which is both weaker and stronger. we be- lieve that fast developments for health and wellbeing are the question of active collaboration between health and wellbeing experts and motivated engineers.
0094ef6c-c56a-461e-b83b-373c4056b354 analyzing lexical expressions in an human-agent online explanation task: influence of affect and characteristics the present study investigated how the variables, consisting of the expressions of emotion and embodied characteristics of the pca and the personal characteristics of the participants, influenced the participants' explanation performance. in the study, a lexical network analysis, focusing on the co-occurrence of key words in the participant's text explanation as dependent variables, was used for automatic evaluation. text-mining and machine learning results show that during the explanation activity the expression and the gender of the pca influence the learners' performance. this paper provides insight into the behavior of humans performing online tasks and suggestions related to the design of efficient online tutoring systems.
00963541-d2a7-4c3e-9a7e-9d9d3082c358 empowerment and restraint in scientific communication. new developments make it easier to share information, but more difficult to deal with dual-use biology. these are great times to be a scientist. never before has communication and access to information been so easy, at a time when the amount of scientific information itself is increasing exponentially. many new technologies allow people to receive and send information and opinions in ways that are readily accessible to anybody with a shared interest. together, these developments have empowered scientistsbut not only scientiststo rise above the information deluge. however, there are also increasing concerns about the risks in making public biological information that is potentially useful for destructive purposes, either by individuals or states. these concerns, expressed mainly by politicians and security experts, have raised the prospect of restraining publication of sensitive results. this article focuses on such restraints in the context of the opportunities.#n##n#several new software developments now provide online empowerment. rss feeds allow the user to customize alerts to new content on websites, while social bookmarking software, such as connotea and del.icio.us, enable people, privately or openly, to share links to websites they find interesting and valuable. the combination of these technologies can greatly reduce the obstacles to finding relevant new information quickly, even if the user did not know it existed.#n##n#interactive weblogs give more power to both the author and the reader. whether openly or secretly, weblogs and emails allow researchers to bypass standard channels of communication to reach out directly to any audience they wantas practised, for example, by us climate researchers who want to speak about their science in an often hostile environment (www.realclimate.org). perhaps the most significant empowerment will come with the semantic web, a set of new standards and protocols devised by tim bernerslee, which will in effect ally human web users to computers and artificial intelligence.#n##n#> as physicists discovered long ago, there are some areas of research...that are ...
009655d3-e0b1-40ec-b2c3-47284f9c82c7 an artificial t cell immune system for predicting mhc-ii binding peptides one key principle of natural immune systems is the extracellular presentation of peptides bound to mhc-ii complexes on the cell surface to represent the internal state. the prediction of those peptides that are presented became a current research topic in machine learning, as they may be used as potential vaccines for immunization. in addition the biological immune system (is) is a learning system in its own right. in this work, we design an artificial immune system (ais) that is based on observations of the natural immune system to predict mhc-ii binding peptides. our strategy simulates the mutable receptors of t lymphocytes as well as their selection during life time.we model the receptor specificity and binding mode as well as the lymphocyte's influence during an inflammatory response. finally, our implementation uses the pathogen specificity of t cells to model the prediction problem.
00968e7a-03b5-423b-ad88-d583e4007ea0 a practical kernel criterion for feature extraction and recognition of mstar sar images complete kernel fisher discriminant analysis (ckfda) is essentially a practical nonlinear feature extraction criterion based on kernel trick. the process is divided into two phases, i.e., kernel principal component analysis (kpca) and linear discriminant analysis (lda). this work uses two different kinds of ckfda methods to extract the features of mstar sar images: one only obtains the regular information in "single discriminant space", the other gains regular and irregular information in "double discriminant subspaces". the inspiring recognition results verify that the features not only overcome aspect sensitivity existent in sar images, but also are robust to variants within the target classes which have small configuration differences
00977443-21a6-4415-bdad-2750cc5d37ba sequential pattern mining with optimization calling mapreduce function on mapreduce framework sequential pattern mining that determines frequent patterns appearing in a given set of sequences is an important data mining problem with broad applications. for example, sequential pattern mining can find the web access patterns, customer's purchase patterns and dna sequences related with specific disease. in this paper, we develop the sequential pattern mining algorithms using mapreduce framework. our algorithms distribute input data to several machines and find frequent sequential patterns in parallel. with synthetic data sets, we did a comprehensive performance study with varying various parameters. our experimental results show that linear speed up can be achieved through our algorithms with increasing the number of used machines.
00978b05-a2b8-483a-84c7-77caff73d264 a data mining approach for the monitoring of active labour market policies the paper addresses the problem of evaluation of the effectiveness of active labour policies in the province of bologna, a manufacturing district in northern italy, during the period 2004/2006. using surviving analysis through kaplan meier filter and a new approach to propensity score computation, the authors shows that the policies run by the labor market authorities are able to compensate the disavatanges that secondary labor forces such as migrants, old age or less educated workers have in getting a job when fired. moreover, they put new light on the transitions from temporary job to permanent jobs, and show that the probability of transitions is very low.
00978e09-e72b-40b8-b485-c670e63f52df postmarketing surveillance of potentially fatal reactions to oncology drugs: potential utility of two signal-detection algorithms purpose#r##n#several data mining algorithms (dmas) are being studied in hopes of enhancing screening of large post-marketing safety databases for signals of novel adverse events (aes). the objective of this study was to apply two dmas to the united states fda adverse event reporting system (aers) database to see whether signals of potentially fatal aes with cancer drugs might have been identified earlier than with traditional methods.
0097c7d1-ed5f-49df-82ac-11daf2226971 constructing semiconductor manufacturing performance indexes and applying data mining for manufacturing data analysis abstract the indexes for semiconductor manufacturing management are complicated and interrelated. therefore, it is hard to clarify the relationships among the indexes and to derive useful rules for production management. existing approaches rely on following individual indexes without considering the production system as a whole. this study aims to fill the gap by reviewing the related studies on semiconductor manufacturing management and developing a complete set of performance indexes in hierarchy. in addition, we apply data mining techniques for analyzing production data collected in a semiconductor fab in taiwan to validate this approach. the empirically derived patterns among the critical indexes were useful for supporting production management decisions. the results demonstrate the practical viability of this approach. this study concludes with results and discussion on future research.
0098ae60-3df6-4fae-a678-99bd476b23b0 data mining techniques for the performance analysis of a learning model - a case study paper deals with a comparative study of the application of various data mining algorithms for the performance analysis of the learning model. the learning model for mathematics is an integration of the various components used for effective learning of mathematics and assessment at the elementary level of education. performance analysis is the analysis of the data stored by the learning model in the mathematical pathway database which is used to track the progress of each child. the analysis classifies the performance of a child into average, below average and above average categories. this aids in timely intervention. the performance analysis using data mining (dm) approach validates the accuracy and efficiency of the learning model leading to reliable and authentic predictions. further any algorithm can be used for predictions of the mathematics learning trends as the performance of all techniques is comparable. this generic novel approach can be extended to other disciplines as well.
0098dbc6-0289-4e7e-8e6d-eec4b14621a6 efficient hyper-parameter optimization for nlp applications hyper-parameter optimization is an important problem in natural language processing (nlp) and machine learning. recently, a group of studies has focused on using sequential bayesian optimization to solve this problem, which aims to reduce the number of iterations and trials required during the optimization process. in this paper, we explore this problem from a different angle, and propose a multi-stage hyper-parameter optimization that breaks the problem into multiple stages with increasingly amounts of data. early stage provides fast estimates of good candidates which are used to initialize later stages for better performance and speed. we demonstrate the utility of this new algorithm by evaluating its speed and accuracy against state-of-the-art bayesian optimization algorithms on classification and prediction tasks.
00997724-e14b-4d4a-b28e-a761eda81aae hiding sensitive rules using sif-idf to preserve privacy in extracting association rules nowadays, data mining and privacy preserving are two important and fundamental issues for organizations, individuals, and data miners. data mining discovers the relations among the items of a database. some of the discovered relations are private for organizations and individuals and must not be available to others. this information is called sensitive information and the database owner tries to hide it. hiding sensitive information has some side effects for the database and insensitive information including loss of insensitive information, creation of new information that doesnt exist in the original database (ghost rules), dissimilarity in the database, etc. all the presented algorithms for privacy preserving try to sanitize databases with the least side effects. in this paper, an algorithm based on sif-idf algorithm in order to hide sensitive rules is proposed. in the proposed algorithm, heuristic technique and support-based approach are used for sanitizing databases. the aim of the proposed algorithm is reducing the side effects of database sanitization including loss of rules, runtime reduction and hiding failure. the proposed algorithm is assessed by 1.b, mdsrrc, and sif-idf algorithms and the results show the efficacy of the proposed algorithm.
0099983a-aeed-42b7-b0ec-70f5ac3989e2 intelligent systems can design optimum fracturing jobs 
0099b5d9-bf74-481f-b3c0-95957827f405 interpreting or describing? measuring verb abstraction the paper describes the results of machine learning experiments with verb classification according to the linguistic category model (lcm). the lcm typology is a well-established tool to measure language abstraction, linked to sentiment and applicable in sentiment-analysis related areas. our goal is to create automated methods of recognizing lcm verb classes. the method, demonstrated in the polish language, turns out to be very promising, especially given the upper bounds set by inter-annotator agreement.
0099efe5-fefe-471a-be12-fe569c2caff1 machine learning techniques for diagnosing and locating faults through the automated monitoring of power electronic components in shipboard power systems the management and control of shipboard medium voltage ac (mvac) and medium voltage dc (mvdc) power system architectures under fault conditions present a number of challenges. the use and resulting interaction of multiple power electronic components in mesh-like power distribution architectures possibly result in the effects of faults being detectable throughout the system, for example, line-to-hull faults on dc systems with high resistive grounding.
009a3f5e-df47-4654-8bb2-149983dfe906 learning fuzzy rules with evolutionary algorithms -- an analytic approach this paper provides an analytical approach to fuzzy rule base optimization. while most research in the area has been done experimentally, our theoretical considerations give new insights to the task. using the symmetry that is inherent in our formulation, we show that the problem of finding an optimal rule base can be reduced to solving a set of quadratic equations that generically have a one dimensional solution space. this alternate problem specification can enable new approaches for rule base optimization.
009ae6da-6c96-4739-bdde-22d8816fbb4f guaranteed safe online learning via reachability: tracking a ground target using a quadrotor while machine learning techniques have become popular tools in the design of autonomous systems, the asymptotic nature of their performance guarantees means that they should not be used in scenarios in which safety and robustness are critical for success. by pairing machine learning algorithms with rigorous safety analyses, such as hamilton-jacobi-isaacs (hji) reachability, this limitation can be overcome. guaranteed safe online learning via reachability (gsolr) is a framework which combines hji reachability with general machine learning techniques, allowing for the design of robotic systems which demonstrate both high performance and guaranteed safety. in this paper we show how the gsolr framework can be applied to a target tracking problem, in which an observing quadrotor helicopter must keep a target ground vehicle with unknown (but bounded) dynamics inside its field of view at all times, while simultaneously attempting to build a motion model of the target. the resulting algorithm was implemented on board the stanford testbed of autonomous rotorcraft for multi-agent control, and was compared to a naive safety-only algorithm and a learning-only algorithm. experimental results illustrate the success of the gsolr algorithm, even under scenarios in which the machine learning algorithm performed poorly (and would otherwise lead to unsafe actions), thus demonstrating the power of this technique.
009b20cc-a1f0-4603-9ad9-1b3985502ce9 shared memory parallelization of decision tree construction using a general data mining middleware 
009b351f-0b24-4226-8179-9a3b4ef749fa neural networks and micromechanics this is an interdisciplinary field of research involving the use of neural network techniques for image recognition applied to tasks in the area of micromechanics. the book is organized into chapters on classic neural networks and novel neural classifiers; recognition of textures and object forms; micromechanics; and adaptive algorithms with neural and image recognition applications. the authors include theoretical analysis of the proposed approach, they describe their machine tool prototypes in detail, and they present results from experiments involving microassembly, and handwriting and face recognition. this book will benefit scientists, researchers and students working in artificial intelligence, particularly in the fields of image recognition and neural networks, and practitioners in the area of microengineering.
009c8b76-1ce5-425a-a1a3-46d7963ac61f assessing the use of the som technique in data mining. 
009ca559-236e-4084-9663-7fb81f18cccf silver cluster interferences in matrix-assisted laser desorption/ionization (maldi) mass spectrometry of nonpolar polymers potential difficulties associated with background silver salt clusters during matrix-assisted laser desorption/ionization mass spectrometry (maldi-ms) of nonpolar polymers are reported. silver salt cluster ions were observed from m/z 1500 to 7000 when acidic, polar matrices, such as 2,5-dihydroxybenzoic acid (dhb), all-trans-retinoic acid (rta) or 2-(4-hydroxyphenylazo)benzoic acid (haba), were used for the analysis of nonpolar polymers. these background signals could be greatly reduced or eliminated by the use of nonpolar matrices such as anthracene or pyrene. representative examples of these background interferences are demonstrated during the analysis of low molecular weight nonpolar polymers including polybutadiene and polystyrene. nonpolar polymers analyzed with acidic, polar matrices (e.g., rta) and silver cationization reagents can yield lower quality mass spectral results when interferences due to silver clusters are present. replacing the polar matrices with nonpolar matrices or the silver salts with copper salts substantially improved the quality of the analytical results. in addition, it was found that silver contamination cannot be completely removed from standard stainless steel sample plates, although the presence of silver contamination was greatly reduced after thorough cleaning of the sample plate with aluminum oxide grit. carry-over silver may cationize polymer samples and complicate the interpretation of data obtained using nonpolar matrices in the absence of added cationization reagents.
009d015f-ee96-4904-983a-a12b338f90ae the research of universal data mining model system based on logistics data warehouse and application this paper proposes an integration system to the logistics enterprise information system in distributed heterogeneous environment. we establish a framework structure of universal data mining system based on logistics data warehouse and apply the proposed system into practical management of logistics and shipping enterprises. feature extraction and data sample classification from large-scale data warehouse is realized by constructing the logical feature space and generating the rule and pattern about the logical feature sub-space, which can help data mining system earn necessary knowledge about a specific part of a real or abstract information and further use the knowledge to match data mining models. the discussed results of illustrative example and numerical simulation show that the developed models system and methodologies can be useful and applicable to realize the intelligent decision support system in the logistics enterprise and supply chain management.
009e124c-32ee-49cb-bab8-2c12f109222e exploration of compiler optimization sequences using clustering-based selection due to the large number of optimizations provided in modern compilers and to compiler optimization specific opportunities, a design space exploration (dse) is necessary to search for the best sequence of compiler optimizations for a given code fragment (e.g., function). as this exploration is a complex and time consuming task, in this paper we present dse strategies to select optimization sequences to both improve the performance of each function and reduce the exploration time. the dse is based on a clustering approach which groups functions with similarities and then explore the reduced search space provided by the optimizations previously suggested for the functions in each group. the identification of similarities between functions uses a data mining method which is applied to a symbolic code representation of the source code. the dse process uses the reduced set identified by clustering in two ways: as the design space or as the initial configuration. in both ways, the adoption of a pre-selection based on clustering allows the use of simple and fast dse algorithms. our experiments for evaluating the effectiveness of the proposed approach address the exploration of compiler optimization sequences considering 49 compilation passes and targeting a xilinx microblaze processor, and were performed aiming performance improvements for 41 functions. experimental results reveal that the use of our new clustering-based dse approach achieved a significant reduction on the total exploration time of the search space (18x over a genetic algorithm approach for dse) at the same time that important performance speedups (43% over the baseline) were obtained by the optimized codes.
009e4810-c8c3-402f-8bd2-670d30a91eaf use data mining to improve student retention in higher education - a case study 
009ea401-07ed-4c76-9ac4-da27a8ec4c94 smart baseline models for solar irradiation forecasting this work presents a kind of smart baseline models for solar irradiation forecasting, as models are only fed with meteorological records and solar-computed values, easy-to-obtain inputs that facilitate their implementation worldwide. global horizontal irradiation (ghi) is predicted for horizons of 1 h in a site of southeast spain. two types of approaches are undertaken: fixed models, trained just once with a global database, and moving models, where the training database is updated based on the features of the testing sample. the approaches are implemented with two machine learning algorithms, support vector regression (svr) and random forest (rfs), along with the classic linear regression and knn. besides, genetic algorithms (gas) are used to automate the training process of fixed models, a task traditionally performed based on the experience or the researcher.#r##n##r##n#significant improvements were obtained over the basic persistence methods with both approaches. in the case of moving models, results proved that the best approach to update the calibration set was by computing the euclidean distance in the principal components space. results of both approaches were comparable in terms of mae and forecast skill (s), though slightly superior predictions were obtained with the moving svr, with a forecast skill ranging from 8% to 23% and a testing mae ranging from 49 to 64 w/m2w/m2 for the different states of cloudiness. anyway, both approaches are valid baselines to compare new forecasting models fed with more difficult-to-obtain features, supplementing the classic but naive persistence models.
009ecda5-985a-4b86-a429-85df736b3c37 large-scale experimental evaluation of gpu strategies for evolutionary machine learning we exhaustively evaluate two gpu strategies for evolutionary machine learning systems.we use synthetic datasets to thoroughly explore the space of problem characteristics.the findings of the evaluation on synthetic datasets translate to real-world problems.our findings help avoiding a blind trial-and-error calibration of gpu data mining code. graphics processing units (gpus) are effective tools for improving the efficiency of many computationally demanding algorithms. gpus have been particularly effective at speeding up the evaluation stage of evolutionary machine learning systems. the speedups obtained in these tasks, depend on many factors: dataset characteristics, the parallel strategy of the gpu code and the fit of the gpu code within the rest of the learning system. a solid understanding of all these factors is required to choose and adjust the most suitable gpu strategy in different scenarios. in this paper we present a large-scale performance evaluation of two gpu strategies for speeding up the evaluation of evolutionary machine learning systems. we use highly-tuneable synthetic problems to exhaustively explore the space of problem characteristics and determine the type of problems where each strategy performs best. the lessons learnt from this extensive evaluation are further confirmed by running experiments on a broad range of real-world datasets. through this thorough evaluation we obtain a solid understanding of the capabilities and limitations of the evaluated gpu strategies for boosting the efficiency of evolutionary machine learning systems.
009efdd3-1437-42b6-a38e-e6bafcde41a0 tagging products using image classification associating labels with online products can be a labor-intensive task. we study the extent to which a standard "bag of visual words" image classifier can be used to tag products with useful information, such as whether a sneaker has laces or velcro straps. using scale invariant feature transform (sift) image descriptors at random keypoints, a hierarchical visual vocabulary, and a variant of nearest-neighbor classification, we achieve accuracies between 66% and 98% on 2- and 3-class classification tasks using several dozen training examples. we also increase accuracy by combining information from multiple views of the same product.
009f56a0-e26e-4cfd-b066-545b78d45898 mining medical data: bridging the knowledge divide due to the signi cant amount of data generated by modern medicine there is a growing reliance on tools such as data mining and knowledge discovery to help make sense and comprehend such data. the success of this process requires collaboration and interaction between such methods and medical professionals. therefore an important question is: how can we strengthen the relationship between two traditionally separate fields (technology and medicine) in order to work simultaneously towards enhancing knowledge in modern medicine. to address this question, this study examines the application of data mining techniques to a large asthma medical dataset. a discussion introducing various methods for a smooth approach, straying from the `jack of all trades, master of none' to a modular cooperative approach for a successful outcome is pro-posed. the results of this study support the use of data mining as a useful tool and highlight the advantages on a global scale of closer relations between the two distinct fields. the exploration of crisp methodology suggests that a `one methodology fits all approach' is not appropriate, but rather combines to create a hybrid holistic approach to data mining.
009f68ae-4afb-4c98-b347-9968745b19cc a variational approach for bayesian blind image deconvolution in this paper, the blind image deconvolution (bid) problem is addressed using the bayesian framework. in order to solve for the proposed bayesian model, we present a new methodology based on a variational approximation, which has been recently introduced for several machine learning problems, and can be viewed as a generalization of the expectation maximization (em) algorithm. this methodology reaps all the benefits of a "full bayesian model" while bypassing some of its difficulties. we present three algorithms that solve the proposed bayesian problem in closed form and can be implemented in the discrete fourier domain. this makes them very cost effective even for very large images. we demonstrate with numerical experiments that these algorithms yield promising improvements as compared to previous bid algorithms. furthermore, the proposed methodology is quite general with potential application to other bayesian models for this and other imaging problems.
009fb92e-6b5d-4a9a-a0c5-085c082f7db2 curated micrornas in urine and blood fail to validate as predictive biomarkers for high-risk prostate cancer. purpose #r##n#the purpose of this study was to determine if microrna profiling of urine and plasma at radical prostatectomy can distinguish potentially lethal from indolent prostate cancer.#r##n#materials and methods #r##n#a panel of micrornas was profiled in the plasma of 70 patients and the urine of 33 patients collected prior to radical prostatectomy. expression of micrornas was correlated to the clinical endpoints at a follow-up time of 3.9 years to identify micrornas that may predict clinical response after radical prostatectomy. a machine learning approach was applied to test the predictive ability of all micrornas profiled in urine, plasma, and a combination of both, and global performance assessed using the area under the receiver operator characteristic curve (auc). validation of urinary expression of mirnas was performed on a further independent cohort of 36 patients.#r##n#results #r##n#the best predictor in plasma using eight mirs yielded only moderate predictive performance (auc = 0.62). the best predictor of high-risk disease was achieved using mir-16, mir-21 and mir-222 measured in urine (auc = 0.75). this combination of three micrornas in urine was a better predictor of high-risk disease than any individual microrna. using a different methodology we found that this set of mirnas was unable to predict high-volume, high-grade disease.#r##n#conclusions #r##n#our initial findings suggested that plasma and urinary profiling of micrornas at radical prostatectomy may allow prognostication of prostate cancer behaviour. however we found that the microrna expression signature failed to validate in an independent cohort of patients using a different platform for pcr. this highlights the need for independent validation patient cohorts and suggests that urinary microrna signatures at radical prostatectomy may not be a robust way to predict the course of clinical disease after definitive treatment for prostate cancer.
009fbdf3-7726-4e53-957d-e46f429770ec a novel page rank algorithm for web mining based on user's interest the web can be viewed as the largest database available and presents a challenging task for effective design and access. with the tremendous growth of web, the main objective is to provide relevant information to the user to fulfil their needs. data mining applied to web has the poten- tial to be quiet beneficial. web mining is the mining of data related to world wide web. web search engines are usually designed to serve user requirement without considering the user's interest. in this paper we present web content mining technique known as personalization. with personalization, web access or the contents of a web page are modified to better fit the desires of the user. this may involve actually creating web pages that are unique per user or using the desires of a user to determine what web documents to re- trieve. this paper deals with the analysis of various page rank algorithms for web mining based on user's interest.
009fc54a-7530-4fc0-9ade-fd8e6790f456 recommending relevant code artifacts for change requests using multiple predictors finding code artifacts affected by a given change request is a time-consuming process in large software systems. various approaches have been proposed to automate this activity, e.g., based on information retrieval. the performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request. thus, we propose to use multiple prediction approaches in combination with machine learning. first experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance.
00a07b28-0389-4ffb-ac93-e50adc8136a9 weka---experiences with a java open-source project weka is a popular machine learning workbench with a development life of nearly two decades. this article provides an overview of the factors that we believe to be important to its success. rather than focussing on the software's functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project.
00a09637-340a-48a4-bb78-8d70249703c3 a review of outlier prediction techniques in data mining 
00a0acfa-6c77-442f-a5ba-ad68896ee46c a study on web usage mining for web based adaptive educational system education is the process of acquiring knowledge through learning. in modern era, technology based education via computer and internet gained the attention of the users. data mining involves prominently in recent educational system to improvise the knowledge. studies proved that data mining develops students learning behavior in a great sense. the reason behind this is its ability to evaluate and exposing the hidden data that causes success or failure. web usage mining falls under one among the categories of data mining, is the process of extracting useful information from server logs as per the interest of the user. the objective of our paper is to review recent research papers which are exploring the significance of web usage mining in educational system. for this study, we selected recent papers from ieee, elsevier, acm and other peer reviewed journals.
00a0ce0b-eb92-4806-a57a-c55fa7c4a462 predicting agent performance in large-scale electricity demand shifting abstracta variety of multi-agent systems methods has been proposed for forming cooperatives of interconnected agents representing electricity producers or consumers in the smart grid. one major problem that arises in this domain is assessing participating agents' uncertainty, and correctly predicting their future behaviour regarding power consumption shifting actions. in this paper we adopt various machine learning techniques and use these to effectively monitor the trustworthiness of agent statements regarding their final shifting actions. in particular, we evaluate the performance of four approaches, one based on a histogram filter, and three on regression approaches, that is, gaussian process, k-nearest neighbours, and kernel regression. we incorporate these to aggregate individual forecasts within a directly applicable scheme for providing cooperative electricity demand shifting services. experiments were conducted on real-world datasets from thousands of users located in kissamos, a municipality of c...
00a162df-dda9-4f4e-8c46-f4ee147cb01f comparative analysis of gene sets in the gene ontology space under the multiple hypothesis testing framework the gene ontology (go) resource can be used as a powerful tool to uncover the properties shared among, and specific to, a list of genes produced by high-throughput functional genomics studies, such as microarray studies. in the comparative analysis of several gene lists, researchers maybe interested in knowing which go terms are enriched in one list of genes but relatively depleted in another. statistical tests such as fisher's exact test or chi-square test can be performed to search for such go terms. however, because multiple go terms are tested simultaneously, individual p-values from individual tests do not serve as good indicators for picking go terms. furthermore, these multiple tests are highly correlated, usual multiple testing procedures that work under an independence assumption are not applicable. in this paper we introduce a procedure, based on false discovery rate (fdr), to treat this correlated multiple testing problem. this procedure calculates a moderately conserved estimator of q-value for every go term. we identify the go terms with q-values that satisfy a desired level as the significant go terms. this procedure has been implemented into the gosurfer software. gosurfer is a windows based graphical data mining tool. it is freely available at http://www.gosurfer.org.
00a1a73e-1d96-4aa8-b0e1-52d456e936ea multi-objective genetic programming for feature extraction and data visualization feature extraction transforms high-dimensional data into a new subspace of lower dimensionality while keeping the classification accuracy. traditional algorithms do not consider the multi-objective nature of this task. data transformations should improve the classification performance on the new subspace, as well as to facilitate data visualization, which has attracted increasing attention in recent years. moreover, new challenges arising in data mining, such as the need to deal with imbalanced data sets call for new algorithms capable of handling this type of data. this paper presents a pareto-based multi-objective genetic programming algorithm for feature extraction and data visualization. the algorithm is designed to obtain data transformations that optimize the classification and visualization performance both on balanced and imbalanced data. six classification and visualization measures are identified as objectives to be optimized by the multi-objective algorithm. the algorithm is evaluated and compared to 11 well-known feature extraction methods, and to the performance on the original high-dimensional data. experimental results on 22 balanced and 20 imbalanced data sets show that it performs very well on both types of data, which is its significant advantage over existing feature extraction algorithms.
00a29c93-2b1e-45da-8f81-a0be4cd40eda why can't a virtual character be more like a human: a mixed-initiative approach to believable agents believable agents have applications in a wide range of human computer interaction-related domains, such as education, training, arts and entertainment. autonomous characters that behave in a believable manner have the potential to maintain human users' suspense of disbelief and fully engage them in the experience. however, how to construct believable agents, especially in a generalizable and cost effective way, is still an open problem. this paper compares the two common approaches for constructing believable agents -- human-driven and artificial intelligence-driven interactive characters -- and proposes a mixed-initiative approach in the domain of interactive training systems. our goal is to provide the user with engaging and effective educational experiences through their interaction with our system.
00a2d607-158b-49e1-b676-9b29c3945868 chimera: large-scale classification using machine learning, rules, and crowdsourcing large-scale classification is an increasingly critical big data problem. so far, however, very little has been published on how this is done in practice. in this paper we describe chimera, our solution to classify tens of millions of products into 5000+ product types at walmartlabs. we show that at this scale, many conventional assumptions regarding learning and crowdsourcing break down, and that existing solutions cease to work. we describe how chimera employs a combination of learning, rules (created by in-house analysts), and crowdsourcing to achieve accurate, continuously improving, and cost-effective classification. we discuss a set of lessons learned for other similar big data systems. in particular, we argue that at large scales crowdsourcing is critical, but must be used in combination with learning, rules, and in-house analysts. we also argue that using rules (in conjunction with learning) is a must, and that more research attention should be paid to helping analysts create and manage (tens of thousands of) rules more effectively.
00a2eb88-84be-4690-ae57-9f68f39f067d youchoose: a performance interface enabling convenient and efficient qos support for consolidated storage systems currently the qos requirements for disk-based storage systems are usually presented in the form of service-level agreement (sla) to bound i/o measures such as latency and throughput of i/o requests. however, sla is not an effective performance interface for users to specify their required i/o service quality for two major reasons. first, for users, it is difficult to determine appropriate latency and throughput bounds to ensure their application performance without resource over-provisioning. second, for storage system administrators, it is a challenge to estimate a user's real resource demand because the specified sla measures are not consistently correlated with the user's resource demand. this makes resource provisioning and scheduling less informative and could greatly reduce system efficiency. we propose the concept of reference storage system (rss), which can be a storage system chosen by users and whose performance can be measured off-line and mimicked on-line, as a performance interface between applications and storage servers. by designating an rss to represent i/o performance requirement, a user can expect the performance received from a shared storage server servicing his i/o workload is not worse than the performance received from the rss servicing the same workload. the storage system is responsible for implementing the rss interface. the key enabling techniques are a machine learning model that derives request-specific performance requirements and an rss-centric scheduling that efficiently allocates resource among requests from different users. the proposed scheme, named as youchoose, supports the user-chosen performance interface through efficiently implementing and migrating virtual storage devices in a host storage system. our evaluation based on trace-driven simulations shows that youchoose can precisely implement the rss performance interface, achieve a strong performance assurance and isolation, and improve the efficiency of a consolidated storage system consisting of different types of storage devices.
00a2fbba-d50a-4a33-bdde-b2b58940da20 using published medical results and non-homogenous data in rule learning many factors limit researchers from accessing studies' original data sets. as a result, much medical and healthcare research is based off of systematic reviews and meta-analysis of published results. however, when research involves the use of aggregated data from multiple studies, traditional machine learning-based means of analysis cannot be used. this paper describes diversity of data and results available in published man-uscripts, and relates them to a rule learning method that can be applied to build classification and predictive models from such input. the method can be used to support meta-analysis and systematic reviews. two ap-plication areas are used to illustrate the discussed issues: diagnosis of liver diseases in patients with metabolic syndrome, and detection of polycystic ovary syndrome.
00a39ac6-de9f-4b2b-a434-de1cc55a2541 research on intelligent architecture of real-time dispatching automation for power systems first of all, this article described current problems on electric power automation system, raised the concept of intelligent power dispatching architecture (ipda) and its corresponding intelligence meaning, it is an extension to the research field for the "intelligence" among artificial intelligence. the problems mentioned on this article can be solved through systematic research of ipda. the real time dispatching intelligent power automation system architecture has been designed, and the meaning of intelligence corresponding to the different level of the architecture has been analyzed based on iec61xxx series industry standard and the mas theories on artificial intelligence. part of concepts has been utilized in reality.
00a3e17f-16ca-4fd2-8296-990b38373cd3 selected papers from the ninth international conference on computational intelligence and security the 2012 international conference on computational intelligence and security (cis) is the ninth one focusing on all areas of two crucial fields in information processing: computational intelligence (ci) and information security (is). in particular, the cis conference provides a platform to explore the potential applications of ci models, algorithms, and technologies to is.#r##n##r##n##r##n#among all accepted papers in cis conference 2012, five papers were further screened and extended to be included in this special issue. they are the following:#r##n##r##n#(i) attribute index and uniform design based multiobjective association rule mining with evolutionary algorithm,#r##n#(ii) reliable execution based on cpn and skyline pptimization for web service composition,#r##n#(iii) robust adaptive control for a class of uncertain nonlinear systems with time-varying delay,#r##n#(iv) bounds of the spectral radius and the nordhaus-gaddum type of the graphs,#r##n#(v) the effects of different representations on static structure analysis of computer malware signatures.#r##n##r##n##r##n##r##n#the first paper formulates the association rule mining as a multiobjective problem, through which the algorithm of attribute index and uniform design-based multiobjective association rule mining with evolutionary algorithm is presented without the user-specified minimum support and minimum confidence anymore. experiments on several databases have demonstrated that the proposed algorithm has excellent performance and that it can significantly reduce the number of comparisons and time consumption. the second paper is to employ the transactional properties and nonfunctional quality-of-service (qos) properties for selecting the web services. furthermore, the third paper presents an adaptive neural control design for a class of perturbed nonlinear mimo time-varying delay systems in a block-triangular form. the proposed control guarantees that all closed-loop signals remain bounded, while the output tracking error dynamics converge to a neighborhood of the desired trajectories. the simulation results have demonstrated the effectiveness of the proposed control scheme. the fourth paper is to study the upper bounds for the spectral radius in quantum chemistry. as a result, an upper bound of the nordhaus-gaddum type is obtained for the sum of laplacian spectral radius of a connected graph and its complement. lastly, the fifth paper is to evaluate a static structure approach to malware modeling using the growing malware signature databases. it has been shown that it is possible to apply standard sequence alignment techniques in bioinformatics to improve accuracy of distinguishing between worm and virus signatures if malware signatures are represented as artificial protein sequences. moreover, aligned signature sequences can be mined through traditional data mining techniques to extract metasignatures that help distinguish between viral and worm signatures.#r##n##r##n#to sum up, the previously mentioned papers will provide readers and researchers with some useful ideas in the fields of computational intelligence and security.#r##n##r##n##r##n#yiu-ming cheung#r##n##r##n#yuping wang#r##n##r##n#hailin liu#r##n##r##n#xiaodong li
00a50d2d-5c7a-42d8-9f21-19768e340fc0 nonlinear committee pattern classification methods which combine outputs of multiple pattern classifiers to enhance the overall classification rate are studied. specific attention is given to combination rules which are independent of the input feature vectors. potential performance enhancement and limits of this so called stack generalization method are discussed. in particular, a phenomenon called "alias" is introduced which gives an upper bound of the performance which can be achieved using stack generation for a given set of member classifiers. experimentation using several machine learning databases are reported.
00a59fa9-a2da-4af6-bd48-86971fc6a5cc discriminative restricted boltzmann machine for invariant pattern recognition with linear transformations how to make a machine automatically achieve invariant pattern recognition like human brain is still very challenging in machine learning community. in this paper, we present a single hidden-layer network ticlassrbm for invariant pattern recognition by incorporating linear transformations into discriminative restricted boltzmann machine. in our model, invariant feature extraction and pattern classification can be implemented simultaneously. the mapping from input features to class label is represented by two groups of weights: transformed weights that connect hidden units to data, and pooling weights that connect pooling units yielded by probabilistic max-pooling to class label. all weights play an important role in the invariant pattern recognition. moreover, ticlassrbm can handle general transformations contained in images, such as translation, rotation and scaling. the experimental studies on the variations of mnist and norb datasets demonstrate that the proposed model yields the best performance among some comparative models.
00a62666-0791-4120-beed-095a83aed8bf generating teacher adapted suggestions for improving distance educational systems with sigma in this paper we present sigma, an adaptable feedback generation tool for teachers which gives information about course development in an educational web environment with diagnostic capabilities. first student results are analysed using statistical calculations and data mining techniques. afterwards, the feedback provision is carried out by a rule-based system that recognizes anomalous behaviour patterns among the analysis results and provides some suggestions for improvement which are adapted to teacher strategies and preferences by means of a teacher model. we present the main proposal and the implementation and a test of a restricted prototype.
00a6d2d8-8e8d-422f-9b99-e8eb86d3f513 natural gas pipeline leak detection based on data mining using data mining's decision tree classification, dbscn cluster analysis, and k-nearest neighbor algorithm realizes the information mining of natural gas pipeline leak, and alse uncovers the objective laws behind the natural gas pipeline transmission, intrinsically linking to the each parameter and development trend. we could reduce the risk of accidents and economic losses, in order to control the natural gas transmission in advance.
00a71fc0-30d2-47f3-8b6a-39ad25c28733 emotion recognition with eigen features of frequency band activities embedded in induced brain oscillations mediated by affective pictures in this study, singular spectrum analysis (ssa) has been used for the first time in order to extract emotional features from well-defined electroencephalography (eeg) frequency band activities (bas) so-called delta (0.54hz), theta (48hz), alpha (816hz), beta (1632hz), gamma (3264hz). these five bas were estimated by applying sixth-level multi-resolution wavelet decomposition (mrwd) with daubechies wavelets (db-8) to single channel nonaveraged emotional eeg oscillations of 6 s for each scalp location over 16 recording sites (fp1, fp2, f3, f4, f7, f8, c3, c4, p3, p4, t3, t4, t5, t6, o1, o2). every trial was mediated by different emotional stimuli which were selected from international affective picture system (iaps) to induce emotional states such as pleasant (p), neutral (n), and unpleasant (up). largest principal components (pcs) of bas were considered as emotional features and data mining approaches were used for the first time in order to classify both three different (p, n, up) and two contrasting (p and up) emotional states for 30 healthy controls. emotional features extracted from gamma bas (gbas) for 16 recording sites provided the high classification accuracies of 87.1% and 100% for classification of three emotional states and two contrasting emotional states, respectively. in conclusion, we found the followings: (1) eigenspectra of high frequency bas in eeg are highly sensitive to emotional hemispheric activations, (2) emotional states are mostly mediated by gba, (3) pleasant pictures induce the higher cortical activation in contrast to unpleasant pictures, (4) contrasting emotions induce opposite cortical activations, (5) cognitive activities are necessary for an emotion to occur.
00a7af82-eda7-416f-928c-81c453162757 leveling the new old transcendence: cognitive coherence in the era of beyondness* the underlying claim of this essay is that contemporary technonoetic culture--from artificial intelligence systems, to genetics, to nanotechnology--is rooted in the binary hyper-logic of transcendence. this principle of impossible-to-resolve oppositions was introduced in its extreme form in late neoplatonism and powerfully re-articulated for the moderns by the romantics. despite claims that we exist in a multiverse of multiple realities, a rhetoric of beyondness continues to undermine any notion of bringing coherence to physical and mental worlds. but recent developments in cognitive science and neurobiology allow us to consider how knowledge might be extended and distributed into cross-linking patterns. uncannily resembling the convergences of times and spaces in a single individual stimulated by viewing a cornell box or the intersective universes of the cabinet of wonders, phenomenologically-inflected consciousness studies offer a coalescing alternative to the vertical impulse of vanishing perspectives.
00a8426b-9d42-492a-addd-58b0d42f4c1a information management systems for pharmacogenomics the value of high-throughput genomic research is dramatically enhanced by association with key patient data. these data are generally available but of disparate quality and not typically directly associated. a system that could bring these disparate data sources into a common resource connected with functional genomic data would be tremendously advantageous. however, the integration of clinical and accurate interpretation of the generated functional genomic data requires the development of information management systems capable of effectively capturing the data as well as tools to make that data accessible to the laboratory scientist or to the clinician. in this review these challenges and current information technology solutions associated with the management, storage and analysis of high-throughput data are highlighted. it is suggested that the development of a pharmacogenomic data management system which integrates public and proprietary databases, clinical datasets, and data mining tools embedded in a...
00a9012d-554b-4e29-99fc-57512d459740 hand-written and automatically extracted rules for polish tagger stochastic approaches to tagging of polish brought results far from being satisfactory however, successful combination of hand-written rules and a stochastic approach to czech, as well, as some initial experiments in acquisition of tagging rules for polish revealed potential capabilities of a rule based approach the goals are: to define a language of tagging constraints, to construct a set of reduction rules for polish and to apply machine learning to extraction of tagging rules a language of functional tagging constraints called joskipi is proposed an extension to the c4.5 algorithm based on introducing complex joskipi operators into decision trees is presented construction of a preliminary hand-written tagging rules for polish is discussed finally, the results of the comparison of different versions of the tagger are given.
00a91993-5f3c-4090-8377-cb6d6eb45cb6 rethinking context as a social construct abstract   this paper argues that in addition to the familiar approach using formal contexts, there is now a need in artificial intelligence to study contexts as social constructs. as a successful example of the latter approach, i draw attention to interpretation (in the sense of literary theory), viz, the reconstruction of the intended meaning of a literary text that takes into account the context in which the author assumed the reader would place the text. an important contribution here comes from wendell harris, enumerating the seven crucial dimensions of context: knowledge of reality, knowledge of language, and the authorial, generic, collective, specific, and textual dimensions. finally, two recent approaches to interpretation, due to jon barwise and jerry hobbs, are analyzed as useful attempts which also come to grips with the notion of context.  it must be noted that there has been a considerable body of contributions connecting linguistic structure with social context. for example, anthropological linguistics, from bronislaw malinowski onwards, has underlined the cultural context of discourse as essential to meaning. this viewpoint became prominent with the emergence of the ethnography of speaking in anthropology. thus, conversation analysis represents a consistent formal effort to contribute to an analysis of the nature of context. while this paper emphasizes and reviews the literary theory approach, it makes various contacts with works of the latter kind (e.g., the landmark contributions of erving goffman, john gumperz, william hanks, john heritage, dell hymes, et al.) in order to deliver a more balanced and complete study of the dimensions of context.
00aa405b-a480-432b-9412-3aac0d3864fe web search with personalization and knowledge although many search engines provide relevantly good search results to the user, they do not consider personal, domain-specific preferences in their searching or ranking algorithms. in an intranet environment we could collect background information about users such as their expertise. if we can accumulate, categorize and personalize web usage information, it can be used to help the user search web pages efficiently and effectively. data analysis and mining can further facilitate web searching in an intelligent way. this paper describes internet search advisor (isa), a personalized, knowledge-driven search system that helps the user find informative web sites. the isa supports multi-dimensional data analysis and data mining based on association rules and sequential patterns.
00aa717e-f997-40ea-b6c8-c1bd17517cdd intrinsic plagiarism detection using complexity analysis we introduce kolmogorov complexity measures as a way of extracting structural information from texts for intrinsic plagiarism detection. kolmogorov complexity measures have been used as features in a variety of machine learning tasks including image recognition, radar signal classification, eeg classification, dna analysis, speech recognition and some text classification tasks (chi and kong, 1998; zhang, hu, and jin, 2003; bhattacharya, 2000; menconi, benci, and buiatti, 2008; frank, chui, and witten, 2000; dalkilic et al., 2006; seaward and saxton, 2007; seaward, inkpen, and nayak, 2008). intrinsic plagiarism detection uses no external corpus for document comparison and thus plagiarism must be detected solely on the basis of style shifts within the text to be analyzed. given the small amount of text to be analyzed, feature extraction is of particular importance. we give a theoretical background as to why complexity measures are meaningful and we introduce some experimental results on the pan'09 intrinsic plagiarism corpus. we show complexity features based on the lempel-ziv compression algorithm slightly increase performance over features based on normalized counts. furthermore we believe that more sophisticated compression algorithms which are suited to com- pressing the english language show great promise for feature extraction for various text classification problems.
00aaae59-4657-4595-8d61-471c4f73c4ae automatically mining similar warnings and warning combinations static analysis tools usually report many warnings. to help with the inspection tasks, we present an approach that uses data mining techniques to work on the results of static analysis of source code to classify similar warnings and warning combinations, and makes the final warning report easier to handle. the underlying algorithms were implemented inside our static analysis tool and used successfully to prioritizing static analysis warnings in large, critical embedded applications.
00aae1f5-894e-436a-aab9-2c42ee5d947a k-mean clustering and correlation analysis in recognition of weather impact on radio signal this paper deals with using a k-means clustering which is used for decision what parameter related to weather affects a propagation of radio waves in mobile telecommunication network. there were analysed parameters from a meteorological service as well as the parameters related to global system of mobile communication network. for this purpose, we studied and used theory of data mining. the second part of the paper is focused on the significant weather parameters as results of k-means analyse. consequently, there have been found some dependencies between weather conditions and receive level using a mathematical tools of correlation analysis via matlab.
00ab31ed-2a52-45d1-a9b7-53ce55550f18 learning in robocup keepaway using evolutionary algorithms manually coordinating the efforts of many autonomous agents can be a formidable challenge, so the idea of using machine learning techniques (such as evolutionary algorithms) to produce such coordination is attractive. we present a study using evolutionary algorithms to train autonomous agents to play the game of keepaway  a sub-problem of the robocup robotic soccer league. our results exceed those previously produced using other methods.
00abec8b-0be5-46a0-bfe2-a39d10f26267 decision tree based data classification for marine wireless communication wireless data communication along with data classification techniques has got wider acceptance in various marine wireless applications. this paper exploits the power of machine learning algorithm to classify wireless communication dataset for effective decision making in marine sector. fishing is among the most risky of professions in the world because once out on the sea, the fishermen are subject to various oceanographic conditions. the unreliable communication between the fishing fleets and to the shore is a serious problem when they face emergency situations like bad weather, border attacks, natural calamities etc. this paper is intended to develop an algorithm to determine the most influential parameters by considering signal strength, wind speed etc. which helps to track, classify and disseminate information to the fishing fleets while they are in deep sea. a decision tree based classification is proposed to find the best node based on the signal strength and the environmental conditions and the scenario has been simulated using ns2 platform. an ensemble based learning algorithm with bagging and adaptive boosting in c4.5 is also employed for improving the performance. the performance comparison has been done and the result shows that the boosted decision tree algorithm has got highest classification accuracy of 95.73%.
00abed89-91bf-41f8-819e-c194caa3df92 4.3.3 an architecture for an intelligent requirements elicitation and assessment assistant abstract#r##n##r##n#requirements engineering is crucial to the success of the system engineering process. unfortunately, while current generation tools provide good support to requirements capture, they offer only modest assistance for cognitive tasks such as requirements assessment. this paper opens by introducing the attributes needed to define a requirement and the qualities of good requirements and requirements sets. an argument is then presented for the inclusion of artificial intelligence to perform requirements analysis and this is followed by a review of intelligent requirement analysis tools identifying the need for enhancements. from this basis, the paper describes the architecture of an intelligent requirements elicitation and assessment assistant that is currently under construction.
00ac1861-653c-4adb-9ca5-f238882698cf sequential event prediction with association rules we consider a supervised learning problem in which data are revealed sequentially and the goal is to determine what will next be revealed. in the context of this problem, algorithms based on association rules have a distinct advantage over classical statistical and machine learning methods; however, there has not previously been a theoretical foundation established for using association rules in supervised learning. we present two simple algorithms that incorporate association rules, and provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. we include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an \adjusted condence" measure that provides a weaker minimum support condition that has advantages over the strict minimum support. the paper brings together ideas from statistical learning theory, association rule mining and bayesian analysis.
00acc622-1f22-49c3-8199-6f06885fd3a5 a perceptually-based theory of mind for agent interaction initiation we endow agents with the capability to open interactions based on their perception of the gaze and direction of attention of others in a virtual environment. the capability is geared towards the earliest part of interaction initiation, where agents may be at some distance from each other and may not initially have knowledge of each other's presence. an important idea in our work is that the start of interaction be initiated in a graceful manner involving exchanges of subtle cues before overt interaction commitments are made. synthetic vision, attention and memory are used to implement the perceptually-based agent theory of mind. theory of mind is used to infer information about the intention of the other to interact based on their eye, head and body directions, locomotion and greeting gestures. an agent's interaction behavior is therefore driven not only by its interaction goal, but also by its theory of the goal of the other based on perception. we have implemented this system and used it to automate and evaluate social interaction behaviors between humans and agents in a virtual environment.
00ad90bf-22fa-417a-8e00-8d1df2c85d6f a generic local algorithm for mining data streams in large distributed systems in a large network of computers or wireless sensors, each of the components (henceforth, peers) has some data about the global state of the system. much of the system's functionality such as message routing, information retrieval and load sharing relies on modeling the global state. we refer to the outcome of the function (e.g., the load experienced by each peer) as the \emph{model} of the system. since the state of the system is constantly changing, it is necessary to keep the models up-to-date. computing global data mining models e.g. decision trees, k-means clustering in large distributed systems may be very costly due to the scale of the system and due to communication cost, which may be high. the cost further increases in a dynamic scenario when the data changes rapidly. in this paper we describe a two step approach for dealing with these costs. first, we describe a highly efficient \emph{local} algorithm which can be used to monitor a wide class of data mining models. then, we use this algorithm as a feedback loop for the monitoring of complex functions of the data such as its k-means clustering. the theoretical claims are corroborated with a thorough experimental analysis.
00ad97b7-c933-44fb-96f5-d872783ee95f a machine learning approach to identification and resolution of one-anaphora we present a machine learning approach to identifying and resolving one-anaphora. in this approach, the system first learns to distinguish different uses of instances of the word one; in the second stage, the antecedents of those instances of one that are classified as anaphoric are then determined. we evaluated our approach on written texts drawn from the informative domains of the british national corpus (bnc), and achieved encouraging results. to our knowledge, this is the first learning-based system for the identification and resolution of one-anaphora.
00af0283-b21a-499c-b493-745acc27f044 an empirical study on user access control in online social networks in recent years, access control in online social networks has attracted academia a considerable amount of attention. previously, researchers mainly studied this topic from a formal perspective. on the other hand, how users actually use access control in their daily social network life is left largely unexplored. this paper presents the first large-scale empirical study on users' access control usage on twitter and instagram. based on the data of 150k users on twitter and 280k users on instagram collected consecutively during three months in new york, we have conducted both static and dynamic analysis on users' access control usage. our findings include: female users, young users and asian users are more concerned about their privacy; users who enable access control setting are less active and have smaller online social circles; global events and important festivals can influence users to change their access control setting. furthermore, we exploit machine learning classifiers to perform an access control setting prediction. through experiments, the predictor achieves a fair performance with the auc equals to 0.70, indicating whether a user enables her access control setting or not can be predicted to a certain extent.
00af8d0c-e05b-49c4-b3a9-fffe63d90f19 multivariate image texture by multivariate variogram for multispectral image classification. traditional image texture measure usually allows a texture description of a single band of the spectrum, characterizing the spatial variability of gray-level values within the singleband image. a problem with the approach while applied to multispectral images is that it only uses the texture information from selected bands. in this paper, we propose a new multivariate texture measure based on the multivariate variogram. the multivariate texture is computed from all bands of a multispectral image, which characterizes the multivariate spatial autocorrelation among those bands. in order to evaluate the performance of the proposed texture measure, the derived multivariate texture image is combined with the spectral data in image classification. the result is compared to classifications using spectral data alone and plus traditional texture images. a machine learning classifier based on support vector machines (svms) is used for image classification. the experimental results demonstrate that the inclusion of multivariate texture information in multispectral image classification significantly improves the overall accuracy, with 5 to 13.5 percent of improvement, compared to the classification with spectral information alone. the results also show that when incorporated in image classification as an additional band, the multivariate texture results in high overall accuracy, which is comparable with or higher than the best results from the existing single-band and two-band texture measures, such as the variogram, cross variogram and gray-level co-occurrence matrix (glcm) based texture. overall, the multivariate texture provides the useful spatial information for land-cover classification, which is different from the traditional single band texture. moreover, it avoids the band selection procedure which is prerequisite to traditional texture computation and would help to achieve high accuracy in the most classification tasks.
00b005b7-0cec-455a-afa5-c8c49318891c layered approximation approach to knowledge elicitation in machine learning domain knowledge elicitation constitutes a crucial task in designing effective machine learning algorithm, and is often indispensable in problem domains that display a high degree of internal complexity such as knowledge discovery and data mining, the recognition of structured objects, human behavior prediction, or multi-agent cooperation. we show how to facilitate this difficult and sometimes tedious task with a hierarchical concept learning scheme, designed to cope with the inherent vagueness and complexity of knowledge therein used. we present how our approach, based on rough mereology and approximate reasoning frameworks, correlate to other well established approaches to machine learning.
00b1b63f-59a2-4793-9e0f-07831b7acb38 mining concept-drifting and noisy data streams using ensemble classifiers mining concept drifting data stream is a challenging area for data mining research. recent years have witnessed an averaging ensemble classifier which is based on the learnable assumption, although this ensemble classifier is an efficient algorithm for mining concept-drifting data streams, it is still inadequate to represent real-world data streams with noisy data. in this paper, we propose a novel ensemble classifier framework for mining concept-drifting data streams with noise. the method, called weap-i, which trains a weighted ensemble classifier on the most n data chunks and trains an averaging ensemble classifier on the most recent data chunk. all the base classifiers are combined to form the weap-i ensemble classifier. our theoretical and empirical study shows that our framework is superior and more robust to averaging ensemble for noisy data streams.
00b20a67-cfef-4bd9-b1db-b51dd82c5c05 equipment fault diagnosisa neural network approach abstract   this paper describes a project involving the development of a neural network (connectionist) expert system for mechanical equipment fault diagnosis in an integrated steel industry. the objective of this work is to automate the diagnostic process by an artificial intelligence approach. the approach is characterized as a neural network expert system. it is different from traditional rule-based expert systems. a neural network consists of a collection of interconnected nodes. each node has an activation value and each interconnecting arc has a numerical weight. the network can be trained by giving training examples. learning here is the generation of optimal weights. the resulting weight matrix serves as the knowledge base of the system. a hybrid approach has been employed for inferencing, comprising forward chaining and backward chaining. these two processes continue as long as required by the satisfiability criteria. the system has a powerful explanation facility. an example for engine fault diagnosis is presented. problems encountered during the development are discussed.
00b2656e-6126-4c28-b311-244daa49560d a survey on big data mining applications and different challenges a big data is a new term applied to find out the datasets that due to their bulk size and involution. we cannot handle them with our current technique or data mining software tools. a big data composes large volume, difficult to analyze or understand and increasing in size data sets with multiple independent sources. big data mining is the ability of deducing helpful knowledge from these large datasets or streams of data, that due to its large volume, differentiability, and fastness, it was not feasible before to do it. a big data are currently expanding into the every science and engineering fields. this survey paper shows all the characteristics and features of big data, some applications of big data, challenging issues in big data and its related work. in addition we focus on different articles studied and written by mostly talented scientist in the area of big data mining. we hope our review will help to remodel the today's data mining technology for resolving the
00b33d6d-7909-4de2-b454-48814253f242 on quantum statistics in data analysis originally, quantum probability theory was developed to analyze statistical phenomena in quantum systems, where classical probability theory does not apply, because the lattice of measurable sets is not necessarily distributive. on the other hand, it is well known that the lattices of concepts, that arise in data analysis, are in general also non-distributive, albeit for completely different reasons. in his recent book, van rijsbergen argues that many of the logical tools developed for quantum systems are also suitable for applications in information retrieval. i explore the mathematical support for this idea on an abstract vector space model, covering several forms of data analysis (information retrieval, data mining, collaborative filtering, formal concept analysis...), and roughly based on an idea from categorical quantum mechanics. it turns out that quantum (i.e., noncommutative) probability distributions arise already in this rudimentary mathematical framework. we show that a bell-type inequality must be satisfied by the standard similarity measures, if they are used for preference predictions. the fact that already a very general, abstract version of the vector space model yields simple counterexamples for such inequalities seems to be an indicator of a genuine need for quantum statistics in data analysis.
00b39057-34ca-4799-a192-7a83ebcff1c8 an improved method for sizing standalone photovoltaic systems using generalized regression neural network in this research an improved approach for sizing standalone pv system (sapv) is presented. this work is an improved work developed previously by the authors. the previous work is based on the analytical method which faced some concerns regarding the difficulty of finding the model&#x2019;s coefficients. therefore, the proposed approach in this research is based on a combination of an analytical method and a machine learning approach for a generalized artificial neural network (grnn). the grnn assists to predict the optimal size of a pv system using the geographical coordinates of the targeted site instead of using mathematical formulas. employing the grnn facilitates the use of a previously developed method by the authors and avoids some of its drawbacks. the approach has been tested using data from five malaysian sites. according to the results, the proposed method can be efficiently used for sapv sizing whereas the proposed grnn based model predicts the sizing curves of the pv system accurately with a prediction error of 0.6&#x25;. moreover, hourly meteorological and load demand data are used in this research in order to consider the uncertainty of the solar energy and the load demand.
00b3e56d-8f8b-4bd2-ac52-756fe3dd2806 low dimensional data privacy preservation using multi layer artificial neural network government agencies, business enterprises and non-profit organizations are searching for innovative methods to collect and analyze data about individuals or businesses to support their decision making processes. data mining techniques are able to derive sensitive knowledge from unclassified data, causing a severe threat to privacy. the authors provide a promising solution to address the demand for privacy preservation in clustering analysis. they propose a novel dimensionality expansion based data privacy preservation technique using multi-layer artificial neural network. by applying this idea, the authors can project a low dimensional data into a high dimensional space to enhance the privacy level. clustering was done using k-means and the results show that privacy level and the nature of data were very much preserved even after this transformation. the results arrived at were significant and the proposed method transformed the data better than the classical geometric data transformation based methods.
00b3e7d4-4aa2-4155-a38b-e737d42ed52a ten requirements for a theory of change in order to focus the discussion on temporal reasoning in artificial intelligence, we propose a list of requirements that a rigorous theory of time and change must meet. these requirements refer to the desired expressiveness of the formalism (such as representing continuous change) and to potential pitfalls (such as theinter- andintra-frame problems) that should be avoided. we benchmark some of the better known temporal formalisms that have been proposed in artificial intelligence by stating which of the requirements are met, in our opinion, by each formalism.
00b4bbc7-1f6d-46e0-a6ab-5d9c9b73c132 healthcare informatics this resource offers a concise, plain-language review of all the major technologies and applications of informatics in health care today, including essentials such as clinical databases, billing, electronic patient records, lab tests, electronic prescriptions, and much more.
00b5b179-e1cf-4262-a048-2816a964ce74 symbolism and connectionism of artificial intelligence the work of simulating human intelligence is discussed from different aspects: (1) physical symbol system hypotheses proposed by symbolism scientists based on the theory of turing machines; (2) another computational model neural network- proposed by connectionism scientists; and (3) quantum computation. we also try to show that human intelligence is not totally computable and so new scientific explanation for consciousness phenomena is needed.
00b64335-4ce8-4f5d-9bab-bfd4ac319af1 regularized network-based algorithm for predicting gene functions with high-imbalanced data motivations.  #n#the gene function prediction problem is a real-world problem consisting in finding new bio-molecular functions of genes/gene products and characterized by hundreds or thousands of functional classes structured according to a predefined hierarchy.#n#this problem can be formalized as a semi-supervised multi-class, multi-label classification problem where the biological functions of new genes can be predicted by exploiting their connections with genes whose biological functions are known.#n#many different approaches have been proposed to address this problem, including "guilt- by-association" [1], "label propagation" [2], module-assisted techniques [3], svms [4]. nevertheless, these methods usually suffer a decay in performance when input data are highly unbalanced, that is positive examples are significantly less than negatives. this scenario characterizes in particular the most specific classes of the ontology, which are the classes more far from the root classes and that better describe the functions of genes.#n#    #n#     methods.  to address these items, we propose a regularization of a hopfield-based cost- sensitive algorithm, cosnet, recently proposed to predict gene functions [5]. this algorithm, although designed to manage the imbalance in labeled data, tends to predict an excessively high proportion of positives when data are particularly unbalanced (that is in particular on most specific classes). by adding a term to the energy function of the network, we are able in modifying the dynamics in order to prevent the number of positives becomes too large. this energy term is minimized when the proportion of positive neurons (current positive rate) resembles the rate of positive labels in the training set (expected positive rate). the higher the difference between current and expected positive rates, the more the penalty to the energy function. we call this regularized version r-cosnet.#n#     results.  we tested r-cosnet on the prediction of yeast genes, by using four different data sets and the classes of the funcat ontology [6]. this ontology is structured in forest of trees, in which each node belong to one of the six levels of specificity. level 1 refers to the root nodes, level i to nodes at distance i from the root. the considered classes are those with at least 20 positives and are spanned from level 1 to level 5. we compared our methods with a label propagation algorithm, lp-zhu [2], and support vector machine (svm) with probabilistic output [4].#n#in figure 1 we report the results in terms of f-score averaged across the functional classes belonging to the level 4 and level 5 of the hierarchy.#n#     references #n##n#1.    oliver, s. guilt-by-association goes global. nature 2000, 403: 601-603.#n#2.    zhu, x, ghahramani, z, and lafferty, j. semi-supervised learning using gaussian fields and harmonic functions. in icml 2003, 912-919.#n#3.    sharan, r, ulitsky, i, and shamir, r. network-based prediction of protein function. molecular systems biology 2007, 3:88.#n#4.    lin, ht, lin, cj, weng, r. a note on platts probabilistic outputs for support vector machines. machine learning 2007, 68(3): 267-276.#n#5.    bertoni, a, frasca, m, valentini, g. cosnet: a cost sensitive neural network for semi- supervised learning in graphs. ecml/pkdd (1) 2011, lecture notes in computer science, 6911: 219-234.#n#6.    ruepp, a, et al. the funcat, a functional annotation scheme for systematic classification of proteins from whole genomes. nucleic acids research 2004, 32(18): 5539-5545.
00b6514f-935e-47e7-9ff6-b9506f903cea topography and soil properties contribute to regional outbreak risk variability of common voles (microtus arvalis) contextcommon voles (microtus arvalis) are the most common vertebrate pests in central european agriculture. during outbreaks common vole populations can increase to an enormous number of individuals; however, this outbreak risk varies regionally.aimsin this study we tested whether topography and soil properties are suited to explain the regional variability in the outbreak risk of the common vole in eastern germany. this study provides the first detailed large-scale study of the association of site characteristics and small mammal outbreak risk at a regional scale.methodsdata on common vole outbreaks were recorded by active burrow counts at 82 sampling sites in eastern germany from almost four decades. data on topography and soil properties  i.e. groundwater fluctuation index, soil air capacity, saturated hydraulic conductivity, soil class and elevation  were obtained from soil maps and a digital elevation model in a geographic information system and were related to outbreak risk classes, applying classification and regression trees (cart). based on these results a map of the outbreak risk was developed for the area.key resultsclassification and regression tree analyses revealed that the mean elevation, area-related percentage of chernozem soils and soil air capacity were the site characteristics best suited to explain local variability in outbreak risk. in the northern german lowland, below an elevation of ~83 m above sea level, the outbreak risk is generally very low. the region of the central upland has an increased risk for outbreaks of common voles. within the region of the central uplands the risk was again elevated if the area covered by chernozem soils was higher than 36%, and increased further if the area covered by soils with a moderate soil air capacity was higher than 90%.conclusionstopography and soil properties, and accordingly the character of a landscape, are static parameters that affect the local risk of common vole outbreak. further detailed field investigations of soil properties are required to link the variation in regional outbreak risk to site characteristics with relevance to common vole ecology.implicationsareas of varying regional outbreak risk of common voles can be defined according to static site characteristics identified in this study. they can provide a spatial framework to relate dynamic parameters, such as meteorological parameters, as well as biological parameters, such as food availability, to common vole outbreaks. this could be used in the future to develop improved predictive models to forecast common vole outbreaks.
00b684bd-4162-49ae-924a-2dc4a3d81f6b teaching computational modeling in the data science era abstract   integrating data and models is an important and still challenging goal in science. computational modeling has been taught for decades and regularly revised, for example in the 2000s where it became more inclusive of data mining. as we are now in the data science era, we have the occasion (and often the incentive) to teach in an integrative manner computational modeling and data science. in this paper, we reviewed the content of courses and programs on computational modeling and/or data science. from this review and our teaching experience, we formed a set of design principles for an integrative course. we independently implemented these principles in two public research universities, in canada and the us, for a course targeting graduate students and upper-division undergraduates. we discuss and contrast these implementations, and suggest ways in which the teaching of computational science can continue to be revised going forward.
00b6ad27-d10e-4ddf-87bf-42d382d3bf19 fault detection in ip-based process control networks using data mining industrial process control ip networks support communications between process control applications and devices. communication faults in any stage of these control networks can cause delays or even shutdown of the entire manufacturing process. the current process of detecting and diagnosing communication faults is mostly manual, cumbersome, and inefficient. detecting early symptoms of potential problems is very important but automated solutions do not yet exist. our research goal is to automate the process of detecting and diagnosing the communication faults as well as to prevent problems by detecting early symptoms of potential problems. to achieve our goal, we have first investigated real-world fault cases and summarized control network failures. we have also defined network metrics and their alarm conditions to detect early symptoms for communication failures between process control servers and devices. in particular, we leverage data mining techniques to train the system to learn the rules of network faults in control networks and our testing results show that these rules are very effective. in our earlier work, we presented a design of a process control network monitoring and fault diagnosis system. in this paper, we focus on how the fault detection part of this system can be improved using data mining techniques.
00b70469-14ed-4d0b-a664-98996ecc0d74 chase algorithm: "ease of driving" classification in this paper, we present the concept of "ease of driving" or eod rating for road scene environment. we introduce the classification by holistic analysis of scene environment (chase) algorithm to automatically classify the eod rating using computer vision and machine learning techniques. with the chase algorithm we were able to achieve an accuracy of 86.84 % to classify the eod. in the near future, eod information will be used to create better navigation services to improve the driving comfort and mobility by leveraging the existing camera sensors in the car.
00b73942-6cae-458e-b9f7-881795f7f137 ebird: a human / computer learning network to improve biodiversity conservation and research n ebird is a citizen-science project that takes advantage of the human observational capaci ty to identify birds to species, and uses these observations to accurately represent patterns of bird occurrences across broad spatial and tem poral extents. ebird employs artifi cial intelli gence techniques such as machine learning to improve data quality by taking advantage of the synergies between human computation and mechanical computation. we call this a human/computer learning network, whose core is an active learning feedback loop between humans and machines that dramatically improves the quality of both and thereby con tinually improves the effectiveness of the net work as a whole. in this article we explore how human/computer learning networks can lever age the contributions of human observers and process their contributed data with artifi cial intelligence algorithms leading to a computa tional power that far exceeds the sum of the individual parts.
00b7bcc6-7b55-498a-b6a9-00199ee76a4b globalization and islam: challenges and opportunities. globalization is a complex process which has affected all aspects of life. no country, no civilization can remain an island to neutralize the effects of globalization. muslim world too is encountering globalization in its own way. it has challenged the very core values of islamic civilization and muslim scholars are finding it difficult to counter the powerful influence of western anglo sexan globalization. the paper aims to highlight both challenges as well opportunities the globalization has for islam. the paper holds the view that in many ways globalization and islam are allies but in many other ways they contradict. furthermore the paper presents, though very small, sketch of islamic globalization.  tent unseen behind html forms, has shorty been documented as a significant gap in search engine coverage. it represents vital contents of the data on the web; retrieving deep-web content is not an easy task for the database community. indexing of the searched data is major problem tackled by web crawlers that has deeply effect on search engine efficiency. latest study about searching contents on the web illustrate that nearly 96% of data over internet is encapsulated as well as hidden i.e. hidden from search engines. the main task faced by the search engines is to retrieve and access hidden web data (web interfaces) or contents at low cost.  the proposed system uses a machine learning approach that is very scalable, totally automatic, and identically efficient to use, that helps to expand data retrieval functionality at lower cost. the proposed system uses focused crawling strategy for accessing perfect searched results related to query and pick out only relevant information or data according to their similarity with respect to query. the proposed algorithm can selects only possible candidates rather than searching whole document for addition in to your web search index. the automatic attribute building is used for form classification that helps to reduce manual training time and data set building.
00b8f49a-8411-4409-b21e-2829c578525c gaussbox: prototyping movement interaction with interactive visualizations of machine learning we present gaussbox, a design support tool for prototyping movement interaction using machine learning. in particular, we propose novel, interactive visualizations that expose the behavior and internal values of machine learning models rather than their sole results. such visualizations have both pedagogical and creative potentials to guide users in the exploration, experience and craft of machine learning for interaction design.
00ba307f-cf25-4833-8896-9c91356e84b1 prdb: managing and exploiting rich correlations in probabilistic databases due to numerous applications producing noisy data, e.g., sensor data, experimental data, data from uncurated sources, information extraction, etc., there has been a surge of interest in the development of probabilistic databases. most probabilistic database models proposed to date, however, fail to meet the challenges of real-world applications on two counts: (1) they often restrict the kinds of uncertainty that the user can represent; and (2) the query processing algorithms often cannot scale up to the needs of the application. in this work, we define a probabilistic database model, prdb, that uses graphical models, a state-of-the-art probabilistic modeling technique developed within the statistics and machine learning community, to model uncertain data. we show how this results in a rich, complex yet compact probabilistic database model, which can capture the commonly occurring uncertainty models (tuple uncertainty, attribute uncertainty), more complex models (correlated tuples and attributes) and allows compact representation (shared and schema-level correlations). in addition, we show how query evaluation in prdb translates into inference in an appropriately augmented graphical model. this allows us to easily use any of a myriad of exact and approximate inference algorithms developed within the graphical modeling community. while probabilistic inference provides a generic approach to solving queries, we show how the use of shared correlations, together with a novel inference algorithm that we developed based on bisimulation, can speed query processing significantly. we present a comprehensive experimental evaluation of the proposed techniques and show that even with a few shared correlations, significant speedups are possible.
00ba8848-e28f-4d68-a480-8d3a4b13127a svm framework for incorporating content-based image retrieval and data mining into the sbim image manager 
00bab0ab-045e-4454-9f72-c83694cdc01d visual exploration of large relational data sets through 3d projections and footprint splatting this paper discusses 3d visualization and interactive exploration of large relational data sets through the integration of several well-chosen multidimensional data visualization techniques and for the purpose of visual data mining and exploratory data analysis. the basic idea is to combine the techniques of grand tour, direct volume rendering, and data aggregation in databases to deal with both the high dimensionality of data and a large number of relational records. each technique has been enhanced or modified for this application. specifically, positions of data clusters are used to decide the path of a grand tour. this cluster-guided tour makes intercluster-distance-preserving projections in which data clusters are displayed as separate as possible. a tetrahedral mapping method applied to cluster centroids helps in choosing interesting cluster-guided projections. multidimensional footprint splatting is used to directly render large relational data sets. this approach abandons the rendering techniques that enhance 3d realism and focuses on how to efficiently produce real-time explanatory images that give comprehensive insights into global features such as data clusters and holes. examples are given where the techniques are applied to large (more than a million records) relational data sets.
00bb0219-7996-41d8-9fdd-fc086ad29f58 bangla handwritten character recognition using deep belief network recognition of bangla handwritten characters is a difficult but important task for various emerging applications. for better recognition performance, good feature representation of the character images is a primary requirement. in this study, we investigate a recently proposed machine learning approach called deep learning [1] for bangla hand written character recognition, with a focus on automatic learning of good representations. this approach differs from the traditional methods of preprocessing the characters for constructing the handcrafted features such as loops and strokes. among different deep learning structures, we employ the deep belief network (dbn) that takes the raw character images as input and learning proceeds in two steps - an unsupervised feature learning followed by a supervised fine tuning of the network parameters. unlike traditional neural networks, the dbn is a probabilistic generative model, i.e., we can generate samples from the model and it can fit both the semi-supervised and supervised learning settings. we demonstrate the advantages of unsupervised feature learning through the experimental studies carried on the bangla basic characters and numerals dataset collected from the indian statistical institute.
00bc5d6f-c5ee-48b6-b3a9-a7db87680f39 active learning enhanced semi-automatic annotation tool for aspect-based sentiment analysis aspect-based sentiment analysis has become popular research field which allows the quantification of textual evaluations of different aspects of products and services. methods of aspect-based sentiment analysis built on machine learning usually depend on manually annotated training corpora. in order to facilitate the processes of their creation, annotation tools dedicated to this purpose are needed. in this work we proposed a semi-automatic annotation tool which uses active learning to increase the effectiveness of the documents annotation. the use of active learning adapted to the needs of aspect-based sentiment analysis is the main difference between the proposed solution and existing annotation tools. we applied it in the domain of hotels evaluations. the results of realized experiments confirmed the faster increase of the annotation suggestions quality in terms of f1-measure in comparison to the scenario without active learning.
00bc94d6-7dcc-473c-aa5a-82606f2d4d17 extensive--form games with heterogeneous populations the adoption of nash equilibrium (ne) in real--world settings is often impractical due to its too restrictive assumptions. game theory and artificial intelligence provide alternative solution concepts. when knowledge about opponents utilities and types is not available, the appropriate solution concept for extensive--form games is the self--confirming equilibrium (sce), which relaxes ne allowing agents to have incorrect beliefs off the equilibrium path. in this paper, we extend sces to capture situations in which a two--agent extensive--form game is played by heterogeneous populations of individuals that repeatedly match (e.g., ebay).
00bcdbf0-e9f6-485a-8de2-e30dd5a64d7a isolation, molecular cloning and characterization of a cold-responsive gene, amduf1517, from ammopiptanthus mongolicus temperature is one of important factors that influence plant growth and development. using cdna-amplified fragment length polymorphism approach, we previously screened 344 temperature-related transcript-derived fragments (tdfs) from ammopiptanthus mongolicus. in this study, we confirmed that 15 of these tdfs were upregulated in response to low- or high-temperature by using semi-quantitative rt-pcr. based on the rapid amplification of cdna ends, pcr and genome walking approaches, full-length cdna and promoter sequence of amduf1517 was cloned and identified. the 906 bp open reading frame of the amduf1517 gene encoded for a protein of 301 amino acids residues. the corresponding genomic dna sequence contains two exons and one intron. bioinformatic analysis showed that a predicted cleavage site for chloroplast transit peptide, a duf1517 domain, two transmembrane domains and two putative sumoylation sites were conserved between amduf1517 and its homolog from arabidopsis thaliana (atduf1517). we further showed that gfp-tagged amduf1517 was indeed targeted to the chloroplast in arabidopsis protoplast. the transcript levels of amduf1517 were increased specifically in leaves in response to cold stress. in addition, treatment of ethylene, salicylic acid, gibberellic acid or nacl induced the transcription of amduf1517. mutation of duf1517 in arabidopsis exhibited enhanced sensitivity to cold stress, which was coupled with increased electrolyte leakage, malondialdehyde content and decreased contents of soluble sugar, proline. interestingly, heterologous expression of amduf1517 in arabidopsisatduf1517 mutants significantly rescued their cold-sensitive phenotypes. altogether, our data suggest the potential roles of both amduf1517 and atduf1517 in the regulation of cold stress tolerance.
00bd04d5-d945-42b9-aa9b-a90caf36626d detecting evolutionary financial statement fraud a fraudulent financial statement involves the intentional furnishing and/or publishing of false information in it and this has become a severe economic and social problem. we consider data mining (dm) based financial fraud detection techniques (such as regression, decision tree, neural networks and bayesian networks) that help identify fraud. the effectiveness of these dm methods (and their limitations) is examined, especially when new schemes of financial statement fraud adapt to the detection techniques. we then explore a self-adaptive framework (based on a response surface model) with domain knowledge to detect financial statement fraud. we conclude by suggesting that, in an era with evolutionary financial frauds, computer assisted automated fraud detection mechanisms will be more effective and efficient with specialized domain knowledge.
00bd180d-209d-4122-940c-a953efcb01d7 multiclass cancer classification and biomarker discovery using ga-based algorithms motivation: the development of microarray-based high-throughput gene profiling has led to the hope that this technology could provide an efficient and accurate means of diagnosing and classifying tumors, as well as predicting prognoses and effective treatments. however, the large amount of data generated by microarrays requires effective reduction of discriminant gene features into reliable sets of tumor biomarkers for such multiclass tumor discrimination. the availability of reliable sets of biomarkers, especially serum biomarkers, should have a major impact on our understanding and treatment of cancer.#r##n##r##n#results: we have combined genetic algorithm (ga) and all paired (ap) support vector machine (svm) methods for multiclass cancer categorization. predictive features can be automatically determined through iterative ga/svm, leading to very compact sets of non-redundant cancer-relevant genes with the best classification performance reported to date. interestingly, these different classifier sets harbor only modest overlapping gene features but have similar levels of accuracy in leave-one-out cross-validations (loocv). further characterization of these optimal tumor discriminant features, including the use of nearest shrunken centroids (nsc), analysis of annotations and literature text mining, reveals previously unappreciated tumor subclasses and a series of genes that could be used as cancer biomarkers. with this approach, we believe that microarray-based multiclass molecular analysis can be an effective tool for cancer biomarker discovery and subsequent molecular cancer diagnosis.#r##n##r##n#contact: xuefeng_ling@yahoo.com#r##n##r##n#supplementary information: http://www.fishgenome.org/publication/liu/bioinformatics/
00bd23f3-3464-4973-b329-4830c5760ede the sense of ensemble: a machine learning approach to expressive performance modelling in string quartets computational approaches for modelling expressive music performance have produced systems that emulate music expression, but few steps have been taken in the domain of ensemble performance. in this paper, we propose a novel method for building computational models of ensemble expressive performance and show how this method can be applied for deriving new insights about collaboration among musicians. in order to address the problem of inter-dependence among musicians we propose the introduction of inter-voice contextual attributes. we evaluate the method on data extracted from multi-modal recordings of string quartet performances in two different conditions: solo and ensemble. we used machine-learning algorithms to produce computational models for predicting intensity, timing deviations, vibrato extent, and bowing speed of each note. as a result, the introduced inter-voice contextual attributes generally improved the prediction of the expressive parameters. furthermore, results on attribute selection show ...
00bd66e7-f981-45df-9718-bcb0c4579117 machine learning approach for analysis and prediction of cloud particle size and shape distribution techniques for analysis and prediction of cloud particle distribution and solar radiation are provided. in one aspect, a method for analyzing cloud particle characteristics includes the steps of: (a) collecting meteorological data; (b) calculating solar radiation values using a radiative transfer model based on the meteorological data and blended guess functions of a cloud particle distribution (c) optimizing the cloud particle distribution by optimizing the weight coefficients used for the blended guess functions of the cloud particle distribution based on the solar radiation values calculated in step (b) and measured solar radiation values; (d) training a machine-learning process using the meteorological data collected in step (a) and the cloud particle distribution optimized in step (c) as training samples; and (e) predicting future solar radiation values using forecasted meteorological data and the machine-learning process trained in step (d).
00bdc8c6-712f-4cb8-9329-d01bd1e29574 a hybrid modeling approach to microarchitecture design space exploring the micro architectural design space of a new processor is too huge for architects to handle with cycle-accurate simulators. previous researches attack this problem by statistical learning methods such as artificial neural networks (ann) and statistical sampling solutions such as simpoint. these approaches greatly reduce the simulation time while keeping the results of cpi precisely. however, all these machine learning and sampling methods are black boxes: although we can get cpi accurately, we cant get detailed information of on-chip components which makes it difficult to find relationships between these components and bottlenecks of a design. thus these approaches are not sufficient to provide enough intuitions for architects to find potential improvements. this paper proposes a novel white box decision-free generalized stochastic petri nets (decision-free gspn) model. we adopt ann to estimate certain parameters for gspn when we consider new design points. our hybrid approach could predict cpi accurately and produce the usage state of instruction queue (iq), micro operation queue (uopq), reservation station (rs), reorder buffer (rob) and so on precise enough to give architects intuitions into the new design. our solution takes only several minutes to finish comparing to days when we adopt cycle-accurate software simulator, with less than 8.6% error rate for cpi. we believe the information our method produces is precise enough to give architects more intuitions and insights about how to change a design comparing to previous machine learning methods.
00bf0477-ebe1-47dc-8f62-32501f5695e6 artificial intelligence based dynamic transmission network expansion planning the paper is focusing on dynamic transmission network expansion planning (tnep). the tnep problem has been approached from the retrospective and prospective point of view. to achieve this goal, the authors are developing two software-tools in matlab environment. power flow computing is performed using conventional methods. optimal power flow and network expansion are performed using artificial intelligence methods. within this field, two techniques have been tackled: particle swarm optimization (pso) and genetic algorithms (ga). the case study refers to well-known ieee 24 rts test power system.
00bf9289-e852-4b67-8c6e-803694ffb4e5 cluster validity through graph-based boundary analysis gaining confidence that a clustering algorithm has produced meaningful results and not an accident of its usually heuristic optimization is central to data mining. this is the issue of cluster validity. we propose here a method by which proximity graphs are used to effectively detect border points and measure the margin between clusters. with analysis of boundary situation, we design a framework and relevant working principles to evaluate the separation and compactness in the clustering results. the method can obtain an insight into the internal structure in clustering result.
00bfe5f5-b13a-4c07-9c32-4bd495e0ff5e statistical techniques for natural language parsing robotics team 1 from kansas state university was the team that perfectly completed the office navigation event in the shortest time at the fifth annual aaai mobile robot competition and exhibition, held as part of the thirteenth national conference on artificial intelligence. the team, consisting of michael novak and darrel fossett, developed its code in an undergraduate software-engineering course. its c++ code used multiple threads to provide separate autonomous agents to solve the meeting scheduling task, control the sonar sensors, and control the actual robot motion. the team's robot software was nicknamed slick willie for the way it gracefully moved through doorways and around obstacles. the resulting code was robust and performed excellently.
00c06d94-0ff4-46e8-a343-90adcb7c7d85 detection of erythemato-squamous diseases using ar-catfishbpso-ksvm nowadays, one of the most important usages of machine learning is diagnosis of diverse diseases. in this work, we introduces a diagnosis model based on catfish binary particle swarm optimization (catfishbpso), kernelized support vector machines (ksvm) and association rules (ar) as our feature selection method to diagnose erythemato-squamous diseases. the proposed model consisted of two stages. in the first stage, ar is used to select the optimal feature subset from the original feature set. next, based on the fact that kernel parameter setting in the svm training procedure significantly influences the classification accuracy and catfishbpso is a promising tool for global searching, a catfishbpso based approach is employed for parameter determination of ksvm. experimental results show that the proposed ar-catfishbpso-ksvm model achieves 99.09% classification accuracy using 24 features of the erythemato-squamous disease dataset which shows that our proposed method is more accurate compared to other popular methods in this literature like support vector machines and ar-mlp (association rules multilayer perceptron). it should be mentioned that we took our dataset from university of california irvine machine learning database.
00c07aa3-7e74-42ae-8b5a-94cf78f474f1 forecasting reading anxiety for promoting english-language reading performance based on reading annotation behavior. to reduce effectively the reading anxiety of learners while reading english articles, a c4.5 decision tree, a widely used data mining technique, was used to develop a personalized reading anxiety prediction model (prapm) based on individual learners' reading annotation behavior in a collaborative digital reading annotation system (cdras). in addition to forecasting immediately the reading anxiety levels of learners, the proposed prapm can be used to identify the key factors that cause reading anxiety based on the fired prediction rules determined by the developed decision tree. by understanding these key factors that cause reading anxiety, instructors can apply reading strategies to reduce reading anxiety, thus promoting english-language reading performance. to assess whether the proposed prapm can assist instructors in reducing the reading anxiety of learners, this study applies the quasi-experimental method to compare the learning performance of three learning groups, which are, respectively, supported ...
00c137b3-bcef-4d34-b7c3-0f166412d4df analysis of dominant factors in chinese power growth based on fuzzy association rules the development of power system in developing countries can be affected by a great many parameters, hence it is necessary to identify the dominant factors, which are the most influential and typical. in this paper, 27 different variables were selected from statistics across china covering the period 1978-2008. by applying recently developed data mining techniques, interesting relationship between electricity consumption and influencing factors were obtained. finally, four dominant factors as well as five sub-dominant factors were declared based on the confidence value of association rules.
00c194f7-3e37-42e9-aa46-ca9e5cff4438 graph-based p2p traffic classification at the internet backbone monitoring network traffic and classifying applications are essential functions for network administrators. in this paper, we consider the use of traffic dispersion graphs (tdgs) to classify network traffic. given a set of flows, a tdg is a graph with an edge between any two ip addresses that communicate; thus tdgs capture network-wide interactions. using tdgs, we develop an application classification framework dubbed graption (graph-based classification). our framework provides a systematic way to harness the power of network-wide behavior, flow-level characteristics, and data mining techniques. as a proof of concept, we instantiate our framework to detect p2p applications, and show that it can identify p2p traffic with recall and precision greater than 90% in backbone traces, which are particularly challenging for other methods.
00c1b6e7-b4f5-40f8-af2d-abe8d40083a0 contextual privacy: the interplay of sensitivity and context 
00c1c64a-954b-4692-b0eb-39f71844e9cd a hybrid method based on genetic algorithm, self-organised feature map, and support vector machine for better network anomaly detection anomaly-based network intrusion detection techniques are a valuable technology to shield our systems and networks against the malicious activities. anomaly detection is done by soft margin support vector machine(svm), which classify the input into any one of the label (normal and anomalous) category with respect to its anomalous behavior. svm gives much better classification, out of wide variety of class discrimination algorithms which deals with huge collection of data. here genetic algorithm (ga) and self-organised feature map (sofm) are used to enhance the feature and information extraction from a huge dataset similar to kdd99. ga gives us the most prominent features contributing to the anomalous behaviour of a connection and sofm helps to identify similar groups from the dataset by using the similarity metric. these two machine learning algorithms help to reduce the volume of dataset and features to train svm. the proposed framework gss (ga-sofm-svm) has 10% increase in detection rate and 50% reduction in false positive and false negative rate compared to soft margin svm.
00c27f65-9ba1-4bc1-85f1-eec6eb075f2f adaptive approach for spam detection spam has emerged as a major problem in recent years. the most widely recognized form of spam, is email spam. the accounts which contain spam messages must waste time deleting annoying and possibly offensive message. in this paper, we present a variety of machine learning algorithms to identify spam in e-mail accounts. we design classifier model to automatically determine spam in the accounts so that time of account holder can be saved and utilized on other work. the dataset we used for our project is named as spambase dataset download from uci machine learning repository. we used the labeling data in conjunction with machine learning techniques provided by weka tool kit, to train a computer to recognize spam instances automatically. the accuracy of 94.28 is shown by the random committee through the experiment.
00c288f0-e091-4635-9df8-350071b2e9c6 pattern recognition in industry preface acknowledgments about the author part i philosophy chapter 1: introduction chapter 2: patterns within data chapter 3: adapting biological principles for deployment in computational science chapter 4: issues in predictive empirical modeling part ii technology chapter 5: supervised learningcorrelative neural nets chapter 6: unsupervised learning: auto-clustering and self-organizing data chapter 7: customizing for industrial strength applications chapter 8: characterizing and classifying textual material chapter 9: pattern recognition in time series analysis chapter 10: genetic algorithms part iii case studies chapter 11: harnessing the technology for profitability chapter 12: reactor modeling through in situ adaptive learning chapter 13: predicting plant stack emissions to meet environmental limits chapter 14: predicting fouling/coking in fired heaters chapter 15: predicting operational credits chapter 16: pilot plant scale-up by interpreting tracer diagnostics chapter 17: predicting distillation tower temperatures: mining data for capturing distinct operational variability chapter 18: enabling new process design based on laboratory data chapter 19: forecasting price changes of a composite basket of commodities chapter 20: corporate demographic trend analysis epilogue appendices appendix a1: thermodynamics and information theory appendix a2: modeling
00c2b472-cb6b-45bb-b829-6c291cda1e9a a hybrid prediction model with f-score feature selection for type ii diabetes databases the medical data are multidimensional, and are represented by a large number of features. hundreds of independent features (parameters) in these high dimensional databases need to be simultaneously considered and analyzed, for valuable decision-making information in medical prediction. most data mining methods depend on a set of features that define the behavior of the learning algorithm and directly or indirectly influence the complexity of the resulting models. hence, to improve the efficiency and accuracy of mining task on high dimensional data, the data must be preprocessed by an efficient dimensionality reduction method. the aim of this study is to improve the diagnostic accuracy of diabetes disease by selecting informative features of pima indians diabetes dataset. this study proposes a hybrid prediction model with f-score feature selection approach to identify the optimal feature subset of the pima indians diabetes dataset. the features of diabetes dataset are ranked using f-score and the feature subset that gives the minimal clustering error is the optimal feature subset of the dataset. the correctly classified instances determine the pattern for diagnosis and are used for further classification process. the improved performance of the support vector machine classifier measured in terms of accuracy of the classifier, sensitivity, specificity and area under curve (auc) proves that the proposed feature approach indeed improves the performance of classification. the proposed prediction model achieves a predictive accuracy of 98.9427 and it is the highest predictive accuracy for diabetes dataset compared to other models in literature for this problem.
00c2d0e2-6da9-4c6f-baf0-64eaf3f0bf44 intrinsic dimensionality predicts the saliency of natural dynamic scenes since visual attention-based computer vision applications have gained popularity, ever more complex, biologically inspired models seem to be needed to predict salient locations (or interest points) in naturalistic scenes. in this paper, we explore how far one can go in predicting eye movements by using only basic signal processing, such as image representations derived from efficient coding principles, and machine learning. to this end, we gradually increase the complexity of a model from simple single-scale saliency maps computed on grayscale videos to spatiotemporal multiscale and multispectral representations. using a large collection of eye movements on high-resolution videos, supervised learning techniques fine-tune the free parameters whose addition is inevitable with increasing complexity. the proposed model, although very simple, demonstrates significant improvement in predicting salient locations in naturalistic videos over four selected baseline models and two distinct data labeling scenarios.
00c2e864-43a1-4216-bdb1-3fdcae42e131 reflex-type regulation of biped robots 
00c38876-42a2-4034-ad44-dbafc4275797 data clustering using group search optimization with alternative fitness functions data clustering is an important tool for statistical data analysis and exploration, and it has been successfully applied in many fields like image understanding, bioinformatics, big data mining, and so on. from the past few decades, evolutionary algorithms (eas) have been introduced to deal with clustering task, given their global search capabilities and their mechanisms to escape from local minima points. eas execution is driven in an attempt to optimize a criterion function, also known as fitness function. in this work, we evaluate the influence of the fitness function on group search optimization (gso) meta-heuristic when applied to data clustering. three different fitness function are proposed to gso. experiments are performed on twelve benchmark data sets obtained from uci machine learning repository to evaluate the performance of all alternative gso models in comparison to other well-known partitional clustering methods from literature.
00c3cb68-360e-49ae-b727-4259a168d9ed ann and wavelet entropy based approach for fault location in series compensated lines this paper presents a novel approach based on combined wavelet transform and artificial intelligence technique for estimating fault location in a series compensated transmission line. in proposed approach the samples of faulty current signals generated from simulink model are used for fault analysis. wavelet transform is utilised for the purpose of feature extraction from the faulty current signals. the fault current signals are decomposed using db5 mother wavelet. features of faulty signals are extracted in terms of standard deviation and norm entropy value of the coefficients and are fed to designed artificial neural network (ann) models for fault distance estimation. the paper also presents a comparison of error in estimating the distance of fault by different neural network i.e. feed-forward, cascade-forward and generalized regression neural network (grnn). the preciseness and workability of the proposed algorithm has been evaluated on a 400 kv, 300 km series compensated transmission line for different fault cases using matlab simulation. the results acquired, indicate that the proposed approach can reliably located the faults points in series compensated transmission line with high accuracy.
00c408a2-ed35-4b3c-a047-61de8135fab0 learning nongenerative grammatical models for document analysis we present a general approach for the hierarchical segmentation and labeling of document layout structures. this approach models document layout as a grammar and performs a global search for the optimal parse based on a grammatical cost function. our contribution is to utilize machine learning to discriminatively select features and set all parameters in the parsing process. therefore, and unlike many other approaches for layout analysis, ours can easily adapt itself to a variety of document analysis problems. one need only specify the page grammar and provide a set of correctly labeled pages. we apply this technique to two document image analysis tasks: page layout structure extraction and mathematical expression interpretation. experiments demonstrate that the learned grammars can be used to extract the document structure in 57 files from the uwiii document image database. we also show that the same framework can be used to automatically interpret printed mathematical expressions so as to recreate the original latex
00c454cd-d9e0-4b2e-aa62-6833fa99aa5f functional regression-based fluid permeability prediction in monodisperse sphere packings from isotropic two-point correlation functions we study fluid permeability in random sphere packings consisting of impermeable monodisperse hard spheres. several different pseudo-potential models are used to obtain varying degrees of microstructural heterogeneity. systematically varying solid volume fraction and degree of heterogeneity, virtual screening of more than 10,000 material structures is performed, simulating fluid flow using a lattice boltzmann framework and computing the permeability. we develop a well-performing functional regression model for permeability prediction based on using isotropic two-point correlation functions as microstructural descriptors. the performance is good over a large range of solid volume fractions and degrees of heterogeneity, and to our knowledge this is the first attempt at using two-point correlation functions as functional predictors in a nonparametric statistics/machine learning context for permeability prediction.
00c46a3c-01b9-4f02-a7f4-08e627828c2d mop: an efficient algorithm for mining frequent pattern with subtree traversing mining frequent patterns in database has emerged as an important task in knowledge discovery and data mining. in this paper, we present an efficient algorithm called mop for fast frequent pattern discovery. mop utilizes a new kind of data structure called op_tree (ordered pattern tree) and some particular properties of frequent patterns to facilitate the process of mining frequent patterns. an op_tree is a special frequent pattern tree, where the children of any node are sorted according to the supports of corresponding items. efficiency of mop is achieved with three techniques: (1) it adopts op_tree to store a large database to avoid repetitive database scans, (2) it finds all frequent 2-patterns in the construction of op_tree to avoid the costly generation of a large number of candidate 2-patterns, (3) the supports of candidate k-patterns (k>2) can be obtained by traversing a few of specific subtrees of the op_tree, which greatly reduces the search space and avoid multi-scans of a database. we experimentally compare our algorithm with the apriori algorithm and the fp-growth algorithm on one real database and one synthetical database. the experimental results show that mop is about an order of magnitude faster than the apriori algorithm. mop also outperforms the fp-growth algorithm, especially when support threshold is very low and databases are quite large.
00c4baae-5d2c-4df0-971d-40b99c8427fe an efficient algorithm for mining frequent sequence with constraint programming the main advantage of constraint programming (cp) approaches for sequential pattern mining (spm) is their modularity, which includes the ability to add new constraints (regular expressions, length restrictions, etc). the current best cp approach for spm uses a global constraint (module) that computes the projected database and enforces the minimum frequency; it does this with a filtering algorithm similar to the prefixspan method. however, the resulting system is not as scalable as some of the most advanced mining systems like zaki's cspade. we show how, using techniques from both data mining and cp, one can use a generic constraint solver and yet outperform existing specialized systems. this is mainly due to two improvements in the module that computes the projected frequencies: first, computing the projected database can be sped up by pre-computing the positions at which an symbol can become unsupported by a sequence, thereby avoiding to scan the full sequence each time; and second by taking inspiration from the trailing used in cp solvers to devise a backtracking-aware data structure that allows fast incremental storing and restoring of the projected database. detailed experiments show how this approach outperforms existing cp as well as specialized systems for spm, and that the gain in efficiency translates directly into increased efficiency for other settings such as mining with regular expressions.
00c503f6-38c3-43e5-bee8-c906f41e9b24 research on telecom customer churn prediction based on customer value classification in 3g environment telecom operators are facing an urgent problem of telecom customer churn that should be solved as soon as possible. this paper, according to the three-month average customer consumption, divides the levels of customer value, comprehensively uses decision tree algorithm and clustering algorithm modeling of data mining, introduces confusion matrix model for model evaluation, and uses the model output rules set for targeted customers maintaining marketing, so as to reduce customer churn, improve the efficiency of marketing, and enhance the core competitiveness of telecom operators in 3g environment.
00c50d4c-2249-4c7f-bbb1-3111197010b4 empirical evaluation of k-means, bisecting k-means, fuzzy c-means and genetic k-means clustering algorithms clustering is one of the most widely studied problem in machine learning and data mining. the algorithms for clustering depend on the application scenario and data domain. k-means algorithm is one of the most popular clustering techniques that depend on distance measure. in this work, an extensive empirical evaluation of three significant variations of k-means algorithm is carried out on the basis of six internal and external validity indices. it has been seen that performance of k-means and bisecting k-means are similar, while fuzzy c-means gives better performance and genetic k-means performs the best. on the light of empirical result obtained in this paper, method for further improvement of the performance of genetic k-means is suggested.
00c56e94-d9a5-43e6-99e8-4940033b2f39 outlier reduction using hybrid approach in data mining the outlier detection is very active area of research in data mining where outlier is a mismatched data in dataset with respect to the other available data. in existing approaches the outlier detection done only on numeric dataset. for outlier detection if we use clustering method , then they mainly focus on those elements as outliers which are lying outside the clusters but it may possible that some of the unknown elements with any possible reasons became the part of the cluster so we have to concentrate on that also. the proposed method uses hybrid approach to reduce the number of outliers. the number of outlier can only reduce by improving the cluster formulation method. the proposed method uses two data mining techniques for cluster formulation i.e. weighted k-means and neural network where weighted k- means is the clustering technique that can apply on text and date data set as well as numeric data set. weighted k- means assign the weights to each element in dataset. the output of weighted k-means becomes the input for neural network where the neural network is the classification and clustering technique of data mining. training is provided to the neural network and according to that neurons performed the testing. the neural network test the cluster formulated by weighted k-means to ensure that the clusters formulated by weighted k-means are group accordingly. there is lots of outlier detection methods present in data mining. the proposed method use integrating semantic knowledge (sof) for outlier detection. this method detects the semantic outlier where the semantic outlier is a data point that behaves differently with other data points in the same class or cluster. the main motive of this research work is to reduce the number of outliers by improving the cluster formulation methods so that outlier rate reduces and also to decrease the mean square error and improve the accuracy. the simulation result clearly shows that proposed method works pretty well as it significantly reduces the outlier. index termsdata mining, clustering, weighted k- means, neural network, outlier, and sof
00c57ac7-d374-40c8-8239-3cffcfb9749d model-based classification of visual information for content-based retrieval most existing approaches to content-based retrieval rely on query by example, or user sketch based on low-level features. however, these are not suitable for semantic (object level) distinctions. in other approaches, information is classified according to a predefined set of classes and classification is either performed manually or by using class-specific algorithms. most of these systems lack flexibility: the user does not have the ability to define or change the classes, and new classification schemes require implementation of new class-specific algorithms and/or the input of an expert. in this paper, we present a different approach to content-based retrieval and a novel framework for classification of visual information, in which (1) users define their own visual classes and classifiers are learned automatically, and (multiple fuzzy-classifiers and machine learning techniques are combined for automatic classification at multiple levels (region, perceptual, object-part, object and scene). we present the visual apprentice, an implementation of our framework for still images and video that uses a combination of lazy-learning, decision trees, and evolution programs for classification and grouping. our system is flexible, in that models can be changed by users over time, different types of classifiers are combined, and user-model definitions can be applied to object and scene structure classification. special emphasis is placed on the difference between semantic and visual classes, and between classification and detection. examples and results are presented to demonstrate the applicability of our approach to perform visual classification and detection. (1998) copyright spie--the international society for optical engineering. downloading of the abstract is permitted for personal use only.
00c58f74-451b-4619-8190-28647e82f08c rendering global light transport in real-time using machine learning some implementations disclosed herein provide techniques and arrangements to render global light transport in real-time or near real-time. for example, in a pre-computation stage, a first computing device may render points of surfaces (e.g., using multiple light bounces and the like). attributes for each of the points may be determined. a plurality of machine learning algorithms may be trained using particular attributes from the attributes. for example, a first machine learning algorithm may be trained using a first portion of the attributes and a second machine learning algorithm may be trained using a second portion of the attributes. the trained machine learning algorithms may be used by a second computing device to render components (e.g., diffuse and specular components) of indirect shading in real-time.
00c5ed1a-2b1d-4df4-8097-41e67341a0ab the emergence of artificial intelligence technology this issue of the at&t technical journal provides a sample of artificial intelligence projects under way at at&t. these efforts extend to research and development, and cover topics from planning and language understanding to vlsi design and sonar interpretation. the breadth of the artificial intelligence field and several important applications are highlighted.
00c667ea-426d-4c81-8965-fba7d4e33c1d data mining: a tool for enhancing business process in banking sector significant shifts in the business environment, economic volatility, changing customer and staff expectations, and the adopti on of new tech- nology make it increasingly challenging for banks to navigate technology strategy alternatives and prioritize technology investments. the banking indus- try around the world has undergone a tremendous change in the way business is conducted. leading banks are using data mining (dm) tools for cus- tomer segmentation and profitability, credit scoring and approval, predicting payment default, marketing, detecting fraudulent transactions, etc. this pa- per provides an overview of the concept of data mining. data might be one of the most valuable assets of any corporation, but only if it knows how to reveal valuable knowledge hidden in raw data. data mining allows extracting diamonds of knowledge from the historical data, and predicting outcomes of future situations. it helps optimize business decisions, increase the value of each customer and communication, and improve customer satisfaction. data mining is the process of extracting previously un- known information, typically in the form of patterns and associations, from large databases. today, organizations are realizing the numerous advantages that come with data mining. it is a valuable tool, by identifying potentially useful information from the large amounts of data collected. an organization can gain a clear advantage over its competitors. the banking sector consists of public sector, private sector and foreign banks, apart from smaller regional and cooperative b anks. in the market, various it-based banking products, services and solutions are available. the most common of them are phone banking; atm facility; credit, debit and smart cards; internet banking & mobile banking; swift network & infinet network; connectivity of bank branches to facilitate anywhere banking.
00c6be9c-de17-47d4-aaba-bb9bb78bb52a accurate mass filtering of ion chromatograms for metabolite identification using a unit mass resolution liquid chromatography/mass spectrometry system acceleration of liquid chromatography/mass spectrometric (lc/ms) analysis for metabolite identification critically relies on effective data processing since the rate of data acquisition is much faster than the rate of data mining. the rapid and accurate identification of metabolite peaks from complex lc/ms data is a key component to speeding up the process. current approaches routinely use selected ion chromatograms that can suffer severely from matrix effects. this paper describes a new method to automatically extract and filter metabolite-related information from lc/ms data obtained at unit mass resolution in the presence of complex biological matrices. this approach is illustrated by lc/ms analysis of the metabolites of verapamil from a rat microsome incubation spiked with biological matrix (bile). ms data were acquired in profile mode on a unit mass resolution triple-quadrupole instrument, externally calibrated using a unique procedure that corrects for both mass axis and mass spectral peak shape to facilitate metabolite identification with high mass accuracy. through the double-filtering effects of accurate mass and isotope profile, conventional extracted ion chromatograms corresponding to the parent drug (verapamil at m/z 455), demethylated verapamil (m/z 441), and dealkylated verapamil (m/z 291), that contained substantial false-positive peaks, were simplified into chromatograms that are substantially free from matrix interferences. these filtered chromatograms approach what would have been obtained by using a radioactivity detector to detect radio-labeled metabolites of interest.
00c6e12a-ed16-464b-b21e-c5b7dbd9afe3 svm classification:its contents and challenges svm (support vector machines) have become an increasingly popular tool for machine learning tasks involving classification, regression or novelty detection. in particular, they exhibit good generalization performance on many real issues and the approach is properly motivated theoretically. there are relatively a few free parameters to adjust and the architecture of the learning machine does not need to be found by experimentation. in this paper, survey of the key contents on this subject, focusing on the most well-known models based on kernel substitution, namely svm, as well as the activated fields at present and the development tendency, is presented.
00c74afc-2e21-47f7-91de-58da86cc13f4 identification of secretory proteins by separated space based linear discriminant analysis signal peptides are short regions of amino acid residues, which have become a crucial tool in finding new drugs or reprogramming cells for gene therapy. owing to the rapidly increasing number of protein, it is highly demanded to develop the automated algorithm to identify the signal peptides. recently, we had adopted a new alignment kernel function to identify secretory proteins. compared with previous works, our method improves the predictive performance and is much more stably. however, we also find it will be more helpful to visualize the classification. study on feature reduction and extracting the useful features for classification, we make full use of the null space of within-class scatter matrix, and propose separated space based linear discriminant analysis(sslda). for signal peptides, with the high-dimension got by indefinite kernel based on global alignment similarity, we apply sslda and get reduced feature, then sequential data can be visualized, which are highly demanded in machine learning, and avoid the lack of physical explanation as classical neural network method did. the classification results also prove the performance of sslda.
00c76111-21ed-4f58-bc49-08dd7cbf5748 gas holdup estimation in bubble columns using passive acoustic waveforms with neural networks abstract#r##n##r##n#passive acoustic waveforms produced experimentally from a bench-scale two-phase bubble column were recorded using a miniature hydrophone at three axial positions. the generated acoustic waveforms were processed and trained using artificial intelligence against global gas hold-up measurements. two neural network architectures, the radial basis function (rbf) neural network and the recurrent elman neural network, were employed. both neural network techniques achieved accurate gas hold-up estimation, characterised by low mean square errors of 2.70 and 1.68% for the rbf and recurrent elman networks respectively. the designed and trained neural networks were found to be a powerful tool for learning and replicating complex two-phase patterns. passive acoustic waveforms were found to be a useful measuring technique for gas hold-up estimation in bubble columns under moderate operating conditions. copyright  2006 society of chemical industry
00c85250-a153-4234-99d9-19d6b3ec3308 system refinement in practice - using a formal method to modify real-life knowledge the pros and cons of formal methods are the subject of many discussions in artificial intelligence (ai). here, the authors describe a formal method that aims at system refinement based on the results of a test case validation technology for rule-based systems. this technique provides sufficient information to estimate the validity of each single rule. validity in this context is estimated by evaluating the test cases that used the considered rule. the objective is to overcome the particular invalidities that are revealed by the validation process. system refinement has to be set into the context of learning by examples. classical approaches are often not useful for system refinement in practice. they often lead to a knowledge base containing rules that are difficult to interpret by domain experts. the refinement process presented here is characterized by (1) using human expertise that also is a product of the validation technique and (2) keeping as much as possible of the (original) knowledge base. this is a way to avoid the drawbacks of other approaches and to enjoy the benefits of formal methods nevertheless. the validation process provides better solutions for test cases that have a solution which received a bad validity assessment by the validating experts. this knowledge is utilized by a formal reduction system. it reconstructs the rule set in a manner that provides the best rated solution for the entire test case set.
00c90be6-6ea9-404c-9e28-f30e935abc71 data fusion in metabolomics using coupled matrix and tensor factorizations with a goal of identifying biomarkers/patterns related to certain conditions or diseases, metabolomics focuses on the detection of chemical substances in biological samples such as urine and blood using a number of analytical techniques, including nuclear magnetic resonance (nmr) spectroscopy, liquid chromatography-mass spectrometry (lcms), and fluorescence spectroscopy. data sets measured using these methods provide partly complementary information, and their joint analysis has the potential to reveal underlying structures, which are, otherwise, difficult to extract. while we can collect vast amounts of data using different analytical methods, data fusion remains a challenging task, in particular, when the goal is to capture the underlying factors and use them for interpretation, e.g., for biomarker identification. furthermore, many data fusion applications require joint analysis of heterogeneous (i.e., in the form of higher order tensors and matrices) data sets with shared/unshared factors. in order to jointly analyze such heterogeneous data sets, we formulate data fusion as a coupled matrix and tensor factorization (cmtf) problem, which has already proved useful in many data mining applications, and discuss its extension to a structure-revealing data fusion model, i.e., a data fusion model that can identify shared and unshared factors. the traditional methods commonly used for data fusion in the presence of shared/unshared factors are matrix factorization-based methods. using both simulations and prototypical experimental coupled data sets, we assess the performance of various state-of-the-art data fusion methods and demonstrate that while matrix factorization-based approaches have limitations when used for joint analysis of heterogeneous data sets, the structure-revealing cmtf model can successfully capture the underlying factors by exploiting the low-rank structure of higher order data sets.
00c974c7-9666-4c4d-b4f6-43b8e7220484 which doctor to trust: a recommender system for identifying the right doctors background: key opinion leaders (kols) are people who can influence public opinion on a certain subject matter. in the field of medical and health informatics, it is critical to identify kols on various disease conditions. however, there have been very few studies on this topic. objective: we aimed to develop a recommender system for identifying kols for any specific disease with health care data mining. methods: we exploited an unsupervised aggregation approach for integrating various ranking features to identify doctors who have the potential to be kols on a range of diseases. we introduce the design, implementation, and deployment details of the recommender system. this system collects the professional footprints of doctors, such as papers in scientific journals, presentation activities, patient advocacy, and media exposure, and uses them as ranking features to identify kols. results: we collected the information of 2,381,750 doctors in china from 3,657,797 medical journal papers they published, together with their profiles, academic publications, and funding. the empirical results demonstrated that our system outperformed several benchmark systems by a significant margin. moreover, we conducted a case study in a real-world system to verify the applicability of our proposed method. conclusions: our results show that doctors profiles and their academic publications are key data sources for identifying kols in the field of medical and health informatics. moreover, we deployed the recommender system and applied the data service to a recommender system of the china-based internet technology company netease. patients can obtain authority ranking lists of doctors with this system on any given disease.  [j med internet res 2016;18(7):e186]
00cc5d1d-ef32-4ab4-a069-e193045f0b87 using artificial intelligence for cyanohab niche modeling: discovery and visualization of microcystisenvironmental associations within western lake erie cyanobacterial harmful algal blooms (cyanohabs), mainly composed of the genus microcystis, occur frequently throughout the laurentian great lakes. we used artificial neural networks (anns) involving 31 hydrological and meteorological predictors to model total phytoplankton (as chlorophyll a) and microcystis biomass from 2009 to 2011 in western lake erie. continuous anns provided modeled-measured correspondences (and modeling efficiencies) ranging from 0.87 to 0.97 (0.75 to 0.94) and 0.71 to 0.90 (0.45 to 0.88) for training-cross-validation and test data subsets of chlorophyll a concentrations and microcystis biovolumes, respectively. classification anns correctly assigned up to 94% of instances for microcystis presence- absence. the influences of select predictors on phytoplankton and cyanohab niches were visualized using biplots and three- dimensional response surfaces. these then were used to generate mathematical expressions for the relationships between modeled cyanohab outcomes and the direct and interactive influences of environmental factors. based on identified conditions (40 to 50 g total phosphorus (tp)l1, 22 to 26 c, and prolonged wind speeds less than 19 kmh1) underlying the likelihood of occurrence and accumulation of phytoplankton and microcystis, a "target" concentration of 30 g tpl1 appears appropriate for alleviating blooms. anns generated robust ecological niche models for microcystis, providing a predictive framework for quantitative visualization of nonlinear cyanohab-environmental interactions.
00cc6340-11ec-4e8e-b61b-89796189c418 two-sample comparison based on prediction error, with applications to candidate gene association studies. summary#r##n##r##n#to take advantage of the increasingly available high-density snp maps across the genome, various tests that compare multilocus genotypes or estimated haplotypes between cases and controls have been developed for candidate gene association studies. here we view this two-sample testing problem from the perspective of supervised machine learning and propose a new association test. the approach adopts the flexible and easy-to-understand classification tree model as the learning machine, and uses the estimated prediction error of the resulting prediction rule as the test statistic. this procedure not only provides an association test but also generates a prediction rule that can be useful in understanding the mechanisms underlying complex disease. under the set-up of a haplotype-based transmission/disequilibrium test (tdt) type of analysis, we find through simulation studies that the proposed procedure has the correct type i error rates and is robust to population stratification. the power of the proposed procedure is sensitive to the chosen prediction error estimator. among commonly used prediction error estimators, the .632+ estimator results in a test that has the best overall performance. we also find that the test using the .632+ estimator is more powerful than the standard single-point tdt analysis, the pearson's goodness-of-fit test based on estimated haplotype frequencies, and two haplotype-based global tests implemented in the genetic analysis package fbat. to illustrate the application of the proposed method in population-based association studies, we use the procedure to study the association between non-hodgkin lymphoma and the il10 gene.
00cc8673-897c-4fef-8481-e6ba204ecbab data mining and analyzing intelligence 
00cca7fe-cf80-438c-97ab-fde542a4a1c4 autoimmune, atopic, and mental health comorbid conditions associated with alopecia areata in the united states. objective  to evaluate the prevalence of comorbid conditions among patients with alopecia areata (aa) seen at tertiary care hospitals in boston, massachusetts, during an 11-year period.  design  retrospective cross-sectional study.  setting  tertiary care hospitals in boston, including brigham and womens hospital and massachusetts general hospital.  participants  we identified 3568 individuals with aa seen in the partners health care system in boston between january 1, 2000, and january 1, 2011. we performed comprehensive searches of the research patient data repository using international classification of diseases, ninth revision code 704.01. we randomly selected 350 patients and manually reviewed their medical records to train and validate a novel artificial intelligence program. this program then used natural language processing to review free-text medical records and confirm a diagnosis of aa. to confirm the algorithm, we manually reviewed a subset of records and found 93.9% validity.  main outcomes and measures  the prevalence of comorbid conditions was assessed.  results  common comorbid conditions included autoimmune diagnoses (thyroid disease in 14.6%, diabetes mellitus in 11.1%, inflammatory bowel disease in 2.0%, systemic lupus erythematosus in 4.3%, rheumatoid arthritis in 3.9%, and psoriasis and psoriatic arthritis in 6.3%), atopy (allergic rhinitis, asthma, and/or eczema in 38.2% and contact dermatitis and other eczema in 35.9%), and mental health problems (depression or anxiety in 25.5%). we also found high prevalences of hyperlipidemia (24.5%), hypertension (21.9%), and gastroesophageal reflux disease (17.3%). this profile was different from that seen in a comparison psoriasis and psoriatic arthritis group.  conclusions and relevance  we found a high prevalence of comorbid conditions among individuals with aa presenting to academic medical centers in boston. physicians caring for patients with aa should consider screening for comorbid conditions.
00cd2cfd-bf5d-49b2-9ac1-2177ffb6cef9 information recovery and causality: a tribute to george judge in professor george judge's pursuit of information recovery and isolating causality in noisy effects observational data, there is a critical distinction between deductive and inductive empirical analysis. for the former, we bring together a synthesis of the literature that has emerged since koopmans' measurement with theory philosophy. for the latter, we present a host of methodologies that attempt to isolate the causal mechanisms existing in patterns revealed in noisy measurement data. the deductive focus is limited by available theoretical constructs, whereas the inductive focus is fraught with data mining complications, ultimately finding its potential validation in forecasting.
00cda099-175e-4ff1-9c90-9c7a2ae03b79 a fuzzy-based decision making procedure for machine selection problem the selection of appropriate machines is one of the most critical decisions in the design and development of an efficient production environment. it is the fact that improper machine selection can result in quality, flexibility, productivity, etc., problems and negatively affect the overall performance and productivity of a manufacturing system. on the other hand, selecting the best machine among many alternatives is a multi-criteria decision making (mcdm) problem. in this paper, a fuzzy-based mcdm approach is used. for this aim, the fuzzy analytic network process (fanp) is used to determine weights of the criteria and preference ranking organization method for enrichment evaluations (promethee) is used to obtain final ranking of alternative machines. the proposed approach is applied for the selection of a cnc router machine (rm) to be purchased in an international company. in the problem addressed, there are four main criteria, namely cost, quality, flexibility and performance with the corresponding fourteen sub-criteria. the results for the case study indicate the best machine among six potential alternatives and provide different managerial insights for the decision makers.
00ce0319-727f-4a7c-91c2-06b281380367 ese: efficient speech recognition engine with sparse lstm on fpga long short-term memory (lstm) is widely used in speech recognition. in order to achieve higher prediction accuracy, machine learning scientists have built increasingly larger models. such large model is both computation intensive and memory intensive. deploying such bulky model results in high power consumption and leads to a high total cost of ownership (tco) of a data center. to speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the lstm model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy. the pruned model is friendly for parallel processing. next, we propose a scheduler that encodes and partitions the compressed model to multiple pes for parallelism and schedule the complicated lstm data flow. finally, we design the hardware architecture, named efficient speech recognition engine (ese) that works directly on the sparse lstm model.   implemented on xilinx ku060 fpga running at 200mhz, ese has a performance of 282 gops working directly on the sparse lstm network, corresponding to 2.52 tops on the dense one, and processes a full lstm for speech recognition with a power dissipation of 41 watts. evaluated on the lstm for speech recognition benchmark, ese is 43x and 3x faster than core i7 5930k cpu and pascal titan x gpu implementations. it achieves 40x and 11.5x higher energy efficiency compared with the cpu and gpu respectively.
00cefb4b-f31d-4dde-af2b-4392c222be0c genetic network programming with estimation of distribution algorithms for class association rule mining in traffic prediction as an extension of genetic algorithm (ga) and genetic programming (gp), a new approach named genetic network programming (gnp) has been proposed in the evolutionary computation field. gnp uses multiple reusable nodes to construct directed-graph structures to represent its solutions. recently, many research has clarified that gnp can work well in data mining area. in this paper, a novel evolutionary paradigm named gnp with estimation of distribution algorithms (gnp-edas) is proposed and used to solve traffic prediction problems using class association rule mining. in gnp-edas, a probabilistic model is constructed by estimating the probability distribution from the selected elite individuals of the previous generation to replace the conventional genetic operators, such as crossover and mutation. the probabilistic model is capable of enhancing the evolution to achieve the ultimate objective. in this paper, two methods are proposed based on extracting the probabilistic information on the node connections and node transitions of gnp-edas to construct the probabilistic model. a comparative study of the proposed paradigm and the conventional gnp is made to solve the traffic prediction problems using class association rule mining. the simulation results showed that gnp-edas can extract the class association rules more effectively, when the number of the candidate class association rules increases. and the classification accuracy of the proposed method shows good results in traffic prediction systems.
00cf82a3-2033-40ff-83b3-35b8e41047f6 toward modeling and classification of naturalistic driving intelligent systems in automobiles need to be aware of the driving and driver context available sensor data streams have to be modeled and monitored in order to do so. we describe a machine learning approach to accomplish this through collecting a large database of naturalistic driving data in a driving simulator. preliminary experiments with a smaller dataset indicate successful modeling of naturalistic driving with hierarchical sequential models such as hidden markov models.
00cf8514-91fc-4b4f-b7c2-bd95a9180824 a data mining approach to predict protein secondary structure in bioinformatics,proteins are coded by strings, called primary structures. biologists have long enough gathered these primary structures in large databases. numerous experiments and analyses of primary structures have revealed that the protein primary structure closely correlates with the protein second structure. in this paper, we present a data mining approach based on machine learning techniques to predict protein second structure. based on majority voting mechanism, the approach combine the predictions of homology analysis classifier, support vector machine(svm) classifier and modified knowledge discovery in databases (kdd*) process. they are validated with 2 different datasets. their predictive accuracy results outperform the best secondary structure predictors by 2.00% on average.
00cfa1aa-5481-4f92-9b37-184f0bcde234 privacy preserving data mining on published data in healthcare: a survey healthcare data is considered very significant to researchers in this field. such information must be published with methods that keep the identity of patients hidden especially when dealing with sensitive information. publishing such information makes it more vulnerable to attackers. as such, many techniques were proposed to preserve the privacy of healthcare data. in this paper, we illustrated a survey for the models and techniques that are used for publishing data about patients.
00d082ff-50f7-47c9-bdd0-6e6d02b93061 guiding the study of brain dynamics by using first-person data: synchrony patterns correlate with ongoing conscious states during a simple visual task even during well-calibrated cognitive tasks, successive brain responses to repeated identical stimulations are highly variable. the source of this variability is believed to reside mainly in fluctuations of the subject's cognitive context defined by his/her attentive state, spontaneous thought process, strategy to carry out the task, and so on ... as these factors are hard to manipulate precisely, they are usually not controlled, and the variability is discarded by averaging techniques. we combined first-person data and the analysis of neural processes to reduce such noise. we presented the subjects with a three-dimensional illusion and recorded their electrical brain activity and their own report about their cognitive context. trials were clustered according to these first-person data, and separate dynamical analyses were conducted for each cluster. we found that (i) characteristic patterns of endogenous synchrony appeared in frontal electrodes before stimulation. these patterns depended on the degree of preparation and the immediacy of perception as verbally reported. (ii) these patterns were stable for several recordings. (iii) preparatory states modulate both the behavioral performance and the evoked and induced synchronous patterns that follow. (iv) these results indicated that first-person data can be used to detect and interpret neural processes.
00d0a68b-a6d7-4909-8944-bd0d1d7edc75 wikionto: a system for semi-automatic extraction and modeling of ontologies using wikipedia xml corpus this paper introduces wikionto: a system that assists in the extraction and modeling of topic ontologies in a semi-automatic manner using a preprocessed document corpus of one of the largest knowledge bases in the world - the wikipedia. based on the wikipedia xml corpus, we present a three-tiered framework for extracting topic ontologies in quick time and a modeling environment to refine these ontologies. using natural language processing (nlp) and other machine learning (ml) techniques along with a very rich document corpus, this system proposes a solution to a task that is generally considered extremely cumbersome. the initial results of the prototype suggest strong potential of the system to become highly successful in ontology extraction and modeling and also inspire further research on extracting ontologies from other semi-structured document corpora as well.
00d20c14-bf07-4916-bd1f-9167dd8ad46f feature dimensionality reduction for example-based image super-resolution support vector regression has been proposed in a number of image processing tasks including blind image deconvolution, image denoising and single frame super- resolution. as for other machine learning methods, the training is slow. in this paper, we attempt to address this issue by reducing the feature dimensionality through principal component analysis (pca). our single frame supper-resolution experiments show that pca successfully reduces the feature dimensionality with- out degrading the performance of svr when the training images and testing images share similarities (i.e. belong to the same category). in fact, in some cases the per- formance in terms of peak signal-to-noise ratio (psnr), is even better.
00d2135d-79ab-4ac3-88c4-7f8bae3c51d3 fuzzy causal patterns of humor and jokes for cognitive and affective computing humor is an advanced emotional and cognitive ability of mankind that involves complex semantic inference and deep passionate appreciation. this paper presents the cognitive foundations of amusement and a general theory of humor based on the recent advances in cognitive informatics, cognitive linguistics, cognitive computing, and fuzzy causal analyses. a theory of fuzzy false causation ffc is introduced that reveals humor and jokes as false causations in fuzzy causal inferences. base on the ffc theory, a general pattern of humor gph is formalized for analyzing the settings and appreciations of a set of sample jokes. a formal measurement of the degree of amusement in jokes and humor is quantitatively described towards the rational explanation of jokes based on cognitive affective assessment. the formal models of humor and jokes enable machines for humor comprehension and appreciation in artificial intelligence, cognitive computing, computational intelligence, and cognitive robots.
00d34f04-af53-4c66-b731-0ca0f29d57ae molecular biomarkers: their increasing role in the diagnosis, characterization, and therapy guidance in pancreatic cancer the rapidly expanding knowledge of the pathogenesis of pancreatic cancer at the molecular level is providing new targets for disease characterization, early diagnosis, and drug discovery and development. gene mutation analysis has provided insight on the pathogenesis and progression from preinvasive lesions to invasive cancer. gene and protein expression profiling has advanced our understanding of pancreatic ductal adenocarcinoma identifying genes that are highly expressed in pancreatic cancers, providing more insight into the clinicopathologic features of pancreatic cancer, and revealing novel features related to the process of tissue invasion by these tumors. the increasing knowledge of the pathway activation profile in pancreatic cancer is yielding new targets but also new markers to select patients and guide and predict therapy efficacy. the discovery of genetic factors of which the presence predisposes pancreatic cancer to successful targeting, such as the association of brca2/fanconi anemia genes defects and sensitivity to mitomycin c, will eventually lead to a more individualized treatment approach. in summary, several decades of intensive research have originated multiple factors or biomarkers that are likely to be helpful in the diagnosis, characterization, and therapy selection of pancreatic cancer patients. a deep understanding of the relative relevance of each biomarker will be key to efficiently diagnose this disease and direct our patients towards the drugs more likely to be of benefit based on their particular profile. the development of new preclinical models is of paramount importance to achieve these goals. [mol cancer ther 2006;5(4):78796]
00d3718b-56ce-4cc3-8742-eded3328ea11 fast sparse matrix-vector multiplication on gpus: implications for graph mining scaling up the sparse matrix-vector multiplication kernel on modern graphics processing units (gpu) has been at the heart of numerous studies in both academia and industry. in this article we present a novel non-parametric, self-tunable, approach to data representation for computing this kernel, particularly targeting sparse matrices representing power-law graphs. using real web graph data, we show how our representation scheme, coupled with a novel tiling algorithm, can yield significant benefits over the current state of the art gpu efforts on a number of core data mining algorithms such as pagerank, hits and random walk with restart.
00d4cbe4-d11d-41e5-8c0c-906b6d3eeb54 anti-money laundering system architecture under distributed heterogeneous environment this paper presents anti-money laundering system architecture under distributed heterogeneous environment.at the same time,logic hiberarchy,basic frame and main flow about the system are discussed.
00d51144-3086-44cd-9608-13aac0d4e72a a framework of multivariant statistical model based tool using particle swarm optimization with fuzzy data for the classification of yeast data yeast is one of the major components for the formation of different medicines and various chemical products. so, yeast data classification is one of the major bioinformatics task. if the type of yeast can be categorized at primary stages based on initial characteristics of it, a lot of technical procedure can be avoided in the preparation of chemical and medical products. in this paper, an effort has been taken to classify yeast data. the yeast dataset, obtained from uci machine learning laboratories, is used. here, 50 selected data samples have been chosen for case study. at first total effect of each selected samples has been calculated with factor analysis (fa) and principal component analysis (pca). on the basis of average error of fa and pca, total effect obtained from factor analysis has been chosen for applying two soft computing models, they are fuzzy time series model (fts), and particle swarm optimization model respectively. the performance of these two soft computing models is then evaluated using residual analysis.
00d54de9-cd93-4bda-86ad-14149c7cbbb2 large margin distribution learning with cost interval and unlabeled data in many real-world applications, different types of misclassification usually suffer from different costs, but the accurate cost is often hard to be determined and usually one can only get an interval-estimation like that one type of mistake is about 5 to 10 times more serious than the other type. on the other hand, there are usually abundant unlabeled data available, leading to great research effort about semi-supervised learning. it is noticeable that cost interval and unlabeled data usually appear simultaneously in practice tasks; however, there is rare study tackling them together. in this paper, we propose the cisldm approach which is able to handle cost interval and exploit unlabeled data in a principled way. rather than maximizing the minimum margin like traditional large margin classifiers, cisldm tries to optimize the margin distribution on both labeled and unlabeled data when minimizing the worst-case total-cost and the mean total-cost simultaneously according to the cost interval. experiments on a broad range of datasets and cost settings exhibit the impressive performance of cisldm. in particular, cisldm is able to reduce 47 percent more total-cost than standard svm and 27 percent more total-cost than cost-sensitive semi-supervised svm which assumes the true cost value is known in advance.
00d5644e-586d-4377-93b7-73705c9b2330 constraint models for sequential planning planning and constraint satisfaction are important areas of artificial intelligence, but surprisingly constraint satisfaction techniques are not widely applied to planning problems where dedicated solving algorithms dominate. classical ai planning deals with finding a sequence of actions that transfer the initial state of the world into a desired state. one of the main difficulties of planning is that the size of the solution --- the length of the plan --- is unknown in advance. on the other hand, a constraint satisfaction problem needs to be fully specified in advance before the constraint model goes to the solver. as kautz and selman showed, the problem of shortest-plan planning can be translated to a series of sat problems, where each sat instance encodes the problem of finding a plan of a given length. first, we start with finding a plan of length 1 and if it does not exist then we continue with a plan of length 2 etc. until the plan is found. the same approach can be applied when constraint satisfaction is used instead of sat. there exists a straightforward constraint model of this type but more advanced constraint models exploit the structure of a so called planning graph, namely gp-csp and csp-plan. there also exist hand-crafted constraint models for particular planning problems such as cplan. all these models share the logical representation of the problem where boolean variables describe whether a given proposition holds in a given state and whether a given action is applied to a given state. constraints are in the form of logical formulae (frequently implications). in our opinion, these models do not exploit fully the potential of constraint satisfaction, namely domain filtering techniques and global constraints. therefore, we suggested to use multi-valued representation of states based on the state variable formalism sas + that contributes to fewer variables with larger domains where domain filtering pays off. also, we proposed to encapsulate the sets of logical constraints into combinatorial constraints with an extensionally defined set of admissible tuples. this extended abstract summarizes our recent results in the design of constraint models for planning problems presented in [1,2].
00d6a8ee-9a4a-4b7e-a01b-a875f8d62ffa an efficient approach for finding software fault prediction using data mining technique identifying and locating defects in software projects is a difficult work. in particular, when project sizes grow, this task becomes expensive. the aim of this research is to establish a method for identifying software defects using data mining applications methods. in this work we used synthetic data program (sd). this work used mining methods to construct a two step model that predicts potentially defected modules within a given set of software modules with respect to their metric data. the data set used in the experiments is organized in two forms for learning and predicting purposes; the training set and the testing set. the experiments show that the two step model enhances defect prediction performance
00d6e2d5-871c-4704-8f78-02098f87d004 learning with positive and unknown features the study of compound-target binding profiles has been a central theme in cheminformatics. for data repositories that only provide positive binding profiles, a popular assumption is all unreported profiles are negative. in this paper, we caution audience not to take such assumption for granted. under a problem setting where binding profiles are used as features to train predictive models, we present empirical evidence that (1) predictive performance degrades when the assumption fails and (2) specific recovery of unreported profiles improves predictive performance. in particular, we propose a joint framework of profile recovery and supervised learning, which shows further performance improvement. the presented study not only calls for more careful treatment of unreported profiles in cheminformatics, but also initiates a new machine learning problem as we called learning with positive and unknown features.
00d72657-dd96-42c0-a1bb-90d2692723f1 discovering thematic objects in image collections and videos given a collection of images or a short video sequence, we define a thematic object as the key object that frequently appears and is the representative of the visual contents. successful discovery of the thematic object is helpful for object search and tagging, video summarization and understanding, etc. however, this task is challenging because 1) there lacks a priori knowledge of the thematic objects, such as their shapes, scales, locations, and times of re-occurrences, and 2) the thematic object of interest can be under severe variations in appearances due to viewpoint and lighting condition changes, scale variations, etc. instead of using a top-down generative model to discover thematic visual patterns, we propose a novel bottom-up approach to gradually prune uncommon local visual primitives and recover the thematic objects. a multilayer candidate pruning procedure is designed to accelerate the image data mining process. our solution can efficiently locate thematic objects of various sizes and can tolerate large appearance variations of the same thematic object. experiments on challenging image and video data sets and comparisons with existing methods validate the effectiveness of our method.
00d7ec6e-90df-4c9f-b8ac-db572bf9f64f ensembles of case-based reasoning classifiers in high-dimensional biological domains abstract#r##n##r##n#in order to extend the capabilities of case-based reasoning (cbr), we implemented an ensemble for case-based reasoning (e4cbr) approach where an ensemble of cbr classifiers is combined with clustering and feature selection. we first select a subset of features of all the cases, and then cluster the cases into disjoint groups, where each group of cases forms the case-base of one of the member classifiers. finally, in each case-base, a subset of features is locally selected individually. to predict the label of an unseen case, each classifier in the ensemble provides a prediction, and the aggregation component of e4cbr combines the predictions by weighing each classifier using a cbr approacha classifier with more cases similar to the test case receives a higher weight.we evaluated e4cbr on four publicly available biological data sets, and also compared the classification error of e4cbr with a single cbr classifier. in our experiments, we use ta3a computational framework for cbr systems. our results show that e4cbr reduces the classification error of our cbr classifier. on the basis of empirical results, our aggregation method outperforms the existing cbr aggregation methods.  2011 john wiley & sons, inc. wires data mining knowl discov 2011 1 164-171 doi: 10.1002/widm.22
00d80717-eb49-482d-99ef-42ef9ef745f1 multiple model based real time estimation of wheel-rail contact conditions the issue of low adhesion between the wheel and the rail has been a problem for the#r##n#design and operation of the railway vehicles. the level of adhesion can be influenced by#r##n#many different factors, such as contamination, climate, and vegetation, and it is#r##n#extremely difficult to predict with certainty. changes in the adhesion conditions can be#r##n#rapid and short-lived, and values can differ from position to position along a route,#r##n#depending on the type and degree of contamination. all these factors present a#r##n#significant scientific challenge to effectively design a suitable technique to tackle this#r##n#problem. this thesis presents the development of a unique, vehicle based technique for#r##n#the real-time estimation of the contact conditions using multiple models to represent#r##n#variations in the adhesion level and different contact conditions. the proposed solution#r##n#exploits the fact that the dynamic behaviour of a railway vehicle is strongly affected by#r##n#the nonlinearities and the variations in creep characteristics. the purpose of the proposed#r##n#scheme is to interpret these variations in the dynamic response of the wheelset,#r##n#developing useful contact condition information. the proposed system involves the use#r##n#of a number of carefully selected mathematical models (or estimators) of a rail vehicle to#r##n#mimic train dynamic behaviours in response to different track conditions. each of the#r##n#estimators is tuned to match one particular track condition to give the best results at the#r##n#specific design point. increased estimation errors are expected if the contact condition is#r##n#not at or near the chosen operating point. the level of matches/mismatches is reflected in#r##n#the estimation errors (or residuals) of the models concerned when compared with the real#r##n#vehicle (through the measurement output of vehicle mounted inertial sensors). the#r##n#output residuals from all the models are then assessed using an artificial intelligence#r##n#decision-making approach to determine which of the models provides a best match to the#r##n#present operating condition and, thus, provide real-time information about track#r##n#conditions.
00d89922-5d1b-47a1-8c88-7ea71561bc4f study and application on automatic tagging algorithm of image semantic information image annotation is an indispensable step in the process of cbir (content-based image retrieval). it comprehensively considered both features of the image visual and text messages, which can improve the accuracy of the content-based image retrieval and make image search system more accurate when getting target image. based on the study about the mainstream technology of the current image mark methods, using cmrm algorithm as machine learning model, this paper realized an image semantic automatic tagging module by combining training methods of image texture characteristics and image retrieval technologies based on color. the merger of this module and early results of cbir enabled the combination of content-based retrieval and keyword retrieval. it made some improvements to the retrieval performance and narrowed the gap of semantics. experimental results demonstrated that this project can to a certain extent help users more precisely retrieve to their target images more precise.
00d8e065-d19e-4414-8b60-6bfb94ceeb4b a survey of data mining techniques on medical data for finding locally frequent diseases in the last decade there has been increasing usage of data mining techniques on medical data for discovering useful trends or patterns that are used in diagnosis and decision making. data mining techniques such as clustering, classification, regression, association rule mining, cart (classification and regression tree) are widely used in healthcare domain. data mining algorithms, when appropriately used, are capable of improving the quality of prediction, diagnosis and disease classification. the main focus of this paper is to analyze data mining techniques required for medical data mining especially to discover locally frequent diseases such as heart ailments, lung cancer, breast cancer and so on. we evaluate the data mining techniques for finding locally frequent patterns in terms of cost, performance, speed and accuracy. we also compare data mining techniques with conventional methods.
00d9052b-3e79-4285-88cc-ae8df96e8c8e biologyoriented synthesis 
00d9651b-3da7-465e-9143-dd41c5f4f17d iteratively refining svms using priors research on scalable machine learning algorithms has gained a considerable amount of traction since the exponential growth in data assets during the past decades. many big data applications resort to somewhat "simple" data modelling techniques due to the computational constraints associated with more complex models. simple models, while being very efficient to estimate, often fail to capture some of the finer details of more complex datasets. in this manuscript, we explore the idea that complex large scale classification can be tractable using a process of iterative refining. in such a process, we focus on non-linearities of the data only after having first found an approximate linear model. this knowledge is then incorporated into the nonlinear model implicitly, allowing the non-linear model to focus on important parts of the data after a rough first estimation. this in turn reduces overall training time and allows for a richer model representation, eventually leading to more predictive power.
00d97b75-f8c5-4bcd-9154-a8f2cbdccde4 letters: hyperplanes for predicting protein-protein interactions prediction of protein-protein interaction is a difficult and important problem in biology. given (numerical) features, one of the existing machine learning techniques can be then applied to learn and classify proteins represented by these features. our computational results demonstrate that a system based on k-local hyperplane outperforms the methods proposed in the literature based on global representation of a protein pair. the approach is demonstrated by building a learning system based on experimentally validated protein-protein interactions in the human gastric bacterium helicobacter pylori dataset and in human dataset.
00d9884a-7ef5-4302-b94a-a08907d5f68d resampling-based multiple testing: examples and methods for p-value adjustment resampling-based adjustments: basic concepts. continuous data applications: univariate analysis. continuous data applications: multivariate analysis. binary data applications. further topics. practical applications. appendices. references. list of algorithms. list of examples. indexes.
00d98fe0-43e5-45b7-bbf3-3fd4a15fb3ae language independent big-data system for the prediction of user location on twitter social media interactions have become increasingly important in today's world. a survey conducted in 2014 among adult americans found that a majority of those surveyed use at least one social media site. twitter, in particular, serves 310 million active users on a monthly basis, and thousands of tweets are published every second. the public nature of this data makes it a prime candidate for data mining. twitter users publish 140-character long messages and have the ability to geo-tag these tweets using a variety of methods: gps coordinates, ip geolocation and user-declared location. however, few users disclose their location, only between 1% and 3% of users provide location data, according to our empirical findings. in this article, we aim to aggregate information from different sources to provide an estimation on the location of any twitter user. we use an hybrid approach, using techniques in the fields of natural language processing and network theory. tests have been conducted on two datasets, inferring the location of each individual user and then comparing it against the actual known location of users with geolocation information. the estimation error is the distance in kilometers between the estimation and the actual location. furthermore, there is a comparison of the relative average error per country, to account for difference in country sizes. our results improve those presented in different researches in the literature. our research has as feature to be independent of the language used by the user, while most of works in the literature use just one language or a reduced set of languages. the article also showcases the evolution of our estimation approach and the impact that the modifications had on the results.
00d9df98-2b51-4f5d-b888-9ada1747d4b3 knowledge-based data mining we describe techniques for combining two types of knowledge systems: expert and machine learning. both the expert system and the learning system represent information by logical decision rules or trees. unlike the classical views of knowledge-base evaluation or refinement, our view accepts the contents of the knowledge base as completely correct. the knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules. an expert system called seas was built to discover sales leads for computer products and solutions. the system interviews executives by asking questions, and based on the responses, recommends products that may improve a business' operations. leveraging this expert system, we record the results of the interviews and the program's recommendations. the very same data stored by the expert system is used to find new predictive rules. among the potential advantages of this approach are (a) the capability to spot new sales trends and (b) the substitution of less expensive probabilistic rules that use database data instead of interviews.
00da2158-c473-4fa8-93b9-e62ef05ebc04 active learning for automatic classification of software behavior a program's behavior is ultimately the collection of all its executions. this collection is diverse, unpredictable, and generally unbounded. thus it is especially suited to statistical analysis and machine learning techniques. the primary focus of this paper is on the automatic classification of program behavior using execution data. prior work on classifiers for software engineering adopts a classical  batch-learning  approach. in contrast, we explore an  active-learning  paradigm for behavior classification. in active learning, the classifier is trained incrementally on a series of labeled data elements. secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the markov property, and that the resultant markov models of individual program executions can be automatically clustered into effective predictors of program behavior. we present a technique that models program executions as markov models, and a clustering method for markov models that aggregates multiple program executions into effective behavior classifiers. we evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.
00da9d2b-209b-4eab-aefa-9135cacac3ca big data and quality: a literature review big data refers to data volumes in the range of exabyte (10 18 ) and beyond. such volumes exceed the capacity of current on-line storage and processing systems. with characteristics like volume, velocity and variety big data throws challenges to the traditional it establishments. computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of big data. there are many issues with big data that warrant quality assessment methods. the issues are pertaining to storage and transport, management, and processing. this paper throws light into the present state of quality issues related to big data. it provides valuable insights that can be used to leverage big data science activities.
00dab369-cf30-428b-b3f8-4ab55cabb944 anomaly detection in online social networks : using data-mining techniques and fuzzy logic this research is a step forward in improving the accuracy of detecting anomaly in a data graph representing connectivity between people in an online social network. the proposed hybrid methods are based on fuzzy machine learning techniques utilising different types of structural input features. the methods are presented within a multi-layered framework which provides the full requirements needed for finding anomalies in data graphs generated from online social networks, including data modelling and analysis, labelling, and evaluation.
00dadf28-d862-4e9a-b530-87dc5d3bedb8 dialogue between user and computer involving artificial intelligence 
00db3945-c1d9-479f-9bb0-6a1e2557a651 a polynomial-time maximum common subgraph algorithm for outerplanar graphs and its application to chemoinformatics metrics for structured data have received an increasing interest in the machine learning community. graphs provide a natural representation for structured data, but a lot of operations on graphs are computationally intractable. in this article, we present a polynomial-time algorithm that computes a maximum common subgraph of two outerplanar graphs. the algorithm makes use of the block-and-bridge preserving subgraph isomorphism, which has significant efficiency benefits and is also motivated from a chemical perspective. we focus on the application of learning structure-activity relationships, where the task is to predict the chemical activity of molecules. we show how the algorithm can be used to construct a metric for structured data and we evaluate this metric and more generally also the block-and-bridge preserving matching operator on 60 molecular datasets, obtaining state-of-the-art results in terms of predictive performance and efficiency.
00db4a4e-420c-4503-a4f7-1a90d511ea4a bayesian theory and artificial intelligence: the quarrelsome marriage abstract   the problem of knowledge-base updating is addressed from an abstract point of view in the attempt to identify some general desiderata the updating mechanism should satisfy. they are recognized to be basically two: evaluating the local impact of new data on the single items of knowledge already stored, and propagating this effect through the knowledge-base maintaining at the same time its global coherence. it will be shown that bayesian updating, difficult to implement, satisfies simultaneously these two requirements, and that, on the other hand, dempstershafer updating, easy to implement, does not satisfy the requirement of global coherent propagation. i will point out the existence of a trade-off between coherence and effectiveness in the methods for representing uncertainty currently proposed in ai. two kinds of learning machines, boltzmann machines and harmonium, will be discussed and considered as first attempts to give a non-behavioral characterization of coherence in a cognitive agent, a characterization still consistent with the behavioral (probabilistic) definition.
00dc40e6-9480-4917-9c46-8ab0acac93df metaphor as a mechanism for reorganizing the type hierarchy abstract   metaphor is not just a surface phenomenon, a particular form of speech, but an indication of some underlying cognitive mechanisms. metaphor provides a way of violating the usual semantic constraints and categories, and joining together ideas that were previously seen as dissimilar, and, in so doing, it extends one's conceptual framework. the paper explores how the mechanisms of metaphor can be used to develop a more flexible means of knowledge representation for artificial intelligence. the theory of metaphor used in the paper is based on max black's interaction theory, and has been developed by way. the knowledge-representation scheme is that of john sowa's conceptual graphs. the techniques for representing knowledge in machines have provided a new language and a new set of conceptual tools which can make sense of black's often vague and intuitive theory.
00dcccea-87ae-4578-a2df-6454ae9deabd the role of icts in downscaling and up-scaling integrated weather forecasts for farmers in sub-saharan africa despite global advancements in technology and inter-trade volumes, sub-saharan africa is the only region where cases of hunger have increased since 1990. rampant and frequent droughts are one of the major causes of this. monumental and mostly donor-funded projects have been mounted to counter this but with little success. one of the latest strategies being experimented is a community-based early warning system that seeks to integrate indigenous knowledge with western climate science. this initiative is informed by the realization that, though crucial, weather forecast information provided by the national meteorological departments has little utilization amongst small-scale farmers. though having generated promising results, the integration project still faces the challenges of scaling up across communities as well as the lack of micro-level weather data. in this paper, we describe how the adoption of mobile phones and wireless sensor networks technology is being used to address these two challenges. use of denser wireless sensor networks to collect local weather data and mobile phones to disseminate forecasts brings information closer to the farmers that need it most. to ensure that the non-mystical aspects of indigenous knowledge are portable across communities, language technologies (part of artificial intelligence) are used in the design of our system.
00dcde25-3c0b-481d-b286-0399d4a2ca10 a data mining framework for automatic online customer lead generation 
00dcf8bc-25fd-4c67-bdd8-4ee503c9fa6f mama: manifest analysis for malware detection in android the use of mobile phones has increased because they offer nearly the same functionality as a personal computer. in addition, the number of applications available for android-based mobile devices has increased. google offers programmers the opportunity to upload and sell applications in the android market, but malware writers upload their malicious code there. in light of this background, we present here manifest analysis for malware detection in android mama, a new method that extracts several features from the android manifest of the applications to build machine learning classifiers and detect malware.
00dd1325-c1b3-4ee7-84e2-cdf3a4959517 a conspectus on operator modeling: past, present and future this paper presents an overview of the state of the art in human/operator modeling. the thrust of such a conspectus is towards describing the ongo ing research as well as some future possible applications resulting from the research, which are currently under investigation at the manmachine system laboratory at suny, binghamton. the paper contains a brief description of conventional human operator models, such as: the crossover model, the hess structural model, and the optimal control model (ocm). current trends in operator modeling are presented in this paper by work done by zeyada and hess at ucla davis and hosman at delft. the major element of the zeyada and hess research is that they extensively utilize intelligent computing tools such as fuzzy inference systems and genetic algorithms, along with conventional tools for operator modeling. the emphasis, however, of this section of the paper is on a description of the work done by gary george and frank cardullo. their research is primarily based on major modifications to the existing hess model by adding the vestibular and somatosensory stimulation channels into the feedback of the model, for the purpose of evaluating the contributi on of simulator cueing devices to operator behavior. the vestibular stimulation channel contai ns models of the vestibular system, a motion platform and motion washouts. the somatosensory cueing channel contains a model of the pacinian corpuscles a dynamic seat and its drive algorithm. the state-of-the-art research section of the conspectus represents the work done by zaychik and cardullo. the main thrust of this work is the development of a methodology of automated tuning of the parameters of the modified hess model. the proposed algorithm heavily involves machine learning tools sometimes known as soft computing techniques. the automatic tuning of the model allow s for the real-time identification of a model of the operator of a vehicle. the paper contains the d escription of potential application in the area of flight safety as well as simulator/handling qualiti es evaluation.
00dd6121-1640-444f-b1ae-e6167a76d069 the clustering of high schools based on national and school examinations: a case study at daerah istimewa yogyakarta province the purpose of indonesian national examination for high school students is to measure and assess students' knowledge and competence in particular subjects. the result is also going to be used as one of consideration for mapping indonesia's national education quality. aside from national examination (ne), each school also conducts school examination (se). both examinations are supposed to represent quality of education since the examinations measure the competence of the same students. however, the results of both examinations are not always linear [1]. in fact, the need of ne in indonesian education is still being pro and cons among society. in order to identify whether ne and se could be used to represent the quality of educations in daerah istimewa yogyakarta province, this paper describes the analysis of ne and se score by performing data mining technique using fuzzy c-means clustering algorithm towards ne score and se score independently. furthermore, the clusters were analyzed using univariate anova, spearman correlation, and crosstabulation. the data used in this research are ne and se scores of natural science department and social science department of all high schools in daerah istimewa yogyakarta province from academic year 2011/2012 to 2014/2015. the results of cluster analysis are three different clusters of ne in natural science department, three different clusters of ne in social science department, three different clusters of se in natural science department, and three different clusters of se in social science department for each year. the clusters are significantly separated. there is an opposite direction relationship between clusters of ne and se. the relationship is weak which means there is no guarantee that a school which belongs to cluster-i of ne will be in the cluster-i of se. both for ne and se memberships, only few members migrated from one cluster to another across years. the number of schools having the same cluster of ne and se in each department varies from year to year, but generally less than 22%. the migrations of ne and se cluster members from higher cluster to lower one and vice versa also vary. in addition, there is inconsistency clustering based on ne and se. since se is not standardized and indeed is a formative test, there might be subjective aspects involved in grading the students. therefore, if the government intends to map indonesia's national education quality, national examination is more suitable than school examination for this purpose.
00dd7528-5cdb-4453-8a73-50d1bab15654 a cooperative multi-agent data mining model and its application to medical data on diabetes we present cole, a model for cooperative agents for mining knowledge from heterogeneous data. cole allows for the cooperation of different mining agents and the combination of the mined knowledge into knowledge structures that no individual mining agent can produce alone. cole organizes the work in rounds so that knowledge discovered by one mining agent can help others in the next round. we implemented a multi-agent system based on cole for mining diabetes data, including an agent using a genetic algorithm for mining event sequences, an agent with improvements to the part algorithm for our problem and a combination agent with methods to produce hybrid rules containing conjunctive and sequence conditions. in our experiments, the cole-based system outperformed the individual mining algorithms, with better rules and more rules of a certain quality. from the medical perspective, our system confirmed hypertension has a tight relation to diabetes, and it also suggested connections new to medical doctors.
00dd95de-2db5-414c-b0d3-91786efafe87 integration of expert system with analytic hierarchy process for the design of material handling equipment selection system a key task in the material handling system design process is the selection and configuration of equipment for material transport and storage in a facility. material handling equipment selection is a complex, tedious task, and there are few tools other than checklists to assist engineers in the selection of appropriate, cost-effective material handling equipment. this paper describes the development of an intelligent material handling equipment selection system called material handling equipment selection advisor (mhesa). the mhesa is composed of three modules: (1) a database to store equipment types with their specifications; (2) a knowledge-based expert system for assisting material handling equipment selection; and (3) an analytic hierarchy process (ahp) model to choose the most favorable equipment type. the concept proposed in this paper can automate the design of a material handling equipment selection system, and provides artificial intelligence in the decision-making process.
00dda6a7-beb6-410a-8b89-c266338c6370 a novel semi-supervised svm based on tri-training one of the main difficulties in machine learning is how to solve large-scale problems effectively, and the labeled data are limited and fairly expensive to obtain. in this paper a new semi-supervised svm algorithm is proposed. it applies tri-training to improve svm. the semi-supervised svm makes use of the large number of unlabeled data to modify the classifiers iteratively. although tri-training doesn't put any constraints on the classifier, the proposed method uses three different svms as the classification algorithm. experiments on uci datasets show that tri-training can improve the classification accuracy of svm and can increase the difference of classifiers, the accuracy of final classifier will be higher. theoretical analysis and experiments show that the proposed method has excellent accuracy and classification speed.
00ddb92a-1fce-496a-a894-ae9877a6d909 a language for formal design of embedded intelligence research systems the construction of complex artifacts of artificial intelligence requires large-scale system integration and collaboration. system architectures are a central issue to enable this process. to develop these, hypotheses must be formulated, validated and evolved. we therefore present systematica 2d, a formalism suitable for both flexible description of hierarchical architecture concepts as well as functional design of the resulting system integration process. we motivate the approach and relate it to other formal descriptions by means of a new formalization measure. it consists of a set of criteria to evaluate how well a formalism supports the expression, construction and reuse of intelligent systems. systematica 2d is compared with existing formalization languages under this measure and shown to have at least their level of expression. in addition, the system properties of incremental composition, partial testability and global deadlock-free operation are formally defined and proven in the formalism.
00de08e7-767d-4392-b0d0-9968e563b73d recurrent memory networks for language modeling recurrent neural networks (rnn) have obtained excellent result in many natural language processing (nlp) tasks. however, understanding and interpreting the source of this success remains a challenge. in this paper, we propose recurrent memory network (rmn), a novel rnn architecture, that not only amplifies the power of rnn but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. we demonstrate the power of rmn on language modeling and sentence completion tasks. on language modeling, rmn outperforms long short-term memory (lstm) network on three large german, italian, and english dataset. additionally we perform in-depth analysis of various linguistic dimensions that rmn captures. on sentence completion challenge, for which it is essential to capture sentence coherence, our rmn obtains 69.2% accuracy, surpassing the previous state-of-the-art by a large margin.
00de1415-61a4-4fcc-bca1-5c0225cf4bda towards the computation of a nash equilibrium game theory has played a progressively more noticeable and important role in computer science topics, such as artificial intelligence, computer networking, and distributed computing, in recent years. in this paper, we provide a preliminary review of where efforts on this topic have been focused over the past several decades and find that currently, the most remarkable interface between algorithmic game theory and theo- retical computer science is the computational complexity of computing a nash equilibrium.
00e009a1-8cde-41a9-a6af-72200776a856 a hybrid model to detect win32 worm executables internet worm is a kind of mal ware which pose a serious threat to computer security. it is designed to damage computer system and spreads over network without the knowledge of the owner using the system. this paper presents a static worm detection model with hybrid feature extraction that uses several data mining technologies, including information gain (ig), principal component analysis, and three classifiers: smo, random forest (rf), and naive bayes (nb). by using the conepts of machine learning and data-mining, we construct a static hybrid worm detection model (hwdm) which has a detection rate of 97.5%.
00e0952a-3b89-40c5-86ba-e91db553aaea speed control of neural network based energy efficient dc drive dc motors have a very fine speed control characteristics and so are widely used in various industrial applications. their performance can be optimised, if along with fine speed control a motor can be made energy efficient. in this paper an attempt is made to achieve the speed control at the condition of maximum efficiency for a separately excited dc motor using both field control and armature control methods. in recent trends, it is seen that artificial intelligence based controllers have superior performance compared to the power electronic based controllers pid controllers. the controller chosen for the desired task is an artificial neural network, as any non-linear data can be fit in it. the network is trained and a model is generated in matlab. the neural network is then connected to a model of a separately excited dc motor and a few simulated results were obtained to verify the task. the whole simulation work is performed in simulink and the output curves were obtained for various types of load torques and reference speeds.
00e097cb-502f-433d-963e-8a14e431da66 a machine-learning based load prediction approach for distributed service-oriented applications by using middleware, we can satisfy the urgent demands of performance, scalability and availability in current distributed service-oriented applications. however to the complex applications, the load peak may make the system suffer extremely high load and the response time may be decreased for this kind of fluctuate. therefore, to utilize the services effectively especially when the workloads fluctuate frequently, we should make the system react to the load fluctuate gradually and predictably. many existing load balancing middleware use the  dampening technology to make the load to be predicative. however, distributed systems are inherently difficult to manage and the dampening factor cannot be treated as static and fixed. the dampening factor should be adjusted dynamically according to different load fluctuate. so we have proposed a new technique based on machine learning for adaptive and flexible load prediction mechanism based on our load balancing middleware.
00e0aa49-520c-4fd8-bf64-09339e75698e singular spectrum analysis of sleep eeg in insomnia in the present study, the singular spectrum analysis (ssa) is applied to sleep eeg segments collected from healthy volunteers and patients diagnosed by either psycho physiological insomnia or paradoxical insomnia. then, the resulting singular spectra computed for both c3 and c4 recordings are assigned as the features to the artificial neural network (ann) architectures for eeg classification in diagnose. in tests, singular spectrum of particular sleep stages such as awake, rem, stage1 and stage2, are considered. three clinical groups are successfully classified by using one hidden layer ann architecture with respect to their singular spectra. the results show that the ssa can be applied to sleep eeg series to support the clinical findings in insomnia if ten trials are available for the specific sleep stages. in conclusion, the ssa can detect the oscillatory variations on sleep eeg. therefore, different sleep stages meet different singular spectra. in addition, different healthy conditions generate different singular spectra for each sleep stage. in summary, the ssa can be proposed for eeg discrimination to support the clinical findings for psycho-psychological disorders.
00e0d1e6-9d69-40c1-8c7c-fbcb0e04708a vibration condition monitoring: latest trend and review vibration analysis has proven to be the most effective method for machine condition monitoring to date. various effective signal analysis methods to analyze and extract fault signature that embedded in the raw vibration signals have been introduced in the past few decades such as fast fourier transform (fft), short time fourier transform (stft), wavelets analysis, empirical mode decomposition (emd), hilbert-huang transform (hht), etc. however, these is still a need for human to interpret vibration signature of faults and it is regarded as one of the major challenge in vibration condition monitoring. thus, most recent researches in vibration condition monitoring revolved around using artificial intelligence (ai) techniques to automate machinery faults detection and diagnosis. the most recent literatures in this area show that researches are mainly focus on using machine learning techniques for data fusion, features fusion, and also decisions fusion in order to achieve a higher accuracy of decision making in vibration condition monitoring. this paper provides a review on the most recent development in vibration signal analysis methods as well as the ai techniques used for automated decision making in vibration condition monitoring in the past two years.
00e0fbed-f9d7-4b51-b8b8-1a9f460b0f58 integration proposal for description logic and attributive logic: towards semantic web rules the current challenge of the semantic web is the development of an expressive yet effective rule language. this paper presents an integration proposal for description logics (dl) and attributive logics (alsv) is presented. these two formalisms stem from fields of knowledge representation and artificial intelligence. however, they are based on different design goals and therefore provide different description and reasoning capabilities. alsv is the foundation of xtt2, an expressive language for rule-based systems. dl provide formulation for expressive ontology languages such as owl2. an important research direction is the development of rule languages that can be integrated with ontologies. the contribution of the paper consists in introducing a possible transition from alsv to dl. this opens up possibilities of using xtt2, a well-founded rule-based system modelling rule language, to improve the design of semantic web rules.
00e0fe22-5b4c-49b0-a0ce-fbfc3de590f1 reasoning with cases in clinical problem solving this paper first describes the effort that has been done so far by researchers in artificial intelligence in the use of case-based reasoning (cbr) for the realisation of computer-based systems in clinical problem solving. then, the approach we have chosen for building a case-based system in a specific medical domain (liver diseases) is presented, stressing the advantages of the use of cbr in medicine. cbr is a recent approach to problem solving and learning; it means using old experiences to understand and solve new problems. a new problem is solved by finding a similar past case and reusing it in the new problem situation. the aim of this work is the development of a cbr module running on an open network architecture.
00e14ad6-697a-4030-8d87-20982f00b05b a planning quality evaluation tool for prostate adaptive imrt based on machine learning purpose: to ensure plan quality for adaptive imrt of the prostate, we developed a quantitative evaluation tool using a machine learning approach. this tool generates dose volume histograms (dvhs) of organs-at-risk (oars) based on prior plans as a reference, to be compared with the adaptive plan derived from fluence map deformation. methods: under the same configuration using seven-field 15 mv photon beams, dvhs of oars (bladder and rectum) were estimated based on anatomical information of the patient and a model learned from a database of high quality prior plans. in this study, the anatomical information was characterized by the organ volumes and distance-to-target histogram (dth). the database consists of 198 high quality prostate plans and was validated with 14 cases outside the training pool. principal component analysis (pca) was applied to dvhs and dths to quantify their salient features. then, support vector regression (svr) was implemented to establish the correlation between the features of the dvh and the anatomical information. results: dvh/dth curves could be characterized sufficiently just using only two or three truncated principal components, thus, patient anatomical information was quantified with reduced numbers of variables. the evaluation of the model using the test data set demonstrated its accuracy         80   %      in prediction and effectiveness in improving art planning quality. conclusions: an adaptive imrt plan quality evaluation tool based on machine learning has been developed, which estimates oar sparing and provides reference in evaluating art.
00e17d8f-7f10-402b-aab6-10524f0433ee iql: a proposal for an inductive query language the overall goal of this paper is to devise a flexible and declarative query language for specifying or describing particular knowledge discovery scenarios. we introduce one such language, called iql. iql is intended as a general, descriptive, declarative, extendable and implementable language for inductive querying that supports the mining of both local and global patterns, reasoning about inductive queries and query processing using logic, as well as the flexible incorporation of new primitives and solvers. iql is an extension of the tuple relational calculus that includes functions as primitives. the language integrates ideas from several other declarative programming languages, such as pattern matching and function typing. we hope that it will be useful as an overall specification language for integrating data mining systems and principles.
00e1bd73-4f4b-45b8-8746-757cdc29dd89 artificial intelligence in cyber defense the speed of processes and the amount of data to be used in defending the cyber space cannot be handled by humans without considerable automation. however, it is difficult to develop software with conventional fixed algorithms (hard-wired logic on decision making level) for effectively defending against the dynamically evolving attacks in networks. this situation can be handled by applying methods of artificial intelligence that provide flexibility and learning capability to software. this paper presents a brief survey of artificial intelligence applications in cyber defense (cd), and analyzes the prospects of enhancing the cyber defense capabilities by means of increasing the intelligence of the defense systems. after surveying the papers available about artificial intelligence applications in cd, we can conclude that useful applications already exist. they belong, first of all, to applications of artificial neural nets in perimeter defense and some other cd areas. from the other side - it has become obvious that many cd problems can be solved successfully only when methods of artificial intelligence are being used. for example, wide knowledge usage is necessary in decision making, and intelligent decision support is one of yet unsolved problems in cd.
00e1cf56-4576-4799-8447-3177fe039430 elastic memory: bring elasticity back to in-memory big data analytics recent big data processing systems provide quick answers to users by keeping data in memory across a cluster. as a simple way to manage data in memory, the systems are deployed as long-running workers on a static allocation of the cluster resources. this simplicity comes at a cost: elasticity is lost. using today's resource managers such as yarn and mesos, this severely reduces the utilization of the shared cluster and limits the performance of such systems. in this paper, we propose elastic memory, an abstraction that can dynamically change the allocated memory resource to improve resource utilization and performance. with elastic memory, we outline how we enable elastic interactive query processing and machine learning.
00e25623-fa93-49d5-aa9f-b5ea40c5ff73 deep learning for finance: deep portfolios: j. b. heaton, n. g. polson and j. h. witte 
00e261f4-07fb-40de-880c-6f4164dba210 clustering methods for statistical analysis of genome databases clustering techniques find interesting and previously unknown patterns in largescale data embedded in a large multi dimensional space and are applied to a wide variety of problems like customer segmentation, biology, data mining techniques, machine learning and geographical information systems. clustering algorithms are used efficiently to scale up with the dimensionality of the data sets and the data base size. hierarchical clustering methods in particular are widely used to find patterns in multi dimensional data. since clustering is an unsupervised learning technique, fewer or greater numbers of clusters may be desired. a key step in the analysis of gene expression data is the identification of groups of genes that are similar in nature. the developments of micro array technologies provide a powerful tool by which the expression patterns of thousands of genes can be monitored simultaneously. this paper describes the major statistical approaches in hierarchical clustering and compares the linkage methods that are used in gene expression data along with experimental results.
00e27aae-2bc5-4312-895d-6b28c2ba9ed9 improving phrase-based machine translation 1 overview current state-of-the-art machine translation systems use a phrase-based scoring model for choosing among candidate translations in a target language, typically english. these models are deemed phrase-based because candidate sentence scores are in large part a product of phrase translation probabilities. these translation probabilities must be learned in some unsupervised manner from a pair of sentence-aligned corpora. with the end goal of improving upon the published results of such systems, our project proceeded in two stages. first, we attempted to duplicate the performance results of existing end-to-end translation systems by piecing together available components and engineering the remainder guided by published techniques. second, we identified two significant shortcomings of published systems and attempted to remedy them via machine learning techniques. in particular, we chose to learn phrase translation probabilities directly rather than deriving them heuristically. we also augmented the scoring model to relax a troublesome independence assumption across phrases.
00e2e68e-42ce-499d-8fa0-1e6e1e3c9b6e automatic detection of radio signal obstruction in wireless sensor networks' on-demand deployment this paper presents an application of machine learning approach for automatic terrain classification suitable for optimal wireless sensor network performance in on-demand deployment. the work entails practical terrain image processing using supervised svm kernel algorithm moving from gray scale level to color and covering every aspect of a typical terrain image. this paper showcases the integral part of proposed intelligent decision making framework that will be used in wireless sensor network deployment process. the proposed system will automatically identify the areas with potential obstructions to radio frequency signal in a pre-deployment procedure or simulation. this research work presents the performance of the approach which is consistence with practical deployment behavior.
00e34ca6-6075-4a32-b33e-7df3d3ab09ad integrated systems view on networking by hormones in arabidopsis immunity reveals multiple crosstalk for cytokinin phytohormones signal and combine to maintain the physiological equilibrium in the plant. pathogens enhance host susceptibility by modulating the hormonal balance of the plant cell. unlike other plant hormones, the detailed role of cytokinin in plant immunity remains to be fully elucidated. here, extensive data mining, including of pathogenicity factors, host regulatory proteins, enzymes of hormone biosynthesis, and signaling components, established an integrated signaling network of 105 nodes and 163 edges. dynamic modeling and system analysis identified multiple cytokinin-mediated regulatory interactions in plant disease networks. this includes specific synergism between cytokinin and salicylic acid pathways and previously undiscovered aspects of antagonism between cytokinin and auxin in plant immunity. predicted interactions and hormonal effects on plant immunity are confirmed in subsequent experiments with pseudomonas syringae pv tomato dc3000 and arabidopsis thaliana. our dynamic simulation is instrumental in predicting system effects of individual components in complex hormone disease networks and synergism or antagonism between pathways.
00e37182-3d4a-4ad9-83a0-5bb8f0323ef3 genetic-based approach for cue phrase selection in dialogue act recognition automatic cue phrase selection is a crucial step for designing a dialogue act recognition model using machine learning techniques. the approaches, currently used, are based on specific type of feature selection approaches, called ranking approaches. despite their computational efficiency for high dimensional domains, they are not optimal with respect to relevance and redundancy. in this paper we propose a genetic-based approach for cue phrase selection which is, essentially, a variable length genetic algorithm developed to cope with the high dimensionality of the domain. we evaluate the performance of the proposed approach against several ranking approaches. additionally, we assess its performance for the selection of cue phrases enriched by phrases type and phrases position. the results provide experimental evidences on the ability of the genetic-based approach to handle the drawbacks of the ranking approaches and to exploit cues type and cues position information to improve the selection. furthermore, we validate the use of the genetic-based approach for machine learning applications. we use selected sets of cue phrases for building a dynamic bayesian networks model for dialogue act recognition. the results show its usefulness for machine learning applications.
00e3dc6a-d047-46b0-92aa-bbb4e40dfcbc visual data mining with self-organising maps for ventricular fibrillation analysis detection of ventricular fibrillation (vf) at an early stage is being deeply studied in order to lower the risk of sudden death and allows the specialist to have greater reaction time to give the patient a good recovering therapy. some works are focusing on detecting vf based on numerical analysis of time-frequency distributions, but in general the methods used do not provide insight into the problem. however, this study proposes a new methodology in order to obtain information about this problem. this work uses a supervised self-organising map (som) to obtain visually information among four important groups of patients: vf (ventricular fibrillation), vt (ventricular tachycardia), hp (healthy patients) and ahr (other anomalous heart rates and noise). a total number of 27 variables were obtained from continuous surface ecg recordings in standard databases (mit and aha), providing information in the time, frequency, and time-frequency domains. self-organising maps (soms), trained with 11 of the 27 variables, were used to extract knowledge about the variable values for each group of patients. results show that the som technique allows to determine the profile of each group of patients, assisting in gaining a deeper understanding of this clinical problem. additionally, information about the most relevant variables is given by the som analysis.
00e44d0c-03d3-428a-87cd-26c1a029860f a parameter estimation method for graph based semi-supervised classification semi-supervised learning have been successfully applied in many applications by the use of unlabeled data to help labeled data in classification. in recent years, graph based semi- supervised learning approaches show great promising. however, the performance of these approaches depends heavily on some estimated parameters for the affinity weight matrix of the graph. we propose a path based maximum effective similarity (mes) method which can estimate parameters according to the path between data points along some low dimensional manifold in feature space. experimental results show the significant improvements in performance over the existing graph based approaches.
00e4623f-c995-4ffe-8d17-fc477b1de153 atomistic and electronic structure of (x2o3)n nanoclusters; n=15, x=b, al, ga, in and tl the stable and metastable, as measured using an all-electron density functional theory approach, stoichiometric clusters of boron, aluminium, gallium, indium and thallium oxide are reported. initial candidate structures were found using an evolutionary algorithm to search the energy landscape, defined using classical interatomic potentials, for alumina and india followed by data mining or rescaling. characterization of the refined structures was performed by electronic structure techniques at the hybrid density functional and many-body gw levels of theory. we make accurate predictions of the spectroscopic properties represented by mean ionization potentials of 11.4, 9.9, 9.8, 8.8 and 8.4 ev and electron affinities of 0.05, 1.1, 1.6, 1.9 and 2.5 ev for boria, alumina, gallia, india and thallia, respectively. the changes in the global minima, atomistic and electronic properties with respect to the cluster and cation size are discussed.
00e49b1c-5964-48e4-a810-dee0668b266b an efficient way to find frequent pattern with dynamic programming approach data mining place viral aspect in many of the applications like market-basket analysis, fraud detection etc. in data mining association rule mining and frequent pattern mining, both are key feature of market-basket analysis. in a given large amount of transnational database where each record consists of items purchased by customer at store. one of the basic market basket analysis algorithm is an apriori, which generate all candidates item-set frequent pattern. in this research paper we describe the improved candidate 1-itemsets generation and candidate 2-itemsets generation from traditional technique. this algorithm utilizes the dynamic programming approach to facilitate fast candidate itemset generation and searching. we have compared results with previous approach that optimize the database scans and eliminate duplicate candidate itemset generation. this technique helps research scholar.
00e4b584-4b6b-421f-8172-18c94e635c53 modelacion molecular y variacion estructural de las integrasas de dos retrovirus humanos: htlv-i y vih-1 molecular modeling and structural variation of two human retrovirus integrases: htlv-i and hiv-1 resumen           objetivo:    analizar las caracteristicas moleculares y de variacion de secuencias de las integrasas del htlv-i y del vih-1 y sus variantes poblacionales.     metodologia:    analisis de secuencias y estructuras obtenidas de diferentes bases de datos; para ello se utilizaron programas computacionales de modelacion de estructuras proteicas e identificacion de sustituciones polimorficas en secuencias de aminoacidos de integrasas del htlv-i y vih-1 previamente reportadas.     materiales y metodos:    tanto la integrasa del htlv-i como la del vih-1 son proteinas compuestas por 288 residuos de aminoacidos. se encontro un parecido de estructuras terciarias entre los dominios cataliticos de las in de vih-1, asv y rsv con la del htlvi. a partir de 103 secuencias completas de la integrasa del vih-1 se registraron, en 46 codones, un total de 53 sustituciones que se localizaron en diferentes posiciones de la proteina nativa; las mas frecuentes fueron: n27g (32,1%), a265v (30,1%), l101i (31,1%) y t123a (27,0%). ninguna de las sustituciones mas frecuentemente encontradas genero un cambio en el plegamiento nativo de la correspondiente region.     conclusion:    la estructura tridimensional del dominio central catalitico de la integrasa condicionaria su actividad y su relacion con moleculas potencialmente inhibidoras. las sustituciones observadas fueron neutrales sin alterar la estructura nativa. los resultados obtenidos confirman que la integrasa es un nuevo y promisorio blanco para el desarrollo de terapias antirretrovirales mas efectivas en el siglo xxi.          palabras clave:    retrovirus, integrasa, modelacion molecular, estructura proteica,    polimorfismos proteicos, resistencia antiviral.          abstract               objective   : to analyze the molecular characteristics and amino acid sequence variations of htlv-i and of hiv-1 integrases and their population variants.     materials adn methods:    data mining and analysis of integrase sequences and protein structure data bases by using appropriate software for modelling and search for polymorphic substitutions in htlv-i and hiv-1 integrase amino acid sequences previously reported.     results:    htlv-i and hiv-1 integrases are proteins of 288 amino acid residues. structural modeling of tertiary folding of htlv-i integrase catalytic central domains, showed closed structural characteristic with those of hiv-1, asv and rsv. from 103 full amino acid sequences of hiv-1 integrase, 53 substitutions located in 46 different codons were recorded. the more frequents correspond to n27g (32,1%), l101i (31,1%), a265v (30,1%) and t123a (27,0%). none of these frequent substitutions introduced changes in the folding of hiv-1 native integrase.     conclusion   : the tridimensional structure of central catalytic domain would influence the integrase activity and its relationship with potentially inhibitory molecules. those observed amino acid substitutions were neutral and do not alter the native protein structure.  our data confirm those previously published, and enable us to propose that in is a new and promissory target for develop more effective antiviral therapies in the xxi century.           keywords:    retrovirus, integrase, molecular modeling, protein structure, protein    polymorphisms, antiviral resistance.
00e4d198-b5e5-4535-aa92-f72a65712590 computational everyday life human behavior model as servicable knowledge a project called `open life matrix' is not only a research activity but also real problem solving as an action research. this concept is realized by large-scale data collection, probabilistic causal structure model construction and information service providing using the model. one concrete outcome of this project is childhood injury prevention activity in new team consist of hospital, government, and many varieties of researchers. the main result from the project is a general methodology to apply probabilistic causal structure models as servicable knowledge for action research. in this paper, the summary of this project and future direction to emphasize action research driven by artificial intelligence technology are discussed.
00e5070b-cee4-4dbb-b172-e4097e7cf1ce artificial life contest - a tool for informal teaching of artificial intelligence 
00e518d1-fd14-4ea3-aad1-bb330ef5b30a data mining for security information: a survey this paper will present a survey of the current published work and products available to do off-line data mining for computer network security information. hundreds of megabytes of data are collected every second that are of interest to computer security professionals. this data can answer questions ranging from the proactive, ''which machines are the attackers going to try to compromise?'' to the reactive, ''when did the intruder break into my system and how?'' unfortunately, there's so much data that computer security professionals don't have time to sort through it all. what we need are systems that perform data mining at various levels on this corpus of data in order to ease the burden of the human analyst. such systems typically operate on log data produced by hosts, firewalls and intrusion detection systems as such data is typically in a standard, machine readable format and usually provides information that is most relevant to the security of the system. systems that do this type of data mining for security information fall under the classification of intrusion detection systems. it is important to point out that we are not surveying real-time intrusion detection systems. instead, we examined what is possible when the analysis more  is done off-line. doing the analysis off-line allows for a larger amount of data correlation between distant sites who transfer relevant log files periodically and may be able to take greater advantage of an archive of past logs. such a system is not a replacement for a real-time intrusion detection system but should be used in conjunction with one. in fact, as noted previously, the logs of the real-time ids may be one of the inputs to the data mining system. we will concentrate on the application of data mining to network connection data, as opposed to system logs or the output of real-time intrusion detection systems. we do this primarily because this data is readily obtained from firewalls or real-time intrusion detectors and it looks the same regardless of the network architecture or the systems that run on the network. this similarity greatly simplifies the data cleansing step and provides a dataset with high orthogonality between multiple sites, increasing the accuracy of the data mining operations. the decision to use connection logs instead of packet logs is discussed below. this paper will survey both the research that has been done in this area to date and publicly available products that perform such tasks.  less
00e5e751-a301-439c-8b6c-1b64c408b379 subjective content accessibility using database approach for digital library todays digital library is a massive collection of various types and categories of documents. the existing search engines do not provide subjective search from the collection, as no information about context is stored. the existing search engine mostly uses the agent based search then the database based search. the database search is simpler easier but static verses dynamic web. the work shows how database become dynamic, subjective and search query becomes simpler. the subjective and context based search is necessity of searching in digital library. the user who may be researcher, students, and even common person expect subject or context and need content accessibility precise and subject specific. this paper presents the topic-word specific subjective search using the database approach in digital library, by data mining technique in warehouse.
00e602a1-9283-43ea-8ac0-164561bd860d image multi-thresholding by combining the lattice boltzmann model and a localized level set algorithm during the last decades, the development of high dimensional large-scale imaging devices increases the need of fast, accurate and parallelizable segmentation methods. due to its intrinsic advantages such as its ability to handle complex shapes, the level set method (lsm) has been widely used. nevertheless, the method is computational expensive in image segmentation, which limits its use in real-time systems and volume images segmentation. in this paper we propose an adaptive image multi-thresholding method which uses a localized level set method to detect automatically the best thresholds values from some initial given values. instead to solve the level set equation (lse) by using the traditional methods based on some finite difference or finite volume, we use the highly parallelizable lattice boltzmann method (lbm). all the more, the method is faster since it is solved in histogram domain rather than the pixel domain. the time complexity is therefore considerably reduced since the number of gray levels is generally much smaller than the size of the image. the method is efficient, highly parallelizable and faster than those based on the lsm. experiments on synthetic, real-world, medical and man-made object images demonstrate the performance of the proposed method.
00e633ee-0ac7-473a-8898-9431587f57f4 using knowledge graph to handle label imperfection the performance of classification tasks extremely relies on data quality, while in real world label noises inevitably exists because of data entry errors, transmit errors and subjectivity of taggers. different methods have been proposed to deal with label imperfection, including robust algorithms by avoid overfitting, filtering mechanism to remove noises and correction mechanism to revise noises. in this paper, we pro- pose an approach based on knowledge graph to perceive and correct the label errors in training data. experiments on a medical q&a data set reveal that our knowledge graph based approach can be effective on pro- moting classification performance and data quality. the results as well show our approach can work in a relatively high noise level and be applied in other data mining tasks demanding deep understanding.
00e654e9-1923-470c-b5ab-f28c08adff2c new framework for dynamic scheduling of production systems a concept for dynamic scheduling in manufacturing systems is proposed. the scope of 'dynamic scheduling' treated includes online dynamic change of some scheduling parameters such as rules for part dispatching, machine selection, or routing. if-then-type heuristic operators are utilized to perform this online real-time rule selection, and offline machine learning is used to obtain more detailed and powerful heuristics than those implemented by human experts or programmers. a learning algorithm has been developed to formulate operators that can treat quantity-type as well as quality-type information. a prototype computer program named learning aided dynamic scheduler (lads) has been developed. a simulation study using lads indicates good results for dynamic scheduling. >
00e6d0f4-400e-49b4-a96d-df532cc290c5 prediction of users retweet times in social network in view of the fact that the propagation path topology cannot effectively deal with complex social network consists of hundreds of millions of users. more researchers choose to use machine learning methods to complete retweet prediction. those use the classification method to judge whether a message will be retweeted or not. this paper argues that retweet prediction should be regression analysis problem, not just the classification problem. through collecting user characteristics on twitter and selecting some features which have an important impact on the retweet behavior, a prediction algorithm based on the logistic regression for users retweet times in social network was proposed. experiment results based on the actual data set show the regression analysis predicting model has a good predicting accuracy in dealing with retweet predicting, the proposed method is effectiveness.
00e6f19c-f73a-4120-90ee-a0bb9acad12f finding relevant parameters for the thinfilm photovoltaic cells production process with the application of data mining methods 
00e70d07-51a2-47b0-80cd-09b3f4ceb4b9 automated number plate recognition system using machine learning algorithms (kstar) in this paper, a simple technique is presented for automated number plate recognition (anpr) system, which can be used many applications for automated recognition of vehicle number plates. a simple algorithm is designed that can help to recognize number plates of vehicles using images taken by camera. the recognition of number plate's algorithm has five parts: acquisition image, pre-processing, edge detection and segmentation, feature extraction and recognition of character of number plates using suitable ml algorithms.
00e79801-fb05-4e80-bcb6-e55cfcd60ac0 music staff removal with supervised pixel classification this work presents a novel approach to tackle the music staff removal. this task is devoted to removing the staff lines from an image of a music score while maintaining the symbol information. it represents a key step in the performance of most optical music recognition systems. in the literature, staff removal is usually solved by means of image processing procedures based on the intrinsics of music scores. however, we propose to model the problem as a supervised learning classification task. surprisingly, although there is a strong background and a vast amount of research concerning machine learning, the classification approach has remained unexplored for this purpose. in this context, each foreground pixel is labelled as either staff or symbol. we use pairs of scores with and without staff lines to train classification algorithms. we test our proposal with several well-known classification techniques. moreover, in our experiments no attempt of tuning the classification algorithms has been made, but the parameters were set to the default setting provided by the classification software libraries. the aim of this choice is to show that, even with this straightforward procedure, results are competitive with state-of-the-art algorithms. in addition, we also discuss several advantages of this approach for which conventional methods are not applicable such as its high adaptability to any type of music score.
00e7b04b-1cf3-4013-a85e-a2de059360d7 the integration and application of spatial data mining and gis with the development of the gis database, the data are so large and come from different sources that the data can not be dealt with by human brains. therefore, it becomes increasely important to find a method that can automatic, quickly, efficiently, get more information from the gis database. the technology of data mining meets the need. therefore, the technology of data mining is introduced into gis. this paper describes the difference between the spatial data mining and the rdms and some methods of spatial data mining. furthermore, a system of integration of spatial data mining and gis and its application are introduced.
00e7e0bf-1fda-49ca-9c88-c7daa50fcac3 anytime contract search heuristic search is a fundamental problem solving paradigm in artificial intelligence.weaddresstheproblemofdevelopingheuristicsearchalgorithmswhere intermediate results are sought at intervals of time which may or may not be known apriori.inthispaper,weproposeanefficientanytimealgorithmcalledanytimecon- tract search (based on the contract search framework) which incrementally explores the state-space with the given contracts (intervals of reporting). the algorithm works without restarting and dynamically adapts for the next iteration based on the current contract and the currently explored state-space. the proposed method is complete on bounded graphs. experimental results with different contract sequences on the sliding-tilepuzzleproblemandthetravellingsalespersonproblem(tsp)showthat anytime contract search outperforms some of the state-of-the art anytime search algorithms that are oblivious to the given contracts. also, the non-parametric version of the proposed algorithm which is oblivious of the reporting intervals (making it an anytime algorithm) performs well compared to many available schemes.
00e80a80-1781-4405-9e8d-84a08180edff support vector regression and immune clone selection algorithm for predicting financial distress in the analysis of predicting financial distress based on support vector regression (svr), irrelevant or correlated features in the samples could spoil the performance of the svr classifier, leading to decrease of prediction accuracy. in order to solve the problems mentioned above, this paper used rough sets as a preprocessor of svr to select a subset of input variables and employed the immune clone selection algorithm (icsa) to optimize the parameters of svr. additionally, the proposed icsa-svr model that can automatically determine the optimal parameters was tested on the prediction of financial distress. then, we compared the proposed icsa-svr model with other artificial intelligence models of (bpn and fix-svr). the experiment indicates that the proposed method is quite effective and ubiquitous.
00e8eba4-53d9-40fd-bc5c-58e682a7f46b network traffic clustering using random forest proximities the recent years have seen extensive work on statistics-based network traffic classification using machine learning (ml) techniques. in the particular scenario of learning from unlabeled traffic data, some classic unsupervised clustering algorithms (e.g. k-means and em) have been applied but the reported results are unsatisfactory in terms of low accuracy. this paper presents a novel approach for the task, which performs clustering based on random forest (rf) proximities instead of euclidean distances. the approach consists of two steps. in the first step, we derive a proximity measure for each pair of data points by performing a rf classification on the original data and a set of synthetic data. in the next step, we perform a k-medoids clustering to partition the data points into k groups based on the proximity matrix. evaluations have been conducted on real-world internet traffic traces and the experimental results indicate that the proposed approach is more accurate than the previous methods.
00e967ad-b304-44f8-b8f1-b6ab06f6dd53 a comparative study of algorithms for land cover change ecosystem-related observations from remote sensors on satellites offer huge potential for understanding the location and extent of global land cover change. this paper presents a comparative study of three time series based algorithms for detecting changes in land cover. the techniques are evaluated quantitatively using forest fire ground truth from the state of california for 2000-2009. on relatively high quality data sets, all three schemes perform reasonably well, but their ability to handle noise and natural variability in the vegetation data differs dramatically. in particular, one of the algorithms significantly outperforms the other two since it accounts for variability in the time series. the climate and earth sciences have recently undergone a rapid transformation from a data- poor to a data-rich environment. in particular, climate and ecosystem related observations from remote sensors on satellites, as well as outputs of climate or earth system models from large-scale computational platforms, provide terabytes of temporal, spatial and spatio-temporal data. these massive and information-rich datasets offer huge potential for advancing the science of land cover change, climate change and anthropogenic impacts. one important area where remote sensing data can play a key role is in the study of land cover change. specifically, the conversion of natural land cover into human-dominated cover types con- tinues to be a change of global proportions with many unknown environmental consequences. in addition, being able to assess the carbon risk of changes in forest cover is of critical importance for both economic and scientific reasons. in fact, changes in forests account for as much as 20% of the greenhouse gas emissions in the atmosphere, an amount second only to fossil fuel emissions. thus, there is a need in the earth science domain to systematically study land cover change in order to understand its impact on local climate, radiation balance, biogeochemistry, hydrology, and the diversity and abundance of terrestrial species. land cover conversions include tree harvests in forested regions, urbanization, and agricultural intensification in former woodland and natural grassland areas. these types of conversions also have significant public policy implications due to issues such as water supply management and atmospheric co2 output. in spite of the importance of this problem and the considerable advances made over the last few years in high-resolution satellite data, data mining, and online mapping tools and services, end users still lack practical tools to help them manage and transform this data into actionable knowledge of changes in forest ecosystems that can be used for decision making and policy planning purposes. for ecosystem data, change detection is the process of identifying changes in the cover type and/or human use of the earth. examples include the conversion of forested land to barren land (possibly due to deforestation or a fire), grasslands to golf courses and farmland to housing developments. there is a large body of research in change detection using remotely sensed image data. most pre- vious change detection studies primarily rely on examining differences between two or more satellite images acquired on different dates (9). however, these techniques have well-known limitations (as
00eaa4e7-fbb3-4a8c-8fc0-32bec1032fcd machine learning techniques for high performance engine calibration 
00eab2e5-6cc6-429d-bc18-3286983a75c0 the snake for visualizing and for counting clusters in multivariate data we introduce the snake, a new tool for the visualization and exploration of a multivariate dataset. the snake connects each data point along a single short path. using techniques from the traveling salesman problem (tsp), it is possible to find such a path in polynomial (nearly quadratic) computational time. a plot of the individual segment lengths versus their position along the path transforms the original multidimensional dataset into a one-dimensional time-series of interpoint distances. the snake traces the local structure of a datacloud, so this visualization is most useful for detecting density fluctuations: regions of high density appear as many consecutive short segments, while regions of low density appear as many consecutive long segments. dips in the time series reveal the presence of clustering and can be used to count the number of modes in the datacloud. we illustrate the technique on a variety of artificial and real-world datasets. copyright  2010 wiley periodicals, inc. statistical analysis and data mining 3: 236-252, 2010
00ead3ab-d9df-487b-bd05-fdfe0742ca54 ensemble model of artificial neural networks with randomized number of hidden neurons conventional artificial intelligence techniques and their hybrid models are incapable of handling several hypotheses at a time. the limitation in the performance of certain techniques has made the ensemble learning paradigm a desirable alternative for better predictions. the petroleum industry stands to gain immensely from this learning methodology due to the persistent quest for better prediction accuracies of reservoir properties for improved hydrocarbon exploration, production, and management activities. artificial neural networks (ann) has been applied in petroleum engineering but widely reported to be lacking in global optima caused mainly by the great challenge involved in the determination of optimal number of hidden neurons. this paper presents a novel ensemble model of ann that uses a randomized algorithm to generate the number of hidden neurons in the prediction of petroleum reservoir properties. ten base learners of the ann model were created with each using a randomly generated number of hidden neurons. each learner contributed in solving the problem and a single ensemble solution was evolved. the performance of the ensemble model was evaluated using standard evaluation criteria. the results showed that the performance of the proposed ensemble model is better than the average performance of the individual base learners. this study is a successful proof of concept of randomization of the number of hidden neurons and demonstrated the great potential for the application of this learning paradigm in petroleum reservoir characterization.
00ead695-c987-40ae-84a3-7b9c29d167ce statistical quality design and control 
00eaf2be-6af5-4ced-9207-5b3cc94ef06e an embedded fuzzy controller for a behavior-based mobile robot with guaranteed performance in this paper, an embedded fuzzy controller for a nonholonomic mobile robot is developed. the mobile robot was built based on the behavior-based artificial intelligence, where several levels of competences and behaviors are implemented. a class of fuzzy control laws is formulated using the lyapunov's direct method, which can guarantee the convergence of the steering errors. theoretical analysis of the fuzzy control algorithms for steering control of the mobile robot is performed. the requirements for a suitable rule base selection in the proposed fuzzy controller are provided, which can guarantee the asymptotical stability of the system. simulation and experimental studies are conducted to investigate the performance of the proposed fuzzy controller. it can achieve the desired turn angle and make the mobile robot follow the target trajectory satisfactorily.
00ec0d59-0b7f-429a-86bb-ef820ddbec46 nanopsychiatrythe potential role of nanotechnologies in the future of psychiatry: a systematic review nanomedicine is defined as the area using nanotechnology's concepts for the benefit of human beings' health and well being. in this article, we aimed to provide an overview of areas where nanotechnology is applied and how they could be extended to care for psychiatric illnesses. the main applications of nanotechnology in psychiatry are (i) pharmacology. there are two main difficulties in neuropharmacology: drugs have to pass the bloodbrain barrier and then to be internalized by targeted cells. nanoparticles could increase drugs bioavailability and pharmacokinetics, especially improving safety and efficacy of psychotropic drugs. liposomes, nanosomes, nanoparticle polymers, nanobubbles are some examples of this targeted drug delivery. nanotechnologies could also add new pharmacological properties, like nanoshells and dendrimers (ii) living analysis. nanotechnology provides technical assistance to in vivo imaging or metabolome analysis (iii) central nervous system modeling. research teams have succeeded to modelize inorganic synapses and mimick synaptic behavior, a step essential for further creation of artificial neural systems. some nanoparticle assemblies present the same small worlds and free-scale networks architecture as cortical neural networks. nanotechnologies and quantum physics could be used to create models of artificial intelligence and mental illnesses. we are not about to see a concrete application of nanomedicine in daily psychiatric practice. even if nanotechnologies are promising, their safety is still inconsistent and this must be kept in mind. however, it seems essential that psychiatrists do not forsake this area of research the perspectives of which could be decisive in the field of mental illness.
00ec14ed-5e79-4d70-ab49-9c295abbb138 kernel optimization in discriminant analysis kernel mapping is one of the most used approaches to intrinsically derive nonlinear classifiers. the idea is to use a kernel function which maps the original nonlinearly separable problem to a space of intrinsically larger dimensionality where the classes are linearly separable. a major problem in the design of kernel methods is to find the kernel parameters that make the problem linear in the mapped representation. this paper derives the first criterion that specifically aims to find a kernel representation where the bayes classifier becomes linear. we illustrate how this result can be successfully applied in several kernel discriminant analysis algorithms. experimental results, using a large number of databases and classifiers, demonstrate the utility of the proposed approach. the paper also shows (theoretically and experimentally) that a kernel version of subclass discriminant analysis yields the highest recognition rates.
00ec3f20-26b8-4cd7-9b79-ec3d4365b3cf using the kernel trick in compressive sensing: accurate signal recovery from fewer measurements compressive sensing accurately reconstructs a signal that is sparse in some basis from measurements, generally consisting of the signal's inner products with gaussian random vectors. the number of measurements needed is based on the sparsity of the signal, allowing for signal recovery from far fewer measurements than is required by the traditional shannon sampling theorem. in this paper, we show how to apply the kernel trick, popular in machine learning, to adapt compressive sensing to a different type of sparsity. we consider a signal to be nonlinearly k-sparse if the signal can be recovered as a nonlinear function of k underlying parameters. images that lie along a low-dimensional manifold are good examples of this type of nonlinear sparsity. it has been shown that natural images are as well [1]. we show how to accurately recover these nonlinearly k-sparse signals from approximately 2k measurements, which is often far lower than the number of measurements usually required under the assumption of sparsity in an orthonormal basis (e.g. wavelets). in experimental results, we find that we can recover images far better for small numbers of compressive sensing measurements, sometimes reducing the mean square error (mse) of the recovered image by an order of magnitude or more, with little computation. a bound on the error of our recovered signal is also proved.
00ec931f-1780-4fd9-a168-9e099e460004 new developments for determinacy analysis: diclique-based approach determinacy analysis (da) is a method that solves tasks of data mining (it enables to describe by the rules the subset of objects determined by the user). there are different treatments in da: step by step and das-like in algoritmic view, one solution and multiple solutions as a result, additive and non-additive sets of rules in systematic view. thereat the essence of the method itself does not change, only the rules change. there is a number of lacks in da base algorithms: they are too labour-intensive (step by step approach) or they find only a limited set of rules (i.e. only one system of rules of many possible systems, in case of das). in this paper we show that da can be reduced to the diclique finding task that is well-known from the graph theory, we present the prerequisites to take into account and explain how it influences the rules. the diclique-based da enables to set up the da tasks of new type: to find a system with minimal number of rules, to find a system with minimal number of shortest rules (for example). reducing da to a diclique finding task, the basis for the new generation of da algorithms is founded.
00ee6405-437d-4fc7-8006-d70cf382f58d suchraumbeschr ankung f ur relationales data mining 
00ee7b52-4ca9-4cb9-97d4-0b5dd1d2aee2 another approach to polychotomous classification 
00eec0e2-0d00-4e08-81d9-1526e553f161 intelligent grid enabled services for neuroimaging analysis this paper reports our work in the context of the neugrid project in the development of intelligent services for a robust and efficient neuroimaging analysis environment. neugrid is an ec-funded project driven by the needs of the alzheimer's disease research community that aims to facilitate the collection and archiving of large amounts of imaging data coupled with a set of services and algorithms. by taking alzheimer's disease as an exemplar, the neugrid project has developed a set of intelligent services and a grid infrastructure to enable the european neuroscience community to carry out research required for the study of degenerative brain diseases. we have investigated the use of machine learning approaches, especially evolutionary multi-objective meta-heuristics for optimising scientific analysis on distributed infrastructures. the salient features of the services and the functionality of a planning and execution architecture based on an evolutionary multi-objective meta-heuristics to achieve analysis efficiency are presented. we also describe implementation details of the services that will form an intelligent analysis environment and present results on the optimisation that has been achieved as a result of this investigation.
00eeede8-a7d1-4202-9dcf-8b019372ab4c learning for distributed artificial intelligence systems in the context of improving the performance of a group of agents, allowing individual agents to improve their performance may not be enough to improve the performance of the group. to apply learning to the overall group performance the agents need to adapt and learn to work with each other. indeed, the agents may not need to learn more about the domain, as in the traditional sense of machine learning, to improve group performance. in fact, to improve the performance of the group, the agents may only need to learn to work together and not necessarily improve their individual performance. in addition, not all of the agents must be able to learn or adapt to allow the group to improve. this papers examines the learning potential for different dimensions of distributed artificial intelligence systems. it concerns adaptation and learning at the knowledge and organizational levels. several existing systems are examined and classified according to the dimensions for learning. the paper does not examine general dimensions for dai, but only those dimensions that can be used for examining learning in a dai system. >
00ef5720-2b4d-4670-a9b5-1aecfb3173f9 integrating vision, touch and natural language in the control of a situation-oriented behavior-based humanoid robot in the personal or service robotics domain very close interaction between humans and robots is crucial. to facilitate such a close interaction we propose to design the robot according to an anthropomorphic model, to combine visual and tactile sensing and to base human-robot communication on natural language. furthermore, we argue that integrating these key technologies into a robot requires a certain kind of behavior-based system architecture that relies on an understanding of situations for the selection of the behavior to be executed. a robot implementing this architecture allows a human interface to be realized that makes the robot appear intelligent and easy to communicate and interact with. results of first experiments are reported.
00ef75ef-1e76-4cd2-8d0b-4162593ee860 multiple self-organizing maps for intrusion detection 
00efb6f2-ac6e-41e5-b364-3afe638c8cd9 random primal-dual proximal iterations for sparse multiclass svm sparsity-inducing penalties are useful tools in variational methods for machine learning. in this paper, we propose two block-coordinate descent strategies for learning a sparse multiclass support vector machine. the first one works by selecting a subset of features to be updated at each iteration, while the second one performs the selection among the training samples. these algorithms can be efficiently implemented thanks to the flexibility offered by recent randomized primal-dual proximal methods. experiments carried out for the supervised classification of handwritten digits demonstrate the interest of considering the primal-dual approach in the context of block-coordinate descent. the efficiency of the proposed algorithms is assessed through a comparison of execution times and classification errors.
00f0391e-1020-4c18-8e96-fe4cb43a4488 artificial bee colony data miner (abc-miner) data mining aims to discover interesting, non-trivial, and meaningful information from large datasets. one of the data mining tasks is classification, which aims to assign the given datasets to the most suitable classes. classification rules are used in many domains such as medical sciences, banking, and meteorology. however, discovering classification rules is challenging due to large size and noisy structure of the datasets, and the difficulty of discovering general and meaningful rules. in the literature, there are several classical and heuristic algorithms proposed to mine classification rules out of large datasets. in this paper, a new and novel heuristic classification data mining approach based on artificial bee colony algorithm (abc) was proposed (abc-miner). the proposed approach was compared with particle swarm optimization (pso) rule classification algorithm and c4.5 algorithm using benchmark datasets. the experimental results show the efficiency of the proposed method.
00f09d2e-0f88-44a0-9d1c-993427fdc666 automatic extraction of lexico-syntactic patterns for detection of negation and speculation scopes detecting the linguistic scope of negated and speculated information in text is an important information extraction task. this paper presents scopefinder, a linguistically motivated rule-based system for the detection of negation and speculation scopes. the system rule set consists of lexico-syntactic patterns automatically extracted from a corpus annotated with negation/speculation cues and their scopes (the bioscope corpus). the system performs on par with state-of-the-art machine learning systems. additionally, the intuitive and linguistically motivated rules will allow for manual adaptation of the rule set to new domains and corpora.
00f0b068-64f6-496b-9677-a5413edb82ef detection of line outage in transmission networks using phasor measurement units aided by support vector machine algorithms many protection applications are based upon the phasor measurement units (pmus) technology. therefore, pmus have been increasingly widespread throughout the power network, and there are several researches have been made to locate the pmus for complete system observability. this paper introduces an important application of pmus in power system protection which is the detection of single line outage. in addition, a detection of the out of service line is achieved depending on the variations of phase angles measured at the system buses where the pmus are located. hence, a protection scheme from unexpected overloading in the network that may lead to system collapse can be achieved. such detections are based upon an artificial intelligence technique which is the support vector machine (svm) classification tool. to demonstrate the effectiveness of the proposed approach, the algorithm is tested using offline simulation for both the ieee 14-bus and the ieee 30-bus systems. two different kernels of the svm are tested to select the more appropriate one (i.e. polynomial and radial basis function (rbf) kernels are used).
00f1fdce-6c18-44eb-b592-f12dd9f39cd5 a hybrid online signature verification system supporting multi-confidential levels defined by data mining techniques online signature verification is used widely for user authentication in many applications. in existing systems, one confidential level is usually used as the threshold for signature verifications. this causes high false rejection ratio (frr) to systems in which signature verification is less important, and on the other hand, high false acceptance ratio (far) to systems in which signature verification is more critical. thus, applying multi-confidential levels to the signature verification is crucial to solve this problem. also, data mining techniques can be used to appropriately define multi-confidential levels. this paper proposes a hybrid online signature verification system supporting multi-confidential levels defined by data mining techniques.
00f26cff-6987-4b29-827a-fd802b05d79c image euclidean distance-based two-dimensional local diversity preserving projection previous works have demonstrated that principal component analysis(pca) well preserves the global information,i.e.,diversity of data,and plays an important role in pattern recognition,machine learning,and image processing.however,pca ignores the spatial relationships among pixels in images and does not well preserve the local diversity of data,which will impair the recognition accuracy and lead to unstableness to the perturbation of images.to address these problems,a novel approach,namely image euclidean distance based two-dimensional local diversity preserving projection(ied-2dldpp) is proposed.ied-2dldpp constructs an adjacency graph to model the variation of data and employs image euclidean distance to characterize the diversity of data,which explicitly considers the spatial relationships among pixels in the images.extensive experiments on yale,ar,and pie databases show the efficiency of the proposed method.
00f2ca1e-0b3b-45c4-b7b8-d3f182b65012 types of cost in inductive concept learning inductive concept learning is the task of learning to assign cases to a discrete set of classes. in real-world applications of concept learning, there are many different types of cost involved. the majority of the machine learning literature ignores all types of cost (unless accuracy is interpreted as a type of cost measure). a few papers have investigated the cost of misclassification errors. very few papers have examined the many other types of cost. in this paper, we attempt to create a taxonomy of the different types of cost that are involved in inductive concept learning. this taxonomy may help to organize the literature on cost-sensitive learning. we hope that it will inspire researchers to investigate all types of cost in inductive concept learning in more depth.
00f3075b-2f24-4baf-a12d-9bdd2e407e6a deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework the recent emergence of deep learning for characterizing complex patterns in remote sensing imagery reveals its high potential to address some classic challenges in this domain, e.g. scene classification. typical deep learning models require extremely large datasets with rich contents to train a multilayer structure in order to capture the essential features of scenes. compared with the benchmark datasets used in popular deep learning frameworks, however, the volumes of available remote sensing datasets are particularly limited, which have restricted deep learning methods from achieving full performance gains. in order to address this fundamental problem, this article introduces a methodology to not only enhance the volume and completeness of training data for any remote sensing datasets, but also exploit the enhanced datasets to train a deep convolutional neural network that achieves state-of-the-art scene classification performance. specifically, we propose to enhance any original dataset by applying th...
00f39717-6825-4514-a9d4-d33d2d34c578 a logic-theoretic classifier called circle we present a novel classifier based upon principles of logic-theoretic boolean function minimization. the classifier, called circle, recursively produces a set of implicants (or rules). the implicant set contains information not only about the presence of features, but also about their absence in determining class values. thus, circle's implicant set is initially non-monotonic with respect to inserting new tuples that have feature values that were not in the training set. one important benefit of this non-monotonicity, however, is that circle is capable of being robust in the presence of novel feature values. we have created a full implementation of circle using java as a host language and oracle database backend. because we are interested in data mining in bioinformatics, particularly genomic data, the database was borne out of necessity to both manage and effectively query the information.
00f426af-cc99-4f14-af23-7feabcb27a31 research on application of distributed data mining in anti-money laundering monitoring system from the reality, this article presents anti-money laundering real-time system architecture on the base of analyzing custom transaction. at the same time, data process, custom transaction and bank node are discussed.
00f4761e-643d-4c31-b190-8e544790c801 heuristic search for model structure: the benefits of restraining greed inductive modeling or "machine learning" algorithms are able to discover structure in high-dimensional data in a nearly automated fashion. these adaptive statistical methods - including decision trees, polynomial networks, projection pursuit models, and additive networks - repeatedly search for, and add on, the model component judged best at that state. because of the huge model space of possible components, the choice is typically greedy; that is, optimal only in the very short term. in fact, it is usual for the analyst and algorithm to be greedy at three levels: when choosing a 1) term within a model, 2) model within a family, and 3) family within a wide collection of methods. it is better, we argue, to "take a longer view" in each stage. for the first stage (term selection) examples are presented for classification using decision trees and estimation using regression. to improve the third stage (method selection) we propose fusing information from disparate models to make a combined model more robust. (fused models merge their output estimates but also share information on, for example, variables to employ and cases to ignore.) benefits of fusing are demonstrated on a challenging classification dataset, where the task is to infer the species of a bat from its chirps.
00f494d0-c65f-4940-83b9-8f8839357623 effect of binding pose and modeled structures on svmgen and glidescore enrichment of chemical libraries virtual screening consists of docking libraries of small molecules to a target protein followed by rank-ordering of the resulting structures using scoring functions. the ability of scoring methods to distinguish between actives and inactives depends on several factors that include the accuracy of the binding pose during the docking step and the quality of the three-dimensional structure of the target. here, we build on our previous work to introduce a new scoring approach (svmgen) that uses machine learning trained with features from statistical pair potentials obtained from three-dimensional crystal structures. we use svmgen and glidescore to explore how enrichment or rank-ordering is affected by binding pose accuracy. to that end, we create a validation set that consists strictly of proteins whose crystal structure was solved in complex with their inhibitors. for the rank-ordering studies, we use crystal structures from pdbbind along with corresponding binding affinity data provided in the database. in ...
00f4da37-aa07-4c1b-bba3-aa34dcd6aa40 privacy and utility preserving task independent data mining 
00f5069d-9a60-4309-80b7-5cb89e691c2c machine learning for simulation-based support of early collaborative design the research and development of a simulation-based decision support system (sb-dss) capable of assisting early collaborative design processes is presented. the requirements for such a system are included. existing collaborative dsss are shown to lack the capability to manipulate complex simulation-based relationships. on the other hand, advances within the machine learning in design community are shown to have a potential for providing, but have not yet addressed, simulation-based support for collaborative design processes. the developed sb-dss is described in terms of its four principal components. first, the behavior-evaluation (be) model is used to both structure individual, domain-specific decision models and organize these models into a collaborative decision model. second, a probabilistic framework for the be model enables management of the uncertainty inherent in learning and using simulation-based knowledge. significantly, this framework provides a constraint satisfaction environment in which simulation-based knowledge is used. third, a statistical neural network approach is used to capture simulation-based knowledge and build the probabilistic behavior models based on this knowledge. fourth, since a probability distribution theory does not exist for the nonlinear neural network approaches, monte carlo simulation is introduced as a method to sample the trained neural networks and approximate the likelihoods of design variable values. consequently, constraint satisfaction problem-solving capability is obtained. in addition, a mapping of the sb-dss architecture onto a collaborative design agent framework is provided. experimental evaluation of a prototype sb-dss system is summarized, and performance of the sb-dss with respect to search and usability metrics is documented. initial results in developing the simulation-based support for collaborative design are encouraging. lastly, a categorization of the machine learning approach and a critique of the proposed categorization scheme is presented.
00f6fbc2-f770-435a-ac86-db98dac72298 enhancement of systematic design processing by diagrams diagrams play a significant role in most knowledge presentation areas. this is especially the case in disciplines concerned with conceptual design where data visualization, processing, documentation and presentation require graphic illustration to express their underlying tacit aspects. despite their significance, the visual tools available in the toolbox of designers to help them apply generative processes are extremely limited. this paper introduces some conceptual and operative diagrams that aim to aid in the visualization of various stages of the design derivation and development processes. the structure underlying the proposed diagrams represents a basis for a diagrammatic framework for architectural designing that may help systematically induce form development, organize design sequence and externalize concept generative forces. the purpose of this framework is not to dictate a specific sequence of design, but rather to help designers process their designs in ways more structured than trial/error-based ones. the scope of the proposed framework includes all phases of designing that include the input, output, presentation and assessment activities, with a focus on the core design processing and form-making activities. to test the applicability of this framework, it was implemented to solve a real design problem in an architectural design studio. the feedback of the diagrammatic framework users was analyzed by a survey about its implementation in comparison to conventional design approaches. the results of this empirical study are reported and discussed. the results appear to support the role of diagrams in the enhancement of designing and learning processes. however, this contribution of diagrams to designing and learning needs further applications in various design settings to assess its limitations and strengths.
00f7c49a-6ae5-4e07-b07f-cc4ce7de24fe information technology implementation for a distributed data system serving earth scientists: seasonal to interannual esip we address the implementation of a distributed data system designed to serve earth system scientists. a consortium led by george mason university has been funded by nasa's working prototype earth science information partner (wp-esip) program to develop, implement, and operate a distributed data and information system. the system will address the research needs of seasonal to interannual scientists whose research focus includes phenomena such as el nino, monsoons and associated climate studies. the system implementation involves several institutions using a multitiered client-server architecture. specifically the consortium involves an information system of three physical sites, gmu, the center for ocean-land-atmosphere studies (cola) and the goddard distributed active archive center, distributing tasks in the areas of user services, access to data, archiving, and other aspects enabled by a low-cost, scalable information technology implementation. the project can serve as a model for a larger wp-esip federation to assist in the overall data information system associated with future large earth observing system data sets and their distribution. the consortium has developed innovative information technology techniques such as content based browsing, data mining and associated component working prototypes; analysis tools particularly grads developed by cola, the preferred analysis tool of the working seasonal to interannual communities; and a java front-end query engine working prototype.
00f83a6e-ce85-4aeb-a422-1ee02ce6779b an efficient comparative machine learning-based metagenomics binning technique via using random forest metagenomics is the study of microorganisms collected directly from natural environments. metagenomics studies use dna fragments obtained directly from a natural environment using whole genome shotgun (wgs) sequencing. sequencing random fragments obtained from whole genome shotgun into taxa-based groups is known as binning. currently, there are two different methods of binning: sequence similarity methods and sequence composition methods. sequence similarity methods are usually based on sequence alignment to known genome like blast, or megan. as only a very small fraction of species is available in the current databases, similarity methods do not yield good results. as a given database of organisms grows, the complexity of the search will also grow. sequence composition methods are based on compositional features of a given dna sequence like k-mers, or other genomic signature(s). most of these current methods for binning have two major issues: they do not work well with short sequences and closely related genomes. in this paper we propose new machine learning related predictive dna sequence feature selection algorithms to solve binning problems in more accurate and efficient ways. in this work we use oligonucleotide frequencies from 2-mers to 4-mers as features to differentiate between sequences. 2-mers produces 16 features, 3-mers produces 64 features and 4-mers produces 256 features. we did not use feature higher than 4-mers as the number of feature increases exponentially and for 5-mers the number of feature would be 1024 features. we found out that the 4-mers produces better results than 2-mers and 3-mers. the data used in this work has an average length of 250, 500, 1000, and 2000 base pairs. experimental results of the proposed algorithms are presented to show the potential value of the proposed methods. the proposed algorithm accuracy is tested on a variety of data sets and the classification/prediction accuracy achieved is between 78% - 99% for various simulated data sets using random forest classifier and 37% - 95% using naive bayes classifier. random forest classifier did better in classification in all the dataset compared to naive bayes.
00f8f21b-e067-46d1-a431-d3869417cac2 data mining: emerging trends, challenges and applications 
00f96cbb-eb75-48eb-823d-344f3caf74f6 multilabel image classification via high-order label correlation driven active learning supervised machine learning techniques have been applied to multilabel image classification problems with tremendous success. despite disparate learning mechanisms, their performances heavily rely on the quality of training images. however, the acquisition of training images requires significant efforts from human annotators. this hinders the applications of supervised learning techniques to large scale problems. in this paper, we propose a high-order label correlation driven active learning (hoal) approach that allows the iterative learning algorithm itself to select the informative example-label pairs from which it learns so as to learn an accurate classifier with less annotation efforts. four crucial issues are considered by the proposed hoal: 1) unlike binary cases, the selection granularity for multilabel active learning need to be fined from example to example-label pair; 2) different labels are seldom independent, and label correlations provide critical information for efficient learning; 3) in addition to pair-wise label correlations, high-order label correlations are also informative for multilabel active learning; and 4) since the number of label combinations increases exponentially with respect to the number of labels, an efficient mining method is required to discover informative label correlations. the proposed approach is tested on public data sets, and the empirical results demonstrate its effectiveness.
00f9af78-717e-44e6-9375-909ad7fad03c face recognition by dividing an image and evaluating a similarity vector with a support vector machine a face recognition and apparatus are provided. according to the method, an svm classifier is created through machine learning on the basis of a degree of similarity of a divided facial image, and a facial image to be authenticated is normalized to a predetermined size using a center between two eyes. the normalized image is divided into more than one image in horizontal and vertical directions, respectively. next, predetermined characteristic vectors from the divided images are extracted and a similarity vector based on a degree of similarity with respect to a registered characteristic vector is created. the similarity vector is input to the svm classifier, so that authentication is performed.
00f9e3f0-7c31-41a1-b873-eb2152234995 integrated model for projectlevel management of flexible pavements pavement management systems usually focus on lifecycle cost as the most important criterion for selecting the optimal pavement strategy at the project level. pavement management decisions, however, are made in the context of multiple and often conflicting objectives of users, operators, and highway agencies. this paper presents an integrated model for projectlevel pavement management, which consists of a life cycle cost model and a costeffectiveness method. the life cycle cost model generates the feasible pavement strategies for a given highway segment and provides the design, performance characteristics, and various costs for each. the costeffectiveness method performs an evaluation, which leads to the identification of the optimal strategies. analysis results of the proposed approach indicate that when the evaluation of pavement strategies explicitly recognizes the multiple types of costs as well as measures of pavement performance, life cycle cost affects the ranking of the strategies, but the optim...
00fa6225-aeb0-4e62-97e5-a2dca969c567 control law reconfiguration for non linear systems based on multilayer neural network and fuzzy model: application to a thermal plant in this paper, a reconfiguration approach using fuzzy logic algorithm and neural network modelling is proposed. when failure has been detected, the state of the degraded system is evaluated by comparing the output of the system with the estimation provided by a neural model. therefore, we propose to use the neural network in order to cover all the operating zone of the faulty system. by combining neural network capabilities and fuzzy logic for fault evaluation, a new control law is determined taking into account the impact of the failure on the system. its potentialities are illustrated through simulation studies on a thermal plant presenting bilinear characteristics. >
00fa95d6-5630-40fe-b488-9cb82f1d7f44 chapter 10  engineering principles in biomedical informatics engineering is one of the main pillars of biomedical informatics, providing design principles, methods, and tools for the effective implementation of computational solutions in health care. the basic engineering approach consists of a number of phases, comprising modeling, designing, testing, and verifying. such an approach has become widely applied in biomedical informatics. in this chapter, we analyze three different engineering approaches crucial for biomedical informatics: (1) the design of computational solutions that use the unified modeling language (uml); (2) the representation, simulation, and learning of careflow systems; and, finally, (3) the role that engineering has in data mining, with a specific focus on temporal data and dynamical systems, as well as on principles for engineering the data analysis process.
00fb41b5-3c47-43ce-87f6-9af82e053568 heart-trend: an affordable heart condition monitoring system exploiting morphological pattern in this paper we leverage the power of smartphone to enable proactive in-house heart condition monitoring. we introduce heart-trend, a nonparametric model to analyze and detect heart abnormality conditions like arrhythmia from photoplethysmogram (ppg) signal. it does on-demand heart status monitoring using smartphones (can also be implemented in pc/icu monitors) and facilitates timely detection of heart condition deterioration to permit early diagnosis and prevention of fatal heart diseases. proposed robust anomaly analytics engine accurately detects the morphological trend to find abnormal heart condition in real time through machine learning based trend prediction. ppg signal is frequently corrupted by ambient noise, and motion artifacts, which lead to high amount of false alarms. we introduce precise denoising technique that identifies and eliminates the corrupted segments of clinical signal to minimize its impact on the decision process and analytics. we demonstrate that heart-trend ensures high detection capability with lower false alarm rates.
00fbe3f1-eadc-43a3-a5e1-39eb9a1289cb classification ensemble to improve medical named entity recognition an accurate named entity recognition (ner) is important for knowledge discovery in text mining. this paper proposes an ensemble machine learning approach to recognise named entities (nes) from unstructured and informal medical text. specifically, conditional random field (crf) and maximum entropy (me) classifiers are applied individually to the test data set from the i2b2 2010 medication challenge. each classifier is trained using a different set of features. the first set focuses on the contextual features of the data, while the second concentrates on the linguistic features of each word. the results of the two classifiers are then combined. the proposed approach achieves an f-score of 81.8%, showing a considerable improvement over the results from crf and me classifiers individually which achieve f-scores of 76% and 66.3% for the same data set, respectively.
00fc38b8-4d8f-48a2-8411-d9ee83eb0dc7 extraction of semantic biomedical relations from medline abstracts using machine learning approach 
00fc5e24-ad3f-43c0-a131-4ec7ff1389a0 optimization elm based on rough set for predicting the label of military simulation data by combining rough set theory with optimization extreme learning machine (oelm), a new hybrid machine learning technique is introduced for military simulation data classification in this study. first, multivariate discretization method is implemented to convert continuous military simulation data into discrete data. then, rough set theory is employed to generate the simple rules and to remove irrelevant and redundant variables. finally, oelm is compared with classical extreme learning machine (elm) and support vector machine (svm) to evaluate the performance of both original and reduced military simulation datasets. experimental results demonstrate that, with the help of rs strategy, oelm can significantly improve the testing rate of military simulation data. additionally, oelm is less sensitive to model parameters and can be modeled easily.
00fc8fcd-9b4e-4479-ac35-0af78b0132cf a fuzzy approach to greenhouse climate control the methodology proposed in the paper deals with the use of artificial intelligence techniques in the modelling and control of climate variables within a greenhouse. the nonlinear physical phenomena governing the dynamics of temperature and humidity on such systems are, in fact, difficult to be modelled and controlled using traditional techniques. the paper proposes a framework for the development of fuzzy logic controllers in modern greenhouses.
00fc9483-c1f6-40cc-9f28-64a3b94acf28 knowledge discovery from web usage data: complete preprocessing methodology summary the exponential growth of the web in terms of web sites and their users during the last decade has generated huge amount of data related to the users interactions with the web sites. this data is recorded in the web access log files of web servers and usually referred as web usage data (wud). knowledge discovery from web usage data (kdwud) is that area of web mining deals with the application of data mining techniques to extract interesting knowledge from the wud. as web sites continue to grow in size and complexity, the results of kdwud have become very critical for efficient and effective management of the activities related to: e-business, eeducation, e-commerce, personalization, website design & management, network traffic analysis, the cache, the proxies, great diversity of web pages in a site, search engines complexity, and to predict users actions. in this paper, we propose a complete preprocessing methodology, one of the important steps in kdwud process. several heuristics have been proposed for cleaning the wud which is then aggregated and recorded in the relational data model. to validate the efficiency of the proposed preprocessing methodology, several experiments were conducted and the results shows that the proposed methodology reduces the size of web access log files down to 73-82% of the initial size and offer richer logs that are structured for further stages of kdwud.
00fcf226-6150-4121-bd2a-ee7310d55bed mining marketing maps for business alliances a business can strengthen its competitive advantage and increase its market share by forming a strategic alliance. with the help of alliances, businesses can bring to bear significant resources beyond the capabilities of the individual co-operating firms. thus how to effectively evaluate and select alliance partners is an important task for businesses because a successful corporation partner selection can therefore reduce the possible risk and avoid failure results on business alliance. this paper proposes the apriori algorithm as a methodology of association rules for data mining, which is implemented for mining marketing map knowledge from customers. knowledge extraction from marketing maps is illustrated as knowledge patterns and rules in order to propose suggestions for business alliances and possible co-operation solutions. finally, this study suggests that integration of different research factors, variables, theories, and methods for investigating this research topic of business alliance could improve research results and scope.
00fd1fa7-294c-42a9-9d60-368286c39760 meaning and mining: the impact of implicit assumptions in data mining for the humanities as the use of data mining and machine learning methods in the humanities becomes more common, it will be increasingly important to examine implicit biases, assumptions, and limitations these methods bring with them. this article makes explicit some of the foundational assumptions of machine learning methods, and presents a series of experiments as a case study and object lesson in the potential pitfalls in the use of data mining methods for hypothesis testing in literary scholarship. the worst dangers may lie in the humanist's ability to interpret nearly any result, projecting his or her own biases into the outcome of an experiment-perhaps all the more unwittingly due to the superficial objectivity of computational methods. we argue that in the digital humanities, the standards for the initial production of evidence should be even more rigorous than in the empirical sciences because of the subjective nature of the work that follows. thus, we conclude with a discussion of recommended best practices for making results from data mining in the humanities domain as meaningful as possible. these include methods for keeping the the boundary between computational results and subsequent interpretation as clearly delineated as possible.
00fe0e27-befe-4624-99cd-cd5a2a5fa20b use of object-oriented intelligent simulation tools for web based education this study explains the development of a web based computer aided intelligent education tool. the tool is designed to enhance the knowledge and decision making capabilities of students studying engineering related environmental issues. the system enables freshmen engineering students to enhance critical thinking ability. the power of interactive simulation, animation, artificial intelligence and object-oriented technologies are incorporated to guide the students in the decision-making processes of real-life case studies. the system developed combines theory, experiments, and software tools and can be used in classroom situations or for self/group-learning. the use of intelligent simulation modules is aimed at introductory level environmental chemistry. the currently available three modules are environmentally safe paint preparation process, automobile pollution and fundamentals of gas laws. the teaching strategy of each section of courseware is centered around a text that introduces the fundamentals, theory and links to simulated laboratories. the text also contains hyperlinks to other text pictures, audio and video resources, and interactive models.
00fee03f-9c7c-4a41-b819-7545e8448967 schubot: machine learning tools for the automated analysis of schuberts lieder 
010089f4-b40d-4a89-9417-734c66bbd032 kernel-based feature extraction with a speech technology application kernel-based nonlinear feature extraction and classification algorithms are a popular new research direction in machine learning. this paper examines their applicability to the classification of phonemes in a phonological awareness drilling software package. we first give a concise overview of the nonlinear feature extraction methods such as kernel principal component analysis (kpca), kernel independent component analysis (kica), kernel linear discriminant analysis (klda), and kernel springy discriminant analysis (ksda). the overview deals with all the methods in a unified framework, regardless of whether they are unsupervised or supervised. the effect of the transformations on a subsequent classification is tested in combination with learning algorithms such as gaussian mixture modeling (gmm), artificial neural nets (ann), projection pursuit learning (ppl), decision tree-based classification (c4.5), and support vector machines (svms). we found, in most cases, that the transformations have a beneficial effect on the classification performance. furthermore, the nonlinear supervised algorithms yielded the best results.
01009aed-eda5-4b41-b245-d69a241b6834 comparison of network pruning and tree pruning on artificial neural network tree artificial neural network (ann) has not been effectively utilized in data mining because of its "black box" nature. this issue was resolved by using the artificial neural network tree (annt) approach in the authors' earlier works. the annt approach derives symbolic knowledge (rules) to provide some explanation of how the classification or prediction of the ann is obtained. to enhance extraction, pruning will be incorporate with this approach where two pruning techniques are evaluated to see which method is best to use with annt. the first technique is to prune the neural network and the second technique is to prune the tree. the first technique analytically measures the amount of information gained by each of the ann links as a result learning (training). the one with the lowest information is pruned. rules are extracted from the pruned network. the second pruning will prune the tree that is built from the neural network and then the tree is converted to rules. these two techniques are evaluated with the annt algorithm in the insurance domain to see which method of pruning is most suitable with annt in terms of comprehensibility and accuracy.
0102345c-409e-4bea-be1e-bad71f04cf26 music-expected maximization gaussian mixture methodology for clustering and detection of task-related neuronal firing rates. researchers often rely on simple methods to identify involvement of neurons in a particular motor task. the historical approach has been to inspect large groups of neurons and subjectively separate neurons into groups based on the expertise of the investigator. in cases where neuron populations are small it is reasonable to inspect these neuronal recordings and their firing rates carefully to avoid data omissions. in this paper, a new methodology is presented for automatic objective classification of neurons recorded in association with behavioral tasks into groups. by identifying characteristics of neurons in a particular group, the investigator can then identify functional classes of neurons based on their relationship to the task. the methodology is based on integration of a multiple signal classification (music) algorithm to extract relevant features from the firing rate and an expectation-maximization gaussian mixture algorithm (em-gmm) to cluster the extracted features. the methodology is capable of identifying and clustering similar firing rate profiles automatically based on specific signal features. an empirical wavelet transform (ewt) was used to validate the features found in the music pseudospectrum and the resulting signal features captured by the methodology. additionally, this methodology was used to inspect behavioral elements of neurons to physiologically validate the model. this methodology was tested using a set of data collected from awake behaving non-human primates.
01025ddb-5bb1-4254-b0d5-1404fc0169b2 bray-curtis ordination: an effective strategy for analysis of multivariate ecological data publisher summary   ordination implies an abstract space in which the entities form a constellation. in the braycurtis ordination, the entities are samples and the attributes are species values in those samples. the aim of this method is to (1) calculate a distance matrix, (2) select two reference points (either real or synthetic samples) for determining direction of each axis, and (3) project all samples onto each such axis by their relationship to the two reference points. there are two major problems common to all ordination techniques, which include a function of the -diversity or heterogeneity of the data setthat is, how different the samples are from one another. all ordinations distort the original multivariate data set and information is inevitably lost. distortion in ordination has two kinds of consequences. the first is compressing and stretching distances in the ordination, compared with the original distance measures and relative to one another. the second consequence is the curvature of environmental axes, and this relates to orloci's types a and c. some of the alternatives to braycurtis ordination are principal component analysis, reciprocal averaging, and iterative-stress minimization techniques.
0102cafd-9023-4bb4-875e-f6bda0e74a06 on producing join results early support for exploratory interaction with databases in applications such as data mining requires that the first few results of an operation be available as quickly as possible. we study the algorithmic side of what can and what cannot be achieved for processing join operations. we develop strategies that modify the strict two-phase processing of the sort-merge paradigm, intermingling join steps with selected merge phases of the sort. we propose an algorithm that produces early join results for a broad class of join problems, including many not addressed well by hash-based algorithms. our algorithm has no significant increase in the number of i/o operations needed to complete the join compared to standard sort-merge algorithms.
0102fe69-cef7-4f53-b16c-2837e698b24c intelligent driver assist system for urban driving driving in an urban environment is hectic and often adventurous. getting accurate routing instructions, finding parking spots, receiving customized information that helps individual drivers reach their destination will significantly reduce the stress of driving, save fuel and reduce unnecessary delays and pollution levels. in this paper we present a system that combines smart navigation with intelligent parking assist and driver diagnostics to considerably improve driving comfort, safety and mobility in an urban environment. the smart navigation employs an on line traffic simulator which provides traffic predictions and improves the accuracy of existing navigation systems which rely on limited traffic data. the intelligent parking assist system predicts the availability of parking at the start of the journey and these predictions get updated as the destination is approached. the system uses machine learning to understand the habits and preferences of the individual driver so that the preferred parking availability information is presented to the driver. the driver diagnostics part learns the driving characteristics of the driver i.e. whether aggressive, semi aggressive or passive, reaction times, following distances etc. and provides this information to the smart navigation and parking assist system for better estimation of travel times. in addition, it can be used to support collision warnings and other driver assist devices. the proposed system has been successfully demonstrated using an audi vehicle in the area of los angeles and san francisco.
010358fe-0a85-449e-8812-0ae154c0ad51 a web-based tool for collaborative social media data analysis user-generated content on social media sites such as twitter and facebook provides opportunity for researchers in various fields to understand human behaviors and social phenomena. on the one hand, these human behaviors and social phenomena are very complex in nature thus require in-depth qualitative analysis. on the other, the magnitude of social media data requires large-scale data analysis techniques. in this paper, we propose a web-based tool named swab (social web analysis buddy) that integrates both qualitative analysis and large-scale data mining techniques. specifically, this tool supports asynchronous collaboration among researchers conducting inductive content analysis on textural data from users' online posts and conversations. it then aggregates the results and calculates the agreement among researchers, and builds modeling algorithms based on the qualitative results to classify large-scale social media text content. this current paper focuses on the overall workflow and user interface design of this tool. we demonstrate the prototype of this tool by analyzing student-posted content on twitter.
01037ee8-8ef9-4d29-ac8f-ee85b119aafc the appropriate use of approximate entropy and sample entropy with short data sets approximate entropy (apen) and sample entropy (sampen) are mathematical algorithms created to measure the repeatability or predictability within a time series. both algorithms are extremely sensitive to their input parameters: m (length of the data segment being compared), r (similarity criterion), and n (length of data). there is no established consensus on parameter selection in short data sets, especially for biological data. therefore, the purpose of this research was to examine the robustness of these two entropy algorithms by exploring the effect of changing parameter values on short data sets. data with known theoretical entropy qualities as well as experimental data from both healthy young and older adults was utilized. our results demonstrate that both apen and sampen are extremely sensitive to parameter choices, especially for very short data sets, n  200. we suggest using n larger than 200, an m of 2 and examine several r values before selecting your parameters. extreme caution should be used when choosing parameters for experimental studies with both algorithms. based on our current findings, it appears that sampen is more reliable for short data sets. sampen was less sensitive to changes in data length and demonstrated fewer problems with relative consistency.
01039ddd-8a3e-4fd4-8c79-e978da6ac90f determination of vocational fields with machine learning algorithm the importance of vocational and technical training is growing day by day in parallel to the developing technology. it is inevitable to utilise opportunities presented by information and communication technologies in order to determine vocational fields in vocational and technical training in the most efficient manner. in this respect, it is possible to create a more efficient tool compared to the current methods by utilising machine learning which is an artificial intelligence model in energy applications that predicts events in the future depending on the past experiences. in the current study, a software is developed that ensures that the system learns about the successful and unsuccessful choices made in the past by applying naive bayes algorithm, which is a machine learning algorithm, to the data collected concerning the individuals who turned out to be successful or unsuccessful in the vocational technical training process in energy applications. in the software developed, it is aimed that the system recommends the most suitable vocational field for the individual by according to the data collected from the individual who is in the occupation selection process in field energy applications.
0103d101-cc80-4026-acee-eb91a43169cc inferring the location of buried uxo using a support vector machine the identification of unexploded ordnance (uxo) using electromagnetic-induction (emi) sensors involves two essentially independent steps: each anomaly detected by the sensor has to be located fairly accurately, and its orientation determined, before one can try to find size/shape/composition properties that identify the object uniquely. the dependence on the latter parameters is linear, and can be solved for efficiently using for example the normalized surface magnetic charge model. the location and orientation, on the other hand, have a nonlinear effect on the measurable scattered field, making their determination much more time-consuming and thus hampering the ability to carry out discrimination in real time. in particular, it is difficult to resolve for depth when one has measurements taken at only one instrument elevation. in view of the difficulties posed by direct inversion, we propose using a support vector machine (svm) to infer the location and orientation of buried uxo. svms are a method of supervised machine learning: the user can train a computer program by feeding it features of representative examples, and the machine, in turn, can generalize this information by finding underlying patterns and using them to classify or regress unseen instances. in this work we train an svm using measured-field information, for both synthetic and experimental data, and evaluate its ability to predict the location of different buried objects to reasonable accuracy. we explore various combinations of input data and learning parameters in search of an optimal predictive configuration.
01044c9b-f186-42c6-b31e-5c3c2ae04056 spatial bayesian surprise for image saliency and quality assessment we propose an alternative interpretation of bayesian surprise in the spatial domain, to account for saliency arising from contrast in image context. our saliency formulation is integrated in three different application scenaria, with considerable improvements in performance: 1) visual attention prediction, validated using eye- and mouse-tracking data, 2) region of interest detection, to improve scale selection and localization, 3) image quality assessment to achieve better agreement with subjective human evaluations.
01046d08-0d52-4575-9825-c59218d1f498 image learning classifier system using genetic algorithms the authors examine aspects of machine learning by classifier systems that use genetic algorithms. in particular, adaptive image learning and classification are considered. standard classifier systems are not well suited for seeking out multiple goals as is necessary in image learning and classification problems. to improve the performance of standard classifier systems for the image learning task, several modifications are suggested. the modifications result in a far better performance for classifier system on the imagelearn domain. >
0104f3ff-fbac-4476-aaaf-d42ea48224b6 lightweight adaptive random-forest for iot rule generation and execution the area of the internet of things is growing rapidly. the volume of transmitted data over the various sensors is growing accordingly. sensors typically are low in resources of storage, memory and processing power. data security and privacy are part of the major concerns and drawbacks of this growing domain. sensor traffic analysis has become an increasingly important domain to protect iot infrastructures from intruders. an iot network intrusion detection system is required to monitor and analyze the traffic and predict possible attacks. machine leaning techniques can automatically extract normal and abnormal patterns from a large set of training sensors data. due to the high volume of traffic and the need for real-time reaction, accurate threat discovery is mandatory. this work focuses on designing a lightweight comprehensive iot rules generation and execution framework. it is composed of three components, a machine learning rule discovery, a threat prediction model builder and tools to ensure timely reaction to rules violation and un-standardized and ongoing changes in traffic behavior. the generated detection model is expected to identify in real-time exceptions and notify the system accordingly. we use random-forest (rf) as the machine learning platform for rules discovery and real-time anomaly detection. to allow rf adaptation to iot we propose several improvements to make it lightweight and propose a process that combines iot network capabilities; messaging and resource sharing, to build a comprehensive and efficient iot security framework.
01053090-0c8c-46f8-a110-df07dddc58c6 knowledge discovery in databases using lattices the rapid pace at which data gathering, storage and distribution technologies are developing is outpacing our advances in techniques for helping humans to analyse, understand, and digest the vast amounts of resulting data. this has led to the birth of knowledge discovery in databases (kdd) and data mininga process that has the goal to selectively extract knowledge from data. a range of techniques, including neural networks, rule-based systems, case-based reasoning, machine learning, statistics, etc. can be applied to the problem. we discuss the use of concept lattices, to determine dependences in the data mining process. we first define concept lattices, after which we show how they represent knowledge and how they are formed from raw data. finally, we show how the lattice-based technique addresses different processes in kdd, especially visualization and navigation of discovered knowledge.
01058e5d-1e91-4bfe-803e-17f2b645ce02 modeling household car ownership by decision rules this paper aims to develop a decision rule-based model of household car ownership by using household travel survey data. the rough set approach as one of data mining techniques provides an effective tool to model the presented data set with decision rule sets. in this paper, the outputted if-then rules show the significant relationships between car ownership type and household characteristics. the performance of three different rule sets is also compared.
0105efd8-34ea-4930-a085-3d2d638cc809 uncertainty management in optimal disassembly planning through learning-based strategies currently there is increasing consensus that one of the main issues differentiating remanufacturing from more traditional manufacturing processes is the need to effectively model and manage the high levels of uncertainty inherent in these new processes. hence, the work presented in this paper concerns the issue of uncertainty modeling and management as it arises in the context of the optimal disassembly planning problem, one of the key problems to be addressed by remanufacturing processes. more specifically, the presented results formally establish that the theory of reinforcement learning, currently one of the most actively researched paradigms in the area of machine learning, constitutes a rigorous, efficient, and effectively implementable modeling framework for providing (near-)optimal solutions to the optimal disassembly problem, in the face of the aforementioned uncertainties. in addition, the proposed approach is exemplified and elucidated by application on a case study borrowed from the relevant li...
01062f5c-056a-4a32-af8f-43359a2e59ac advantages of unbiased support vector classifiers for data mining applications many learning algorithms have been used for data mining applications, including support vector classifiers (svc), which have shown improved capabilities with respect to other approaches, since they provide a natural mechanism for implementing structural risk minimization (srm), obtaining machines with good generalization properties. svc leads to the optimal hyperplane (maximal margin) criterion for separable datasets but, in the nonseparable case, the svc minimizes the l1 norm of the training errors plus a regularizing term, to control the machine complexity. the l1 norm is chosen because it allows to solve the minimization with a quadratic programming (qp) scheme, as in the separable case. but the l1 norm is not truly an "error counting" term as the empirical risk minimization (erm) inductive principle indicates, leading therefore to a biased solution. this effect is specially severe in low complexity machines, such as linear classifiers or machines with few nodes (neurons, kernels, basis functions). since one of the main goals in data mining is that of explanation, these reduced architectures are of great interest because they represent the origins of other techniques such as input selection or rule extraction. training svms as accurately as possible in these situations (i.e., without this bias) is, therefore, an interesting goal.#r##n##r##n#we propose here an unbiased implementation of svc by introducing a more appropriate "error counting" term. this way, the number of classification errors is truly minimized, while the maximal margin solution is obtained in the separable case. qp can no longer be used for solving the new minimization problem, and we apply instead an iterated weighted least squares (wls) procedure. this modification in the cost function of the support vector machine to solve erm was not possible up to date given the quadratic or linear programming techniques commonly used, but it is now possible using the iterated wls formulation. computer experiments show that the proposed method is superior to the classical approach in the sense that it truly solves the erm problem.
01065397-8063-499f-9df7-80683c1c8c62 service-oriented organization management and coordination control of mas based on the researches of service-oriented architecture and distributed artificial intelligence, a service-oriented organization management model of mas (multi-agent system) is designed. the central-decentralized management mode and the management strategy and mechanism of mas are expounded. then, the coordination control features and forms of mas are analyzed and the multi-level coordination control and partial global planning strategy is proposed.
01067ed1-a5db-4918-a537-d3779ab59db4 structural determinants for selective recognition of a lys48-linked polyubiquitin chain by a uba domain although functional diversity in polyubiquitin chain signaling has been ascribed to the ability of differently linked chains to bind in a distinctive manner to effector proteins, structural models of such interactions have been lacking. here, we use nmr to unveil the structural basis of selective recognition of lys48-linked di- and tetraubiquitin chains by the uba2 domain of hhr23a. although the interaction of uba2 with lys48-linked diubiquitin involves the same hydrophobic surface on each ubiquitin unit as that utilized in monoubiquitin:uba complexes, our results show how the "closed" conformation of lys48-linked diubiquitin is crucial for high-affinity binding. moreover, recognition of lys48-linked diubiquitin involves a unique epitope on uba, which allows the formation of a sandwich-like diubiqutin:uba complex. studies of the uba-tetraubiquitin interaction suggest that this mode of uba binding to diubiquitin is relevant for longer chains.
0106a8e1-bef3-41a4-bcfa-542d73213479 mapping soil organic carbon content over new south wales, australia using local regression kriging various digital soil mapping techniques ranging from simple linear models to complex machine learning techniques have been employed for soil organic carbon (soc) mapping. when soc mapping over a large region is required, the usual approach has to employ a model calibrated for the whole area. an alternative is to use a series of locally calibrated models to map smaller areas that collectively cover the large region of interest. the accuracy of the soc products generated by these two approaches can potentially vary. however, performance of whole-area calibrated models versus locally calibrated models in mapping soc of large extents has seldom been explored in detail, particularly with respect to the type of model being employed. our study aims to fill this gap by evaluating the soc prediction performance of three common models, multiple linear regression (mlr), regression tree model; cubist and support vector regression (svr) that are calibrated locally and for the whole study area.#r##n##r##n#this study was carried out using eight identified local areas in new south wales (nsw), australia and across the whole state entirely. every model was calibrated separately for each local area and for the entire state. the local and whole-area models were validated using the same test data set over 50 realizations. in particular, local prediction accuracy of whole-area calibrated models was compared to that of locally calibrated models. the models were tested separately for the standard soil depth layers including 05 cm, 515 cm, 1530 cm, 3060 cm; 60100 cm. the results show that svr models have a superior performance out of three tested models for all standardized depth layers. in general the local models outperform the whole-area models for all three tested models with respect to the accuracy of predictions. all models displayed area specific performances proving the importance of inclusion of prevailing local conditions in soc modelling and mapping. therefore, we introduce a moving window approach where a hybrid series of locally calibrated models and a whole-area calibrated model can be used against using one calibrated model for the modelling very large mapping extents. moving window approach provides more accurate results having the lowest error compared to the whole-area model. also it provides the least biased predictions. therefore, this novel approach provides a promising way of increasing the efficiency and accuracy of digital soil mapping.
0106ba7f-7aad-4458-b913-340a307e836b combining rules and machine learning for extraction of temporal expressions and events from clinical narratives objective identification of clinical events (eg, problems, tests, treatments) and associated temporal expressions (eg, dates and times) are key tasks in extracting and managing data from electronic health records. as part of the i2b2 2012 natural language processing for clinical data challenge, we developed and evaluated a system to automatically extract temporal expressions and events from clinical narratives. the extracted temporal expressions were additionally normalized by assigning type, value, and modifier.#n##n#materials and methods the system combines rule-based and machine learning approaches that rely on morphological, lexical, syntactic, semantic, and domain-specific features. rule-based components were designed to handle the recognition and normalization of temporal expressions, while conditional random fields models were trained for event and temporal recognition.#n##n#results the system achieved micro f scores of 90% for the extraction of temporal expressions and 87% for clinical event extraction. the normalization component for temporal expressions achieved accuracies of 84.73% (expression's type ), 70.44% ( value ), and 82.75% ( modifier ).#n##n#discussion compared to the initial agreement between human annotators (8789%), the system provided comparable performance for both event and temporal expression mining. while (lenient) identification of such mentions is achievable, finding the exact boundaries proved challenging.#n##n#conclusions the system provides a state-of-the-art method that can be used to support automated identification of mentions of clinical events and temporal expressions in narratives either to support the manual review process or as a part of a large-scale processing of electronic health databases.
0106cb1a-5e07-463f-ab49-8dc3c4fe61cc a new method for traffic forecasting based on the data mining technology with artificial intelligent algorithms this study aims to investigate the traffic information forecasting based on the data mining technology. as well known, useful knowledge in traffic management system often hides in a large amount of traffic data. generally, prior data pattern labels have been used to train the artificial neural network (ann) to identify the traffic conditions in the traffic information forecasting. the performance of the ann models suffers from the prior information of the experts. to relieve this impact in the traffic information forecasting, a new ann model is proposed based on the data mining technology in this study. the self-organized feature map (sofm) is firstly employed to cluster the traffic data through an unsupervised learning and provide the labels for these data. then the labeled data were used to train the ga-chaos optimized rbf neural network. herein, the ga-chaos algorithm is used to train the rbf parameters. experimental tests use practical data sets from the intelligent transportation systems (its) were implemented to validate the performance of the proposed ann model. the analyses results demonstrate that the proposed method can extract the potential patterns hidden in the traffic data and can accurately predict the future traffic state. the prediction accuracy is beyond 95%. hence, the new data mining model can provide practical application for traffic information forecasting in the its system.
01070988-4b21-4d45-ba41-e96da3044079 using artificial intelligence methods to design new conducting polymers in the last years the possibility of creating new conducting polymers exploring the concept of copolymerization (different structural monomeric units) has attracted much attention from experimental and theoretical points of view. due to the rich carbon reactivity an almost infinite number of new structures is possible and the procedure of trial and error has been the rule. in this work we have used a methodology able of generating new structures with pre-specified properties. it combines the use of negative factor counting (nfc) technique with artificial intelligence methods (genetic algorithms - gas). we present the results for a case study for poly(phenylenesulfide phenyleneamine) (ppsa), a copolymer formed by combination of homopolymers: polyaniline (pani) and polyphenylenesulfide (pps). the methodology was successfully applied to the problem of obtaining binary up to quinternary disordered polymeric alloys with a pre-specific gap value or exhibiting metallic properties. it is completely general and can be in principle adapted to the design of new classes of materials with pre-specified properties.
01071d5d-1d8e-4213-8402-64052985af04 application on web mining for web usability analysis with the rapid development of internet, web data mining, especially weblog mining, plays a more and more important role in many fields, including personalized information services, improving designs and services of web sites and so on. this paper introduces web data mining firstly, and then discusses the process of web log mining. based on these studies and a case study, a web log mining tool is presented in detail. the conclusion of the research and the direction of further study are pointed out in the last part of this paper.
010793c8-fedb-49ee-88bc-1e20f8bae870 birch: an efficient data clustering method for very large databases finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of  clusters,  or densely populated regions, in a multi-dimensional dataset. prior work does not adequately address the problem of large datasets and minimization of i/o costs.this paper presents a data clustering method named  birch  (balanced iterative reducing and clustering using hierarchies), and demonstrates that it is especially suitable for very large databases.  birch  incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).  birch  can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans.  birch  is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.we evaluate  birch 's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. we also present a performance comparisons of  birch  versus  clarans,  a clustering method proposed recently for large datasets, and show that  birch  is consistently superior.
0107a797-ee37-45a7-a4c2-fa334273b489 extracting certainty from uncertainty: regret bounded by variation in costs prediction from expert advice is a fundamental problem in machine learning. a major pillar of the field is the existence of learning algorithms whose average loss approaches that of the best expert in hindsight (in other words, whose average regret approaches zero). traditionally the regret of online algorithms was bounded in terms of the number of prediction rounds.#r##n##r##n#cesa-bianchi, mansour and stoltz (mach. learn. 66(2---3):21---352, 2007) posed the question whether it is be possible to bound the regret of an online algorithm by the variation of the observed costs. in this paper we resolve this question, and prove such bounds in the fully adversarial setting, in two important online learning scenarios: prediction from expert advice, and online linear optimization.
01081fe5-e787-49ef-ab9a-5f9a4f60ff11 exploiting financial news and social media opinions for stock market analysis using mcmc bayesian inference stock market analysis by using information and communication technology methods is a dynamic and volatile domain. over the past years, there has been an increasing focus on the development of modeling tools, especially when the expected outcomes appear to yield significant profits to the investors' portfolios. in alignment with modern globalized economy, the available resources are becoming gradually more plentiful, thus difficult to be analyzed by standard statistical tools. thus far, there have been a number of research papers that emphasize solely in past data from stock bond prices and other technical indicators. nevertheless, throughout recent studies, prediction is also based on textual information, based on the logical assumption that the course of a stock price can also be affected by news articles and perhaps by public opinions, as posted on various web 2.0 platforms. despite the recent advances in natural language processing and data mining, when data tend to grow both in number of records and attributes, numerous mining algorithms face significant difficulties, resulting in poor forecast ability. the aim of this study is to propose a potential answer to the problem, by considering a markov chain monte carlo bayesian inference approach, which estimates conditional probability distributions in structures obtained from a tree-augmented naive bayes algorithm. the novelty of this study is based on the fact that technical analysis contains the event and not the cause of the change, while textual data may interpret that cause. the paper takes into account a large number of technical indices, accompanied with features that are extracted by a text mining methodology, from financial news articles and opinions posted in different social media platforms. previous research has demonstrated that due to the high-dimensionality and sparseness of such data, the majority of widespread data mining algorithms suffer from either convergence or accuracy problems. results acquired from the experimental phase, including a virtual trading experiment, are promising. certainly, as it is tedious for a human investor to read all daily news concerning a company and other financial information, a prediction system that could analyze such textual resources and find relations with price movement at future time frames is valuable.
0108645a-a3dc-4b8d-b64c-116319a28034 scalable feature mining for sequential data many real world data sets contain irrelevant or redundant attributes. this might be because the data was collected without data mining in mind or without a priori knowledge of the attribute dependences. many data mining methods such as classification and clustering degrade prediction accuracy when trained on data sets containing redundant or irrelevant attributes or features. selecting the right feature set not only can improve accuracy but also can reduce the running time of the predictive algorithms and lead to simpler, more understandable models. good feature selection is thus a fundamental data preprocessing step in data mining. to provide good feature selection for sequential domains, we developed featuremine, a scalable feature mining algorithm that combines two powerful data mining paradigms: sequence mining and classification algorithms. tests on three practical domains demonstrate that featuremine can efficiently handle very large data sets with thousands of items and millions of records.
0108670b-01a3-4a98-b835-3cd32d916f82 on the formulation and solution of the convoy routing problem in this work, we will identify important variables that contribute to vehicular movement in an emergency environment. in particular, we formulate and pose the convoy routing problem. we suggest a method for modeling the problem and formulate a precise problem statement that significantly reduces the number of variables under consideration relative to similar previous work; even so, we prove that the decision version of this problem is np-complete. after devising an algorithm using artificial intelligence techniques, we then empirically analyze this model (via software simulation) to get computational results on a single instance of the problem.
0108b461-e91e-4e3c-a29b-11d35ffb1232 supervised representation learning for audio scene classification this paper investigates the use of supervised feature learning approaches for extracting relevant and discriminative features from acoustic scene recordings. owing to the recent release of open datasets for acoustic scene classification (asc) problems, representation learning techniques can now be envisioned for solving the problem of feature extraction. this paper makes a step towards this goal by first studying models based on convolutional neural networks (convnet). because the scale of the datasets available is still small compared to those available in computer vision, we also introduce a technical contribution denoted as supervised non-negative matrix factorization (snmf). our goal through this snmf is to induce the matrix decomposition to carry out discriminative information in addition to the usual generative ones. we achieve this objective by augmenting the nmf optimization problem with a novel loss function related to class labels of acoustic scenes. our experiments show that despite the small-scale setting, supervised feature learning is favorably competitive compared to the current state-of-the-art features. we also point out that for smaller scale dataset, supervised nmf is indeed slightly less prone to overfitting than convolutional neural networks. while the performances of these learned features are interesting per se, a deeper analysis of their behavior in the acoustic scene problem context raises open and difficult questions that we believe, need to be addressed for further performance breakthroughs.
01096647-390e-407f-a5b0-5ee427b3615b a web mining based measurement and monitoring model of urban mass panic in emergency management it is very important to discover the mass panic when city is in emergency. a traditional approach is to do a survey among urban mass, which would cost much time and money. in our opinion, a more suitable and effective approach is to do web-based investigation which includes discovery, measurement, and monitoring. a framework including information retrieval, data mining, and area knowledge analysis is presented. great efforts have been put to the urban mass panic measurement model in terms of over 10 indicators which have covered most relevant web sites, consisting of portal, forums, bbs and other interactive web sites. experimental results are discussed at the end of the article.
010a0b60-08e3-4f70-b669-32d5cb475084 an application of rough sets to monk's problems solving in this paper, the main techniques of inductive machine learning are united to the knowledge reduction theory based on rough sets from the theoretical point of view. and then the monk's problems are resolved again employing rough sets. as far as accuracy and conciseness are concerned, the learning algorithms based on rough sets have remarkable superiority to the previous methods.
010a40d5-3d57-4074-b7f7-6943f2dc3403 a machine learning pipeline for quantitative phenotype prediction from genotype data background#r##n#quantitative phenotypes emerge everywhere in systems biology and biomedicine due to a direct interest for quantitative traits, or to high individual variability that makes hard or impossible to classify samples into distinct categories, often the case with complex common diseases. machine learning approaches to genotype-phenotype mapping may significantly improve genome-wide association studies (gwas) results by explicitly focusing on predictivity and optimal feature selection in a multivariate setting. it is however essential that stringent and well documented data analysis protocols (dap) are used to control sources of variability and ensure reproducibility of results. we present a genome-to-phenotype pipeline of machine learning modules for quantitative phenotype prediction. the pipeline can be applied for the direct use of whole-genome information in functional studies. as a realistic example, the problem of fitting complex phenotypic traits in heterogeneous stock mice from single nucleotide polymorphims (snps) is here considered.
010a416c-b446-4c41-ae26-fe85afd4cbbf retrieval of images using data mining techniques data mining is an emerging research area, because of the generation of large volume of data. the image mining is new branch of data mining, which deals with the analysis of image data. there is several methods for retrieving images from a large dataset. but they have some drawbacks. in this paper using image mining techniques like clustering and associations rules mining for mine the data from image. and also it uses the fusion of multimodal features like visual and textual. this system produces a better precise and recalls values.
010c2016-eae2-45e0-9a08-7c2f15e37094 "from artificial intelligence to human computer interaction - an interview with terry winograd " (born 1946) began his academic career within the field of artificial intelligence. his early research on natural language understanding by computers was a milestone in artificial intelligence. later he moved to the field of human computer interaction, and within this field he has done extensive research and writing on design of human-computer interaction. foremost, focusing on the theoretical background and conceptual models for human-computer interaction design. terry winograd received his b.a. in mathematics from the colorado college in 1966. he studied linguistics at university college, london in 1966-1967, and earned his ph.d. in applied mathematics at massachusetts institute of technology (mit) in 1970. from 1970 he was an instructor and assistant professor of electrical engineering at mit, before coming to stanford university in 1973, where he is now a professor of computer science. at stanford, he directs the project on people, computers, and design, and the teaching and research program on human-computer interaction design. he is one of the principal investigators in the stanford digital libraries project, and the interactive workspaces project. he is also a consultant to interval research corporation, and serves on a number of journal editorial boards, including the journal of human computer interaction, computer supported cooperative work, and personal technologies. prof. winograd is a longtime advocate for socially responsible computing and is a founding member and past national president (1987-1990) of the computer professionals for social responsibility, and is on the national advisory board of the association for software design. his publications include understanding natural language (1972), understanding computers and cognition: a new foundation for design (with fernando flores, 1986)
010c922e-5321-4d34-97ff-0f8bcb384275 biomarker discovery with seldi-tof ms in human urine associated with early renal injury: evaluation with computational analytical tools background. urine proteomics is one of the key emerging technologies to discover new biomarkers for renal disease, which may be used in the early diagnosis, prognosis and treatment of patients. in the present study, we validated surface-enhanced laser desorption/ionization time-of-flight mass spectrometry (seldi-tof ms) for biomarker discovery in patients with mild ischaemic kidney injury. methods. we used first-morning mid-stream urine samples from healthy volunteers, and from intensive care unit patients we collected urine 1224 h after coronary artery bypass graft (cabg) surgery. samples of 50 volunteers were mixed to establish a reference sample (master pool). urine samples were analysed with constant creatinine levels. results. the average intra- and interchip variation was found to be in the normal experimental range (cv of 10 to 30%). computational analysis revealed (i) low intra-individual day-to-day variation in individual healthy volunteers; (ii) high concordance between the master pool sample and individual samples. machine learning techniques for classification of cabg condition vs healthy patients showed that (iii) in the 3-20 kda range, the joint activity of four protein peaks effectively discriminated the two classes, (iv) in the 2070 kda range, a single m/z marker was sufficient to achieve perfect separation. conclusions. our results substantiate the effectiveness of seldi-tof ms-based computational analysis as a tool for discovering potential biomarkers in urine samples associated with early renal injury.
010cc889-2de8-4937-b831-a2228f284212 host intrusion detection for long stealthy system call sequences in this paper, we present sc2, an unsupervised learning classifier for detecting host intrusions from sequences of process system calls. sc2 is a naive bayes-like classifier based on markov model. we describe the classifier, and then provide experimental results on the university of new mexico's four system call trace data sets, namely synthetic sendmail unm, synthetic sendmail cert, live lpr unm and live lpr mit. sc2 classification results are compared to leading machine learning techniques namely, naive bayes multinomial (nbm), c4.5 (decision tree), ripper (rp), support vector machine (svm), and logistic regression (lr). initial findings show that the accuracy of sc2 is comparable to those of leading classifiers, while sc2 has a better detection rate than some of these classifiers on some data sets. sc2 can classify efficiently very long stealthy sequences by using a backtrack, scale and re-multiply technique, together with estimation of standard ieee 754-2008 relative error of floating-point arithmetic for an acceptable classification confidence.
010d698d-cfa1-4495-9e42-21ebbbf8a3a6 an integrated approach to determining the sequence of machining operations for prismatic parts with interacting features this paper proposed a framework for integrating artificial intelligence (ai) techniques and mechanistic models in the task of generating process plans for prismatic parts. the main issue of process planning addressed in this paper is the determination of the operation sequence for mechanical parts containing interacting features where machining of one feature might adversely affect the surface quality of other features. the informed graph-search strategy in ai is used to achieve the function of operation sequence planning. the search graph is built by considering alternative machining operations in converting a blank stock into the final part configuration. several heuristic functions concerning machining practices are developed to help to reduce the complexity of the search graph. mechanistic models of different interaction machining situations are also considered to help the graph-search process so that the process plan generated can ensure the machining of high-quality mechanical parts. this idea of incorporating mechanistic models in operation sequence planning activities makes it possible to build in surface-quality considerations at the early process-planning stage.
010d6ce8-3aa2-41d0-b11b-45eccb09549a knowledge gateways: the building blocks abstract   technological advances over the past two decades have made data retrieval faster and easier, giving rise to a substantial industry providing access to business, professional, and scientific information. some progress has been made towards increasing the relevancy of the data obtained by the information user. despite these activities, information sources remain scattered, hard to find, and difficult to access. using technology, visionary individuals will build  knowledge gateways  capable of leading knowledge-seekers to the needed information, wherever it may be stored. some basic  building blocks , or components, of a knowledge gateway have already been developed. these include multiple system interfaces, access to private and public databases, invisible logon protocols, assistance with database selection, postprocessing, assistance with query formulation, and limited natural language input. some of the technologies useful in building knowledge gateways are artificial intelligence and expert systems, networking, online retrieval systems, optical storage, and natural language processing; they can be linked together to provide a powerful connection between the user and the stores of electronic information throughout the world.  four fledgling gateways are the easynet system, the department of defense gateway information system (dgis), a library access station under development by the library network operated by at&t bell laboratories, and the national materials property data network.
010e8977-1ff2-4c21-b2c6-3d15a6788d09 use of the hough transformation to detect lines and curves in pictures hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. this paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. it also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.
010e929c-a1de-4b91-889b-f2af6089efc1 location accuracy impact on cell outage detection in lte-a networks automated and timely detection of malfunctioning cells in long-term evolution (lte) networks is of high importance. sleeping cell is a particular type of cell degradation hardly detectable by traditional network monitoring systems. recent introduction of minimization of drive test (mdt) functionality enables to collect user-level statistics from regular user devices without expensive and time-consuming drive-test and measurement campaigns. in this study data mining techniques are used to process mdt measurements to detect efficiently a sleeping cell. the developed earlier data mining framework is briefly overviewed in the paper. special attention is devoted to post-processing stage as one of the key elements of the detection scheme. in practice, location information of collected measurements might contain considerable errors. this factor impacts the precision of malfunctioning cell detection. therefore several post-processing algorithms are proposed, where location accuracy is taken into account. the performance of the algorithms is compared based on the results of thorough system-level lte network simulations. combined post-processing method shows the best reliability against location errors in terms of root mean squared error (rmse) and percent gain.
010f0dcc-a68f-43cf-92a8-169a8e587f20 phylogenetic utility of two existing and four novel nuclear gene loci in reconstructing tree of life of ray-finned fishes: the order cypriniformes (ostariophysi) as a case study. after the completion of several entire genome projects and a remarkable increase in public genetic databases in the recent years the results of post-genomic analyses can facilitate a better understanding of the genomic evolution underlying the diversity of organisms and the complexity of gene function. this influx of genomic information and resources is also beneficial to the discipline of systematic biology. in this paper, we describe a set of 6 previous and 22 new pcr/sequencing primers for rag1, rhodopsin and four novel nuclear markers from irbp, egr1, egr2b and egr3 that we developed through an approach making use of public genetic/genomic data mining for one of the ongoing tree of life projects aimed at understanding the evolutionary relationships of the planet's largest clade of freshwater fishes  the cypriniformes. the primers and laboratory protocols presented here were successfully tested in 33 species comprising all cypriniform family and subfamily groups. phylogenetic performance of each gene, as well as their implications in the investigation of the evolution of cypriniform fishes were assessed and discussed.
0110eb7a-fc80-4f89-9c8e-5f238581855a loss minimization in dc motor drives in this work, a method to minimize dc motor drive losses is presented. it is based on a model that includes motor and converter losses. the method is theoretically presented and experimental results to validate the theory are shown. a controller for torque and speed is proposed.
0110f1d1-2910-4290-a14b-a766e7e25b71 learning language from perceptual context: a challenge problem for ai we present the problem of learning to understand natural language from examples of utterances paired only with their relevant real-world context as an important challenge problem for ai. machine learning has been adopted as the most effective way of developing naturallanguage processing systems; however, currently, complex annotated corpora are required for training. by learning language from perceptual context, the need for laborious annotation is removed and the systems resulting understanding is grounded in its perceptual experience.
0111378e-5c9d-47b3-af47-be065f363b51 a data mining based intelligent system for worsted process decision textile production is a very complex industrial process, whose planning still depends on experts' knowledge and experience. with traditional techniques, a great many process parameters have to be repeatedly computed and the optimization of process parameters is also getting more and more difficult. however the proliferation of a huge mass of data from real production has been creating many new opportunities for those working in textile science, engineering and business. the field of data mining (dm) and knowledge discovery from database (kdd) has emerged as a new discipline in engineering and computer science. this paper investigates data mining methods from the industrial database, and presents a novel dm-based intelligent model (dmim) for worsted process decisions through an integral application of case-based reasoning (cbr) and artificial neural network (ann) techniques. first, from the rich existing process database, cbr is able to retrieve and recommend a similar process case as a process template; t...
011185b0-0901-4748-bc82-cd61c94c49e7 predicting zoonotic risk of influenza a viruses from host tropism protein signature using random forest influenza a viruses remain a significant health problem, especially when a novel subtype emerges from the avian population to cause severe outbreaks in humans. zoonotic viruses arise from the animal population as a result of mutations and reassortments, giving rise to novel strains with the capability to evade the host species barrier and cause human infections. despite progress in understanding interspecies transmission of influenza viruses, we are no closer to predicting zoonotic strains that can lead to an outbreak. we have previously discovered distinct host tropism protein signatures of avian, human and zoonotic influenza strains obtained from host tropism predictions on individual protein sequences. here, we apply machine learning approaches on the signatures to build a computational model capable of predicting zoonotic strains. the zoonotic strain prediction model can classify avian, human or zoonotic strains with high accuracy, as well as providing an estimated zoonotic risk. this would therefore allow us to quickly determine if an influenza virus strain has the potential to be zoonotic using only protein sequences. the swift identification of potential zoonotic strains in the animal population using the zoonotic strain prediction model could provide us with an early indication of an imminent influenza outbreak.
0111962d-8b08-4840-901b-25e4a85479b2 data mining of audiology patient records: factors influencing the choice of hearing aid type in this paper we describe our analysis of a database of over 180,000 patient records, collected from over 23,000 patients, by the hearing aid clinic at james cook university hospital in middlesbrough, uk. these records consist of audiograms (graphs of the faintest sounds audible to the patient at six different pitches), categorical data (such as age, gender, diagnosis and hearing aid type) and brief free text notes made by the technicians. we mine this data to determine which factors contribute to the decision to fit a bte (worn behind the ear) hearing aid as opposed to an ite (worn in the ear) hearing aid. from pca (principal component analysis) we determined four main audiogram types, and we relate these to the type of hearing aid chosen. we combine the effects of age, gender, diagnosis, masker, mould and individual audiogram frequencies into a single model by means of logistic regression. we also discovered some significant keywords in the free text fields by using the chi-squared ( 2 ) test, which can also be used in the model. the final model can act a decision support tool to help decide whether an individual patient should be offered a bte or an ite hearing aid.
0111afc2-d5ec-4538-afa0-d0cba9a688be analysis of a planetary scale scientific collaboration dataset reveals novel patterns scientific collaboration networks are an important component of scientific output and contribute significantly to expanding our knowledge and to the economy and gross domestic product of nations. here we examine a dataset from the mendeley scientific collaboration network. we analyze this data using a combination of machine learning techniques and dynamical models. we find interesting clusters of countries with different characteristics of collaboration. some of these clusters are dominated by developed countries that have higher number of self connections compared with connections to other countries. another cluster is dominated by impoverished nations that have mostly connections and collaborations with other countries but fewer self connections. we also propose a complex systems dynamical model that explains these characteristics. our model explains how the scientific collaboration networks of impoverished and developing nations change over time. we also find interesting patterns in the behaviour of countries that may reflect past foreign policies and contemporary geopolitics. our model and analysis gives insights and guidelines into how scientific development of developing countries can be guided. this is intimately related to fostering economic development of impoverished nations and creating a richer and more prosperous society.
0111ccb3-0298-4e0e-92cc-1228958c3e27 crossnets: high-performance neuromorphic architectures for cmol circuits abstract: the exponential, moore's law, progress of electronics may be continued beyond the 10-nm frontier if the currently dominant cmos technology is replaced by hybrid cmol circuits combining a silicon mosfet stack and a few layers of parallel nanowires connected by self-assembled molecular electronic devices. such hybrids promise unparalleled performance for advanced information processing, but require special architectures to compensate for specific features of the molecular devices, including low voltage gain and possible high fraction of faulty components. neuromorphic networks with their defect tolerance seem the most natural way to address these problems. such circuits may be trained to perform advanced information processing including (at least) effective pattern recognition and classification. we are developing a family of distributed crossbar network (crossnet) architectures that permit the combination of high connectivity neuromorphic circuits with high component density. preliminary estimates show that this approach may eventually allow us to place a cortex-scale circuit with about 1010 neurons and about 1014 synapses on an approximately 10  10 cm2 silicon wafer. such systems may provide an average cell-to-cell latency of about 20 nsec and, thus, perform information processing and system training (possibly including self-evolution after initial training) at a speed that is approximately six orders of magnitude higher than in its biological prototype and at acceptable power dissipation.
0111ce27-f0ba-442b-9ed6-6738c2489f5a payload modeling for network intrusion detection systems a number of intrusion detection systems (ids) research efforts have demonstrated that network-based attacks can be detected by modeling normal network packet payloads and watching for anomalies. in this paper, we explore a data mining technique based on principal component analysis that can identify specific features within packet payloads that are highly representative of the network traffic. of their respective services. apart from reducing the processing overhead through minimization of the feature space, the autonomous identification of such sub-groups of features can readily enable ids's to develop classifiers that are more apt at separating normal traffic from anomalous traffic. we demonstrate the effectiveness of this techniques by generating feature sets from a collection of network traffic and applying them to the training and detection phases of a payload-based ids. the results show that it is able to separate network attacks while maintaining low false positive rates. we also show that random sampling of less than 100% of the payload is possible and allows the ids to combat attack obfuscation.
0111d329-00da-433a-9aaf-61aa4086eb56 i-questionnaire - a software service tool for data analysis in e-business customization and flexibility of generating managed source codes for questionnaire in data analysis with the approach of software as a service (saas) becomes an essential and critical mission in e-business. today, building a on-demand software service for web-based questionnaire generation system is new in cloud computing environment. this paper describes a saas approach to dynamically generate a full set of asp.net source codes and database schema for web-based questionnaire systems for data analysis in e-business. the tool we proposed, i-questionnaire, adopts saas for the software delivery model in which software and its associated data are hosted centrally and accessed by users using a thin client. the tool i-questionnaire provides the template selections for designers to choose and construct the questionnaires they prefer. the generated asp.net source codes can be presented in the form of the questionnaires shown on the mobile or web-based platforms via internet. the results of the answers are stored in repository for data analysis and data mining. it also provides the table schema definition for any relational database server in a cloud computing environment. in addition, the generated source codes present the result data on-line in graphical presentation in a technical and professional way. in this paper, four major components are identified in the saas framework, which have been fully implemented with the c++ language in our prototyping. the outcomes of i-questionnaire, source codes of asp.net and sql table schema, can be simply placed in iis server under cloud computing environment to perform business analysis.
0111de73-081b-47cb-80d1-b355aff75b56 comparison of artificial intelligence techniques for river flow forecasting the use of artificial intelligence methods is be- coming increasingly common in the modeling and forecast- ing of hydrological and water resource processes. in this study, applicability of adaptive neuro fuzzy inference sys- tem (anfis) and artificial neural network (ann) methods, generalized regression neural networks (grnn) and feed forward neural networks (ffnn), and auto-regressive (ar) models for forecasting of daily river flow is investigated and seyhan river and cine river was chosen as case study area. for the seyhan river, the forecasting models are es- tablished using combinations of antecedent daily river flow records. on the other hand, for the cine river, daily river flow and rainfall records are used in input layer. for both stations, the data sets are divided into three subsets, training, testing and verification data set. the river flow forecasting models having various input structures are trained and tested to investigate the applicability of anfis and ann and ar methods. the results of all models for both training and test- ing are evaluated and the best fit input structures and methods for both stations are determined according to criteria of per- formance evaluation. moreover the best fit forecasting mod- els are also verified by verification set which was not used in training and testing processes and compared according to criteria. the results demonstrate that anfis model is supe- rior to the grnn and ffnn forecasting models, and anfis can be successfully applied and provide high accuracy and reliability for daily river flow forecasting.
0111fdbb-bed7-451a-9adb-a339ce6087c7 an epistemological and pattern analysis of empirical data that influencesemergency loan need among graduate students this analysis studies closely education affordability through the epistemology of emergency loan need that signals economic challenges on the horizon for domestic and international students seeking a post graduate credential at any cost. prior studies have been very helpful; however, to the best of our knowledge there is not a comprehensive study that has investigated the comparison of small vs. large emergency student loans taken out by graduate students. also, to the best of our knowledge and to date there are no studies that have investigated the patterns and relationships among ethnicity, gender, marital status, degree type, and college awarded for both small vs. large emergency loans. to fill the gaps in the literature, we conducted our research by collecting datasets from 335 graduate students enrolled in a large public university located in north america. our data analysis provides strong indicators and evidence that both small and large emergency loan needs exist in a diverse spectrum of colleges, degree types, ethnicities, genders, ages, and marital statuses. also, the regression analysis indicates that there is not a significant relationship between gpa and emergency loan needs for both small and large loans. we also, used data mining technique to investigate patters and relationships among ethnicity, gender, marital status, degree type, and college awarded for both small vs. large emergency loans. our study contains vast research and managerial implications for both academia and top managements.
0112d91a-17a9-45f4-b4b5-8b300f9de526 attributes reduction model with user preferences in rough set theory, attributes reduction algorithms are utilized to extract patterns or rules from the table-formed decision systems. the attributes reduction algorithms aim to find the minimum subset of attributes which can distinguish all the items of different classes. however, most existing attributes reduction algorithms over-focus the distinguishability of attributes and neglect the user requirements in real applications. to tackle this problem, we propose an attributes reduction model with preferences to discover patterns according to user interests. in the proposed model, user preferences in data mining task are formally represented by attribute orders. for the attributes reduction algorithm implementation, we design the novel data structure of linked list to storage the elements for discerning pairwise items and adopt discernibility thresholds to avoid overfitting. the proposed attributes reduction model with user preferences is performed on uci data sets. experimental results demonstrate that the proposed attributes reduction model is effective to extract the data patterns consistent with user requirements.
011334d0-2863-4e0f-9ddf-e821480d45fa biomimetic approach to tacit learning based on compound control the remarkable capability of living organisms to adapt to unknown environments is due to learning mechanisms that are totally different from the current artificial machine-learning paradigm. computational media composed of identical elements that have simple activity rules play a major role in biological control, such as the activities of neurons in brains and the molecular interactions in intracellular control. as a result of integrations of the individual activities of the computational media, new behavioral patterns emerge to adapt to changing environments. we previously implemented this feature of biological controls in a form of machine learning and succeeded to realize bipedal walking without the robot model or trajectory planning. despite the success of bipedal walking, it was a puzzle as to why the individual activities of the computational media could achieve the global behavior. in this paper, we answer this question by taking a statistical approach that connects the individual activities of computational media to global network behaviors. we show that the individual activities can generate optimized behaviors from a particular global viewpoint, i.e., autonomous rhythm generation and learning of balanced postures, without using global performance indices.
01142bff-7fa5-485e-94de-436595c1492a kddml: a middleware language and system for knowledge discovery in databases kddml (kdd markup language) is a middleware language and system designed to support the development of final applications or higher level systems which deploy a mixture of data access, data pre-processing, extraction and deployment of data mining models.we present our three-years' experience in the development of kddml. the design principles are motivated by requirements derived from recurring patterns in the kdd process.the kddml language is xml-based, both for query syntax and data/model representation. a kddml query is an xml-document where xml tags correspond to operations on data/models, xml attributes correspond to parameters of those operations and xml sub-elements define arguments passed to the operators. we present the operators for data access and preprocessing, model extraction and deployment, and control flow ones.the core of the kddml system is a kddml language interpreter with modularity and extensibility requirements as the main goals. additional data sources, and preprocessing and mining algorithms can be easily plugged in the system.
011432d5-e4c1-4a94-be4b-1bfeeb7e6a22 emergence from brain architectures: a new cognitive science? the way in which artificial intelligence has developed over the last 50 years has had a major role in shaping cognitive science as it is today. this has generated computational models of behaviour. the connectionist revival of the 1980s added a tinge of neurodynamics to this. here i suggest that some post-connectionist work in artificial intelligence is turning towards an understanding and formalisation of the mechanisms of brain architectures which contribute to an emergence of cognition providing a closer link between brain mechanisms and experienced brain states. this even addresses the neurological basis of consciousness.
01153892-3c54-4671-93d0-d5dd76272233 photometric classification of type ia supernovae in the supernova legacy survey with supervised learning in the era of large astronomical surveys, photometric classification of supernovae (sne) has become an important research field due to limited spectroscopic resources for candidate follow-up and classification. in this work, we present a method to photometrically classify type ia supernovae based on machine learning with redshifts that are derived from the sn light-curves. this method is implemented on real data from the snls deferred pipeline, a purely photometric pipeline that identifies sne ia at high-redshifts (0.2 < z < 1.1). our method consists of two stages: feature extraction (obtaining the sn redshift from photometry and estimating light-curve shape parameters) and machine learning classification. we study the performance of different algorithms such as random forest and boosted decision trees. we evaluate the performance using sn simulations and real data from the first 3 years of the supernova legacy survey (snls), which contains large spectroscopically and photometrically classified type ia samples. using the area under the curve (auc) metric, where perfect classification is given by 1, we find that our best-performing classifier (extreme gradient boosting decision tree) has an auc of 0.98.we show that it is possible to obtain a large photometrically selected type ia sn sample with an estimated contamination of less than 5%. when applied to data from the first three years of snls, we obtain 529 events. we investigate the differences between classifying simulated sne, and real sn survey data. in particular, we find that applying a thorough set of selection cuts to the sn sample is essential for good classification. this work demonstrates for the first time the feasibility of machine learning classification in a high-z sn survey with application to real sn data.
011549f4-6b75-43fd-b277-e12ac83f0d89 a data mining based approach to predict spatiotemporal changes in satellite images the interpretation of remotely sensed images in a spatiotemporal context is becoming a valuable research topic. however, the constant growth of data volume in remote sensing imaging makes reaching conclusions based on collected data a challenging task. recently, data mining appears to be a promising research field leading to several interesting discoveries in various areas such as marketing, surveillance, fraud detection and scientific discovery. by integrating data mining and image interpretation techniques, accurate and relevant information (i.e. functional relation between observed parcels and a set of informational contents) can be automatically elicited.#r##n##r##n#this study presents a new approach to predict spatiotemporal changes in satellite image databases. the proposed method exploits fuzzy sets and data mining concepts to build predictions and decisions for several remote sensing fields. it takes into account imperfections related to the spatiotemporal mining process in order to provide more accurate and reliable information about land cover changes in satellite images. the proposed approach is validated using spot images representing the saint-denis region, capital of reunion island. results show good performances of the proposed framework in predicting change for the urban zone.
01159a79-41fa-48fd-9a4c-bb2bb3babde1 incremental and decremental proximal support vector classification using decay coefficients this paper presents an efficient approach for supporting decremental learning for incremental proximal support vector machines (svm). the presented decremental algorithm based on decay coefficients is compared with an existing window-based decremental algorithm, and is shown to perform at a similar level in accuracy, but providing significantly better computational performance.
011631dc-8fa0-4b45-96ca-9e37f448192f a knowledge management framework for imbalanced data using frequent pattern mining based on bloom filter managing medical environments and organizations performance depend directly on the knowledge management (km) systems. knowledge discovery (kd) is responsible for digging information from datasets and finding internal knowledge within organizations or external sources. data mining (dm) is the core of kd process. although recent mining techniques have proven their accuracy in discovering the knowledge from balanced data, where the class distribution is balanced, the problem of discovering knowledge from unbalanced data is still a challenge that needs to be addressed. a clustered knowledge management framework (ckmd) is presented in this paper, for enhancing the performance of kd from unbalanced data. a simple hybrid sampling approach (shsa) is proposed to reduce the adverse impacts of imbalanced data. mining frequent pattern process plays an important role in kd process. moreover, a frequent pattern mining algorithm based on bloom filter (fpmbf) is proposed to discover items that frequently co-occur in the data using the bloom filter, that requires a single scan of the data, which leads to less time consuming in discovering knowledge for imbalanced data. finally, the performance of the proposed methods is evaluated using real datasets and comparative experiments.
011639f2-9db5-44f0-8b12-5d31e40a5728 characterization of dynamic bayesian network the dynamic bayesian network as temporal network in this report, we will be interested at dynamic bayesian network (dbns) as a model that tries to incorporate temporal dimension with uncertainty. we start with basics of dbn where we especially focus in inference and learning concepts and algorithms. then we will present different levels and methods of creating dbns as well as approaches of incorporating temporal dimension in static bayesian network. the majority of events encountered in everyday life are not well described based on their occurrence at a particular point in time but rather they are described by a set of observations that can produce a comprehensive final event. thus, time is an important dimension to take into account in reasoning and in the field of artificial intelligence in general. to add the time dimension in bayesian networks, different approaches have been proposed. the common names used to describe this new dimension are "temporal" and "dynamic ".
0116eee2-436f-43db-98d0-e1f07530ebfc tools for constructing knowledge-based systems the original expert systems for the most part were handcrafted directly, using various dialects of the lisp programming language. the inference and knowledge representation components of these systems can be separated from the domain-specific portion of the expert system and can be used again for an entirely different task. some of these tools, generically called shells, are discussed. although these shells provide help in building knowledge-based systems, considerable skill in artificial intelligence programming is still necessary to create an expert system that accomplishes a nontrivial task.
01175615-d164-42a4-a1a0-b1995409d941 automatic annotation approach of events in news articles daily, several news agencies publish thousands of articles concerning many events of all types (political, economic, cultural, etc.). the decision-makers find themselves in front of a great number of events, a few of which concern them. the automatic treatment of such events becomes increasingly necessary. thus, we propose a machine learning-based approach that allows annotating news articles to generate an automatic summary of the events. we propose a new similarity measurement between events and we validate our approach by the development of the "annotev" system.
01178ba4-065b-41a0-8a19-75a88d816829 speaker and text independent language identification using predictive error histogram vectors a predictive vector quantization (gray, m., 1984; jain, a.k. et al., 2000) based speaker and text independent language identification system is proposed, which uses the statistical distribution of predictive error vectors to recognize the language spoken by native speakers. according to stan c. kwasny et al. (see proc. 5th midwest artificial intelligence and cognitive science soc. conf., p.53-7, 1993), most high level features of speech, such as tone of voice, rhythm, style, pace, accent, etc., appear to be related to distributional patterns or statistical aggregates of speech waveforms. we further develop the method used by qian-rong gu and tadashi shibata (6th world multiconference on systemics, cybernetics and informatics - sci2002, 2002) to extract these statistical distributional patterns directly from raw speech waveforms and then use them to identify language. the system has been trained and. tested by speech from english and japanese native speakers. a best identification ratio of 76.8% can be achieved by our system.
0117bde9-c8c4-4ddf-8383-add76c04b554 support vector regression with automatic accuracy control 
0117f04b-f013-4afb-b4e3-d9b0f40dd567 a survey on filter techniques for feature selection in gene expression microarray analysis a plenitude of feature selection (fs) methods is available in the literature, most of them rising as a need to analyze data of very high dimension, usually hundreds or thousands of variables. such data sets are now available in various application areas like combinatorial chemistry, text mining, multivariate imaging, or bioinformatics. as a general accepted rule, these methods are grouped in filters, wrappers, and embedded methods. more recently, a new group of methods has been added in the general framework of fs: ensemble techniques. the focus in this survey is on filter feature selection methods for informative feature discovery in gene expression microarray (gem) analysis, which is also known as differentially expressed genes (degs) discovery, gene prioritization, or biomarker discovery. we present them in a unified framework, using standardized notations in order to reveal their technical details and to highlight their common characteristics as well as their particularities.
01183c50-6c12-4237-9bfa-3453570e0ca7 sentiments analysis of twitter data using data mining with rapid growth in user of social media in recent years, the researcher get attracted towords the use of social media data for sentiments analysis of people or particular product or person or event. twitter is one of the widely used social media platform to express the thoughts. this paper presents approch for analysing the sentiments of users using data mining classifiers. it also compares the performance of single classifiers for sentiments analysis over ensemble of classifier. expermental results obtained demonstrates that k-nearest neighbour classifier gives very high predictive accuracy. result also demonstrate that single classifiers outperforms ensemble of classifier approch.
011895aa-c59a-4916-a484-71026955d778 what recommendation systems for software engineering recommend rsses mainly output source code artifacts and experts, other types are less explored.rsses providing a comprehensive support for testing phase were not identified.rsses are very task specific, but not environment specific. a recommendation system for software engineering (rsse) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. present the results of a systematic literature review to reveal the typical functionality offered by existing rsses, research gaps, and possible research directions. we evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of rsse systems. we include papers describing tools that support source code related development published between 2003 and 2013. the results show that rsses typically visualize source code artifacts. they aim to improve system quality, make the development process more efficient and less expensive, lower developer's cognitive load, and help developers to make better decisions. they mainly support reuse actions and debugging, implementation, and maintenance phases. the majority of the systems are reactive. unexploited opportunities lie in the development of recommender systems outside the source code domain. furthermore, current rsse systems use very limited context information and rely on simple models. context-adapted and proactive behavior could improve the acceptance of rsse systems in practice.
011916fa-1b37-47ea-80d9-03032f208b00 profitability prediction model based on support vector machines classification techniques which hold a very important place in business and economic research have drawn many research interests in previous literatures. recent studies have shown that machine learning techniques can achieve better performance. however, the application of support vector machines (svm) for profitability prediction has not been widely explored. this study investigates the efficacy of applying svm in profitability prediction problem and attempts to suggest a new model with better explanatory power and stability. the experimental results demonstrate that the proposed classifier of svm approach to the problem of profitability prediction of companies upon the basis of a set of financial ratios.
01199b0a-6a33-4f99-960d-49a223a33aee can artificial intelligence and fuzzy logic be integrated into virtual reality applications in mining 
011a633c-a8fd-49d8-b6cc-b627063a6196 diagnosis of asian raiamas (teleostei: cyprinidae: chedrina) with comments on chedrin relationships and previously proposed diagnostic characters for opsaridium and raiamas as currently recognized, raiamas is the sole chedrin genus with a transoceanic distribution. while a previous morphological study suggested that asian raiamas are morphologically differentiated from their african congeners, a single transoceanic genus was retained. among the characters previously proposed, only the plesiomorphic presence of maxillary barbels in asian raiamas serves to distinguish them from their african congeners. here we provide five novel apomorphies that do diagnose asian raiamas as phylogenetically distinct from the african species. molecular data corroborate the non-monophyly of raiamas, indicating that the asian species, raiamas bola and raiamas guttatus, form a clade that is sister to either the entire african chedrin assemblage or to the genus opsarius. african raiamas are not monophyletic and are instead arrayed in three groups. review of previously proposed synapomorphies for opsaridium and raiamas suggests that none can be considered diagnostic for these two genera, and our study underscores the need for a comprehensive review of the generic composition and intrarelationships among the poorly studied african chedrins.
011b1a17-04b8-477e-b1e7-4744334b372e computer models for identifying instrumental citations in the biomedical literature the most popular method for evaluating the quality of a scientific publication is citation count. this metric assumes that a citation is a positive indicator of the quality of the cited work. this assumption is not always true since citations serve many purposes. as a result, citation count is an indirect and imprecise measure of impact. if instrumental citations could be reliably distinguished from non-instrumental ones, this would readily improve the performance of existing citation-based metrics by excluding the non-instrumental citations. a citation was operationally defined as instrumental if either of the following was true: the hypothesis of the citing work was motivated by the cited work, or the citing work could not have been executed without the cited work. this work investigated the feasibility of developing computer models for automatically classifying citations as instrumental or non-instrumental. instrumental citations were manually labeled, and machine learning models were trained on a combination of content and bibliometric features. the experimental results indicate that models based on content and bibliometric features are able to automatically classify instrumental citations with high predictivity (auc = 0.86). additional experiments using independent hold out data and prospective validation show that the models are generalizeable and can handle unseen cases. this work demonstrates that it is feasible to train computer models to automatically identify instrumental citations.
011b920d-0f04-4f0b-a6fc-00469895a942 managed biometric identity a computing system such as a game console maintains and updates a biometric profile of a user. in one aspect, biometric data of the user is continuously obtained from a sensor such as an infrared and visible light camera, and used to update the biometric profile using a machine learning process. in another aspect, a user is prompted to confirm his or her identify when multiple users are detected at the same time and/or when the user is detected with a confidence level which is below a threshold. a real-time image of the user being identified can be displayed on a user interface with user images associated with one or more accounts. in another aspect, the biometric profile is managed by a shell on the computing system, where the shell makes the biometric profile available to any of a number of applications on the computing system.
011c97fa-a114-4a74-a26d-c7fa94f3266b learning-based power/performance optimization for many-core systems with extended-range voltage/frequency scaling near-threshold computing has emerged as a promising solution to significantly increase the energy efficiency of next-generation multicore systems. this paper evaluates and analyzes the behavior of dynamic voltage and frequency scaling for multicore systems operating under extended range: including near-threshold, nominal, and turbo modes. we adapt the model selection technique from machine learning to determine the relationship between performance and power. the theoretical results show that the resulting models satisfy convexity, which efficiently determines the optimal voltage/frequency operating points for: 1) minimizing energy consumption under throughput constraints or 2) maximizing throughput under a given power budget. we validate our models on finfet-based chip-multiprocessors. considering process variations (pvs), experimental results show that at 30% pv levels, our proposed method: 1) reduces energy consumption by 31.09% at iso-performance condition and 2) increases throughput by 11.46% at iso-power when compared with variation-agnostic nominal case.
011d1ce3-3c18-4216-a85a-1f45470cecb2 a recommender system approach to enhance web search and query formulation aics 2008: the 19th irish conference on artificial intelligence and  cognitive science, 27-29 aug 2008, cork
011d602f-b0bf-4f53-a124-2a93039fb885 color and size image dataset normalization protocol for natural image classification: a case study in tomato crop pathologies in computer vision research, the construction of image datasets is a critical process, given the need for robust experimentation frameworks that ensure the quality and validity of the resulting conclusions and performance measurements in each particular study. therefore, experimental datasets must optimize their statistical, visual and computational properties through an adequate selection of representative and useful visual data, according to the specific research question being addressed. this paper proposes a dataset construction protocol for ad hoc acquired images in a particular machine learning application: tomato crop health assessment.
011eceb2-f5e4-46a2-8562-9eb5c116d5c5 lapin-spam: an improved algorithm for mining sequential pattern sequence pattern mining is an important research problem because it is the basis of many other applications. yet how to efficiently implement the mining is difficult due to the inherent characteristic of the problem - the large size of the data set. in this paper, by combining spam, we propose a new algorithm called last position induction sequential pattern mining (abbreviated as lapin-spam), which can efficiently get all the frequent sequential patterns from a large database. the main difference between our strategy and the previous works is that when judging whether a sequence is a pattern or not, they use s-matrix by scanning projected database (prefixspan) or count the number by joining (spade) or anding with the candidate item (spam). in contrast, lapin-spam can easily implement this process based on the following fact - if an items last position is smaller than the current prefix position, the item can not appear behind the current prefix in the same customer sequence. lapin-spam could largely reduce the search space during mining process and is considerable effectiveness in mining sequential pattern. our experimental results show that lapin-spam outperforms spam up to three times on all kinds of dataset.
011ed426-0f64-49f3-a11f-5cd87a2adfb6 multi-class support vector machines 
011f8514-6a5f-4e18-900e-0597aa1b1207 a robust meaning extraction methodology using supervised neural networks a large amount of information, stored in intranets and internet databases and accessed through the world-wide web, is organized in the form of full-text documents. efficient retrieval of this information with regards to its meaning and content is an important problem in data mining systems for the creation, management and querying of very large such information bases. in this paper we deal with the main aspect of the problem of extracting meaning from documents, namely, with the problem of text categorization, outlining a novel and systematic approach to it's solution. we present a text categorization system for non-domain specific full-text documents based on the learning and generalization capabilities of neural networks. the main contribution of this paper lies on the feature extraction methodology which, first, involves word semantic categories and not raw words as other rival approaches. as a consequence of coping with the problem of dimensionality reduction, the proposed approach introduces a novel second order approach for text categorization feature extraction by considering word semantic categories cooccurrence analysis. the suggested methodology compares favorably to widely accepted, raw word frequency based techniques in a collection of documents concerning the dewey decimal classification (ddc) system. in these comparisons different multilayer perceptrons (mlp) algorithms as well as the support vector machine (svm), the lvq and the conventional k-nn technique are involved.
011fea21-9726-47b9-bbb1-631b78200e03 a multiworld testing decision service applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. we create a service that uses machine learning to accomplish this goal. the service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle. #r##n#the system is general: it works for any discrete choices, with respect to any reward metric, and can work with many learning algorithms and feature representations. the service has a simple api, and was designed to be modular and reproducible to ease deployment and debugging, respectively. we demonstrate how these properties enable learning systems that are robust and safe. #r##n#our evaluation shows that the decision service makes decisions in real time and incorporates new data quickly into learned policies. a large-scale deployment for a personalized news website has been handling all traffic since jan. 2016, resulting in a 25% relative lift in clicks. by making the decision service externally available, we hope to make optimal decision making available to all.
01206ccb-ff17-480e-8c5c-558831417fdb intelligent power management in a vehicular system with multiple power sources this paper presents an optimal online power management strategy applied to a vehicular power system that contains multiple power sources and deals with largely fluctuated load requests. the optimal online power management strategy is developed using machine learning and fuzzy logic. a machine learning algorithm has been developed to learn the knowledge about minimizing power loss in a multiple power sources and loads (m_ps&ld) system. the algorithm exploits the fact that different power sources used to deliver a load request have different power losses under different vehicle states. the machine learning algorithm is developed to train an intelligent power controller, an online fuzzy power controller, fpc_mps, that has the capability of finding combinations of power sources that minimize power losses while satisfying a given set of system and component constraints during a drive cycle. the fpc_mps was implemented in two simulated systems, a power system of four power sources, and a vehicle system of three power sources. experimental results show that the proposed machine learning approach combined with fuzzy control is a promising technology for intelligent vehicle power management in a m_ps&ld power system.
0121b8a6-acb9-4fc7-8be9-3cb195747792 a genetic algorithm for discovering small disjunct rules in data mining this paper addresses the well-known classification task of data mining, where the goal is to discover rules predicting the class of examples (records of a given dataset). in the context of data mining, small disjuncts are rules covering a small number of examples. hence, these rules are usually error-prone, which contributes to a decrease in predictive accuracy. at first glance, this is not a serious problem, since the impact on predictive accuracy should be small. however, although each small-disjunct covers few examples, the set of all small disjuncts can cover a large number of examples. this paper presents evidence that this is the case in several datasets. this paper also addresses the problem of small disjuncts by using a hybrid decision-tree/genetic-algorithm approach. in essence, examples belonging to large disjuncts are classified by rules produced by a decision-tree algorithm (c4.5), while examples belonging to small disjuncts are classified by a genetic-algorithm specifically designed for discovering small-disjunct rules. we present results comparing the predictive accuracy of this hybrid system with the prediction accuracy of three versions of c4.5 alone in eight public domain datasets. overall, the results show that our hybrid system achieves better predictive accuracy than all three versions of c4.5 alone.
0121ca48-8eb2-4e06-ba96-4d35223df928 comparing intended and real usage in web portal: temporal logic and data mining nowadays the software systems, including web portals, are developed from a priori assumptions about how the system will be used. however, frequently these assumptions hold only partly and are dened also partially. therefore one must be capable to compare the a priori as- sumptions with the actual user behavior in order to decide how the sys- tem could be improved. to tackle this problem, we consider a promising approach to employ the same formalism to express the intended usage, the web portal model and the frequent real usage patterns, extracted from the experimental data by data mining algorithms. this allows to automate the verication whether the frequent real usage patterns satisfy the intended usage in the web portal model. we propose to use temporal logic and kripke structure as such a common formalism.
012214ad-ce33-4edb-b92b-9c5107196cd6 predicting defect-prone software modules using support vector machines effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. support vector machines (svm) have been successfully applied for solving both classification and regression problems in many applications. this paper evaluates the capability of svm in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four nasa datasets. the results indicate that the prediction performance of svm is generally better than, or at least, is competitive against the compared models.
01222746-5ca7-499f-874f-cd8f57a70682 a confidence-based approach for balancing fairness and accuracy we study three classical machine learning algorithms in the context of algorithmic fairness: adaptive boosting, support vector machines, and logistic regression. our goal is to maintain the high accuracy of these learning algorithms while reducing the degree to which they discriminate against individuals because of their membership in a protected group. #r##n#our first contribution is a method for achieving fairness by shifting the decision boundary for the protected group. the method is based on the theory of margins for boosting. our method performs comparably to or outperforms previous algorithms in the fairness literature in terms of accuracy and low discrimination, while simultaneously allowing for a fast and transparent quantification of the trade-off between bias and error. #r##n#our second contribution addresses the shortcomings of the bias-error trade-off studied in most of the algorithmic fairness literature. we demonstrate that even hopelessly naive modifications of a biased algorithm, which cannot be reasonably said to be fair, can still achieve low bias and high accuracy. to help to distinguish between these naive algorithms and more sensible algorithms we propose a new measure of fairness, called resilience to random bias (rrb). we demonstrate that rrb distinguishes well between our naive and sensible fairness algorithms. rrb together with bias and accuracy provides a more complete picture of the fairness of an algorithm.
012235be-5186-4128-8538-ca4dac2096a1 application of artificial neural networks to effluent phosphate prediction in struvite recovery in advanced wastewater treatment plants (awwtp), the recovery of phosphorus (p) has become a recent focus of the wastewater engineering industry. the potential economic savings behind improved sludge management and the control of struvite encrustation in awwtp are two of the primary driving forces behind this. process control of phosphorus (struvite) recovery systems has only been partially successful because: (1) key control variables have yet to be identified and (2) there is no adequate performance evaluation model that is applicable to struvite recovery technologies. in process control, two different types of modeling are most commonly seen: mechanistic and black-box style models. in recent years, varying models have been developed to try to predict the formation of struvite in both sludge digestion process lines and p-recovery technologies designed for struvite removal. all of these are strictly mechanistic models, based on either the chemical equilibrium of the system or the associated kinetic par...
01228683-42fd-40f9-9b82-51a8c2009807 grassmann clustering an important tool in high-dimensional, explorative data mining is given by clustering methods. they aim at identifying samples or regions of similar characteristics, and often code them by a single codebook vector or centroid. one of the most commonly used partitional clustering techniques is the k-means algorithm, which in its batch form partitions the data set into k disjoint clusters by simply iterating between cluster assignments and cluster updates. the latter step implies calculating a new centroid within each cluster. we generalize the concept of k-means by applying it not to the standard euclidean space but to the manifold of subvectorspaces of a fixed dimension, also known as the grassmann manifold. important examples include projective space i.e. the manifold of lines and the space of all hyperplanes. detecting clusters in multiple samples drawn from a grassmannian is a problem arising in various applications. in this manuscript, we provide corresponding metrics for a grassmann k-means algorithm, and solve the centroid calculation problem explicitly in closed form. an application to nonnegative matrix factorization illustrates the feasibility of the proposed algorithm.
012290c5-24ee-4cfa-b278-9bf61106539c cluster merging based on a decision threshold data clustering is an important unsupervised learning technique and has wide application in various fields including pattern recognition, data mining, image analysis and bioinformatics. a vast amount of clustering algorithms have been proposed in the past decades. however, existing algorithms still face many problems in practical applications. one typical problem is the parameter dependence, which means that user-specified parameters are required as input and the clustering results are influenced by these parameters. another problem is that many algorithms are not able to generate clusters of non-spherical shapes. in this paper, a cluster merging method is proposed to solve the above-mentioned problems based on a decision threshold and the dominant sets algorithm. firstly, the influence of similarity parameter on dominant sets clustering results is studied, and it is found that the obtained clusters become larger with the increase in similarity parameter. we analyze the reason behind this behavior and propose to generate small initial clusters in the first step and then merge the initial clusters to improve the clustering results. specifically, we select a similarity parameter which generates small but not too small clusters. then, we calculate pairwise merging decisions among the initial clusters and obtain a merging decision threshold. based on this threshold, we merge the small clusters and obtain the final clustering results. experiments on several datasets are used to validate the effectiveness of the proposed algorithm.
0122bad6-58b6-46cd-b888-850acd067056 big data mining based on computational intelligence and fuzzy clustering 
012341aa-dc2c-49b9-8b13-04f1f93e65f9 fundamental and technical analysis: substitutes or complements? although the fundamental and technical analysis literatures invest considerable effort in assessing their respective ability to explain share prices, they invariably do so without reference to each other. in this context, we propose an equity valuation model integrating both fundamental and technical analysis and, in doing so, recognize their potential as complements rather than as substitutes. testing confirms the complementary nature of fundamental and technical analysis by showing that, although each performs well in isolation, models integrating both have superior explanatory power. while our findings relate to the valuation of shares, they also have implications for other valuation exercises. copyright (c) the authors. journal compilation (c) 2009 afaanz.
0123b161-db88-481a-877f-185a0c282c6b procesamiento del lenguaje natural, un reto de la inteligencia artificial natural language processing a challenge for artificial intelligence this paper presents a comparison between the natural language and the artificial languages, highlighting that human language is complex, multifaceted and rich in expressions, yet, is ambiguous and requires interpretation according to the context and intent of the speaker, while that artificial languages are designed for a specific purpose, are limited in their syntax and semantics, thus are more accurate, with less space for free interpretation and context free. it shows the importance of research on the automatic processing of natural language, is referenced several major advances in this field and the areas which require natural language processing to improve computer systems.
012467d6-872b-4039-bc6e-021a55c9dd90 the mind-brain continuum guidelines for submitting commentspolicy: comments that contribute to the discussion of the article will be posted within approximately three business days. we do not accept anonymous comments. please include your email address; the address will not be displayed in the posted comment. cell press editors will screen the comments to ensure that they are relevant and appropriate but comments will not be edited. the ultimate decision on publication of an online comment is at the editors' discretion. formatting: please include a title for the comment and your affiliation. note that symbols (e.g. greek letters) may not transmit properly in this form due to potential software compatibility issues. please spell out the words in place of the symbols (e.g. replace  with alpha). comments should be no more than 8,000 characters (including spaces ) in length. references may be included when necessary but should be kept to a minimum. be careful if copying and pasting from a word document. smart quotes can cause problems in the form. if you experience difficulties, please convert to a plain text file and then copy and paste into the form.
01247c9b-01af-4816-9b42-d0193564a8d3 a short review on the application of computational intelligence and machine learning in the bioenvironmental sciences this paper aims to provide a short review on the application of computational intelligence (ci) and machine learning (ml) in the bioenvironmental sciences. to clearly illustrate the current status, we limit our focus to some key approaches, namely fuzzy systems (fss), artificial neural networks (anns) and genetic algorithms (gas) as well as some ml methods. the trends in the application studies are categorized based on the targets of the model such as animal, fish, plant, soil and water. we give an overview of specific topics in the bioenvironmental sciences on the basis of the review papers on model comparisons in the field. the summary of the modelling approaches with respect to their aim and potential application fields can promote the use of ci and ml in the bioenvironmental sciences.
0124d017-e249-401a-802f-8cdbda9477b3 dominance test on cp-nets abstract   researching preference is significative and interesting in artificial intelligence. the paper describes the concept of cp-nets which is a tool of preference representation firstly. and then the strict and weak dominance is discussed, some useful theorem of strict and weak dominant relation is proposed. finally, the future work on cp-nets reasoning is presented.
0124d3d5-440a-4080-8c23-9563c17ccbdd an optimization approach to services sales forecasting in a multi-staged sales pipeline. services organization manage a pipeline of sales opportunities with variable enterprise sales engagement lifespan, maturity levels (belonging to progressive sales stages), and contract values at any given point in time. accurate forecasting of contract signings by the end of a time period (e.g., a quarter) is a desire for many services organizations in order to get an accurate projection of incoming revenues, and to provide support for delivery planning, resource allocation, budgeting, and effective sales opportunity management. while the problem of sales forecasting has been investigated in its generic context, sales forecasting for services organizations entails the consideration of additional complexities, which has not been thoroughly investigated: (i) considering opportunities in multi-staged sales pipeline, which means providing stage-specific treatment of sales opportunities in each group, and (ii) using the information of the current pipeline build-up, as well as the projection of the pipeline growth over the remaining time period before the end of the target time period in order to make predictions. in this paper, we formulate this problem, considering the service-specific context, as a machine learning problem over the set of historical services sales data. we introduce a novel optimization approach for finding the optimized weights of a sales forecasting function. the objective value of our optimization model minimizes the average error rates for predicting sales based on two factors of conversion rates and growth factors for any given point in time in a sales period over historical data. our model also optimally determines the number of historical periods that should be used in the machine learning framework to predict the future revenue. we have evaluated the presented method, and the results demonstrate superior performance (in terms of absolute and relative errors) compared to a baseline state of the art method.
0124e86c-6883-433c-b72e-99b555d32083 selection of entropy based features for automatic analysis of essential tremor biomedical systems produce biosignals that arise from interaction mechanisms. in a general form, those mechanisms occur across multiple scales, both spatial and temporal, and contain linear and non-linear information. in this framework, entropy measures are good candidates in order provide useful evidence about disorder in the system, lack of information in time-series and/or irregularity of the signals. the most common movement disorder is essential tremor (et), which occurs 20 times more than parkinsons disease. interestingly, about 50%70% of the cases of et have a genetic origin. one of the most used standard tests for clinical diagnosis of et is archimedes spiral drawing. this work focuses on the selection of non-linear biomarkers from such drawings and handwriting, and it is part of a wider cross study on the diagnosis of essential tremor, where our piece of research presents the selection of entropy features for early et diagnosis. classic entropy features are compared with features based on permutation entropy. automatic analysis system settled on several machine learning paradigms is performed, while automatic features selection is implemented by means of anova (analysis of variance) test. the obtained results for early detection are promising and appear applicable to real environments.
01250508-295d-4ab5-917a-d17083dd2c44 an empirical study on chinese microblog stance detection using supervised and semi-supervised machine learning methods. 
0125e0f3-cc17-416d-96fc-a19b750a66a1 health educators and nutrition education: food for thought-a commentary as health educators involved in our profession, as members of our professional organizations, and as authors, reviewers, and members of various professional journal editorial boards, and along with involvement with professional preparation programs, our interest was piqued by the journal article titled, "what about health educators? nutrition education for allied health professionals: a review of the literature." (1) this study had three purposes: (1) to determine the definition of, and criteria for, nutrition education among allied health professionals; (2) to identify commonalities across health professions for nutrition education definitions and training requirements; and (3) to determine if there are criteria for nutrition education and training for health educators. specifically, we do not take exception that, according to the authors, "common across the studies of allied health professions included in this review was a sense of the importance of nutrition training and education of these professionals." the paper does document some shortcomings and obstacles in some other professional areas. nor do we take exception with the general recommendations to improve such training, which included suggestions to "increase the number of clinical hours spent in nutrition, increase the number of nutrition-focused continuing education hours, or integrate nutrition into the current training curriculum." however, after reading the article, our initial enthusiasm quickly waned and then evaporated in terms of its major purpose to "determine if there are criteria for nutrition education and training for health educators." while the topic of nutrition education certainly is worthy of inquiry, our concerns are threefold. this article appears to be a classic example of reporting findings and conclusions based on: (1) inadequate and faulty methodology, (2) a demonstrated misunderstanding of the health education discipline and, (3) several practical limitations. inadequate and faulty methodology scope of the search once the methodology was described, we anticipated that the findings and the conclusions would be self-evident without further reading. not surprisingly, they are consistent and perfectly fit the methodology. unfortunately, the limitations inherent in their methodology render any valid conclusions inadequate. specifically, the study is based on a reported comprehensive review of the literature. according to the authors, "this review involved rigorous search and assessment strategies to examine the body of literature advocating for the teaching of nutrition in the university curriculum for allied health professionals." to accomplish their review the authors searched two major heath science bibliography databases, medline and ebsco. reviews of literature, regardless of topic, require very detailed search strategies that have become more complicated over the years given the many new electronic databases that now exist. these search and data mining strategies were not described in enough detail to give us confidence that they accurately located all the existing articles in the professional literature. the small sample size of articles used in this review provides some evidence of this. for example, while the authors used the national library of medicine's medline bibliographic database that contains over 19 million references, these are primarily journal articles in the life sciences with a concentration in biomedicine. further, while one of the key distinctive features of medline is its use of medical subject headings known as mesh[r], the authors do not specifically mention their use of mesh descriptors or terms used in their searches. using different mesh terms and even reordering these terms can elicit very differing results. while they do describe using words such as, "students, health, occupation and nutrition/dietetics" they seem to have omitted other search terms such as "curriculum and education" that may have provided different results. ...
0125f92f-6ba2-4396-9036-8c827e5fed3f contextual reasoning. complexity analysis and decision procedures. formal accounts of contextual reasoning are of great importance for the#r##n#development of sophisticated artificial intelligence theory and applications.#r##n#this thesis contribution to the theory of contextual reasoning is twofold.#r##n#first, it delineates the computational complexity of contextual reasoning.#r##n#a first insight is obtained by translating contextual reasoning into a rather#r##n#simple form of reasoning in bounded modal logic. a more direct and general#r##n#understanding, as well as more refined complexity results, are established by#r##n#achieving the so-called bounded model property for contextual satisfiability.#r##n#second, the thesis describes two conceptually orthogonal approaches to#r##n#automatically deciding satisfiability in a contextual setting. firstly, the#r##n#bounded model property is exploited so as to encode contextual satisfiability#r##n#into propositional satisfiability. this approach provides for the implementation#r##n#of contextual reasoners based on existing propositional sat solvers.#r##n#subsequently, a distributed decision procedure is proposed, which maximally#r##n#exploits the potential amenity of localizing reasoning and restricting it to#r##n#relevant contexts only. the latter approach is shown to be computationally#r##n#superior to the former translation based procedure, and can be implemented#r##n#using off-the-shelf efficient reasoning procedures.
0125fd5c-2899-4468-99d2-8f9a2aba31c0 performance improvement of direct torque controlled interior permanent magnet synchronous motor drives using artificial intelligence the main theme of this paper is to present novel controller, which is a genetic based fuzzy logic controller, for interior permanent magnet synchronous motor drives with direct torque control. a radial basis function network has been used for online tuning of the genetic based fuzzy logic controller. initially different operating conditions are obtained based on motor dynamics incorporating uncertainties. at each operating condition, a genetic algorithm is used to optimize fuzzy logic parameters in closed-loop direct torque control scheme. in other words, the genetic algorithm finds optimum input and output scaling factors and optimum number of membership functions. this optimization procedure is utilized to obtain the minimum speed deviation, minimum settling time, zero steady-state error. the control scheme has been verified by simulation tests with a prototype interior permanent magnet synchronous motor.
0126ff4f-7e7f-42e4-b31e-0019050063f0 a graph-based approach to detect abnormal spatial points and regions spatial outliers are the spatial objects whose nonspatial attribute values are quite different from those of their spatial neighbors. identification of spatial outliers is an important task for data mining researchers and geographers. a number of algorithms have been developed to detect spatial anomalies in meteorological images, transportation systems, and contagious disease data. in this paper, we propose a set of graph-based algorithms to identify spatial outliers. our method first constructs a graph based on k-nearest neighbor relationship in spatial domain, assigns the differences of nonspatial attribute as edge weights, and continuously cuts high-weight edges to identify isolated points or regions that are much dissimilar to their neighboring objects. the proposed algorithms have three major advantages compared with other existing spatial outlier detection methods: accurate in detecting both point and region outliers, capable of avoiding false outliers, and capable of computing the local outlierness of an object within subgraphs. we present time complexity of the algorithms, and show experiments conducted on us housing and census data to demonstrate the effectiveness of the proposed approaches.
01277d3e-589f-431d-a62b-6c1e304f252f avoiding unintended ai behaviors artificial intelligence (ai) systems too complex for predefined environment models and actions will need to learn environment models and to choose actions that optimize some criteria. several authors have described mechanisms by which such complex systems may behave in ways not intended in their designs. this paper describes ways to avoid such unintended behavior. for hypothesized powerful ai systems that may pose a threat to humans, this paper proposes a two-stage agent architecture that avoids some known types of unintended behavior. for the first stage of the architecture this paper shows that the most probable finite stochastic program to model a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions.
01277fea-7bc8-4078-bb09-d61d4528a84a a definition of artificial intelligence in this paper we offer a formal definition of artificial intelligence and this directly gives us an algorithm for construction of this object. really, this algorithm is useless due to the combinatory explosion. #r##n#the main innovation in our definition is that it does not include the knowledge as a part of the intelligence. so according to our definition a newly born baby also is an intellect. here we differs with turing's definition which suggests that an intellect is a person with knowledge gained through the years.
0127c6f6-33c9-484f-b144-796d23b85f12 ensemble feature selection using bi-objective genetic algorithm feature selection problem in data mining is addressed here by proposing a bi-objective genetic algorithm based feature selection method. boundary region analysis of rough set theory and multivariate mutual information of information theory are used as two objective functions in the proposed work, to select only precise and informative data from the data set. data set is sampled with replacement strategy and the method is applied to determine non-dominated feature subsets from each sampled data set. finally, ensemble of such bi-objective genetic algorithm based feature selectors is developed with the help of parallel implementations to produce much generalized feature subset. in fact, individual feature selector outputs are aggregated using a novel dominance based principle to produce final feature subset. proposed work is validated using repository especially for feature selection datasets as well as on uci machine learning repository datasets and the experimental results are compared with related state of art feature selection methods to show effectiveness of the proposed ensemble feature selection method.
0128a207-088c-4669-b193-22d8798fb44c local probabilistic descriptors for image categorisation image categorisation involves the well known difficulties with different visual appearances of a single object, but also introduces the problem of within-category variation. this within-category variation makes highly distinctive local descriptors less appropriate for categorisation. difficulties because of the within category variation and clutter are tackled by modelling image fragments in a new manner. the authors propose a family of local image descriptors, called probabilistic patch descriptors (ppds). ppds encode the appearance of image fragments as well as their variability within a category. ppds extend the usual local descriptors by also modelling the variance of the descriptors' elements, for example pixels or bins in a histogram. to compare two ppds, and a ppd with an image, a new similarity measure called ppd matching score is introduced. for each object category, a set of representative ppds is learnt. images are represented as feature vectors of the best matching scores obtained for representative ppds in images. support vector machine classifiers are then trained on the feature vectors. ppds are applied to image categorisation using machine learning where the features are the matching scores between images and ppds. the authors experiment with two variants of ppds that are based on complementary local descriptors. an interesting observation is that combining the two ppd variants improves the accuracy of categorisation. experiments indicate that the benefits of modelling the within-category variation give results that are comparable with the state-of-the-art categorisation methods, and show good robustness with respect to noise and occlusions.
0128fe23-1577-4b12-b66c-b293e8f14121 colour tag design of robot soccer based on computational verb theory the vision system is the key to the robot soccer competition process. while all the information related to the environment in the competition process is obtained through the vision system identification of the colour tag design. therefore the colour tag design is essential in the robot soccer competition process. the computational verb theory is a new theory of artificial intelligence domain, it has not only made up for the insufficiency of the static image processing, but also lightened the image processing computation burden. to design an anti-jamming colour tag design according to the verb image processing characteristic and to recognize the colour tag design using the computational verb theory can not only enhance the recognition precision, but also simplify the algorithm, enhance the recognition speed, satisfies the real time of the competition. experiments show that the design has improved the stability and accuracy of vision system reduced, the preparation time for competition and achieved favorable effect in play. it has a bright future in application prospects.
012950bf-a863-4be4-b99a-9efaa0dad69a a logic of deliberation deliberation typically involves the formation of a plan or intention from a set of values and beliefs. i suggest that deliberation, or "practical reasoning," is a form of normative reasoning and that the understanding and construction of reasoning systems that can deliberate and act intentionally presupposes a theory of normative reasoning. the language and semantics of a deontic logic is used to develop a theory of defeasible reasoning in normative systems and belief systems. this theory may be applied in action theory and to artificial intelligence by identifying expressions of values, beliefs, and intentions with various types of modal sentences from the language.
01299098-2b60-4866-b076-004b31ea927a production probability estimators for context-free grammars abstract   the problem of production probability estimation is considered. we examine the estimation of production probabilities for context-free probabilistic grammars (cfpgs). given a context-free grammar,  g , and a random sample of strings from  l ( g ), we define ratio estimators to estimate the production probabilities of  g . it is shown that the statistical analysis of the estimators becomes much less complex by using the theory of random walks. the main result of the article is that the biases and variances of all ratio estimators for any chomsky normal form cfpg are approximately directly proportional to   1  n  , where  n  is the size of the random sample. random samples consisting of strings generated from an independent source are used to validate theoretical results. we show that the estimates can be used to increase the efficiency of parsing strings in a language. the ability to parse strings efficiently is extremely useful in compiler theory and the theory of artificial intelligence.
012ae65f-21e5-48d6-9962-95dd69aa8621 autonomous operations through onboard artificial intelligence the autonomous sciencecraft experiment (ase) will fly onboard the air force techsat 21 constellation of three spacecraft scheduled for launch in 2006. ase uses onboard continuous planning, robust task and goal-based execution, model-based mode identification and reconfiguration, and onboard machine learning and pattern recognition to radically increase science return by enabling intelligent downlink selection and autonomous retargeting. demonstration of these capabilities in a flight environment will open up tremendous new opportunities in planetary science, space physics, and earth science that would be unreachable without this technology.
012ae754-9b09-474a-a0ec-8290c74e5ded mining team characteristics to predict wikipedia article quality in this study, we were interested in studying which characteristics of virtual teams are good predictors for the quality of their production. the experiment involved obtaining the spanish wikipedia database dump and applying different data mining techniques suitable for large data sets to label the whole set of articles according to their quality (comparing them with the featured/good articles, or fa/ga). then we created the attributes that describe the characteristics of the team who produced the articles and using decision tree methods, we obtained the most relevant characteristics of the teams that produced fa/ga. the team's maximum efficiency and the total length of contribution are the most important predictors. this article contributes to the literature on virtual team organization.
012b0fef-9abf-425e-938a-873bf13c8672 uses of artificial intelligence in the brazilian customs fraud detection system there is an increasing concern about the control of customs operations. while globalization incentives the opening of the market, increasing amounts of imports and exports have been used to conceal several illicit activities, such as, tax evasion, smuggling, money laundry, and drug trafic. this fact makes it paramount for governments to find automatic or semi-automatic solutions to guide the customs' activities in order to minimize the number of manual inspections of goods. in this context, this paper presents an overview of some approaches developed in the harpia project that is a partnership between universities and the brazilian federal revenue for the development of computational intelligence solutions to the management of customs risk.
012b15f9-64f1-48f1-8775-d5dd23bf23eb e-commerce recommendation with personalized promotion most existing e-commerce recommender systems aim to recommend the right products to a consumer, assuming the properties of each product are fixed. however, some properties, including price discount, can be personalized to respond to each consumer's preference. this paper studies how to automatically set the price discount when recommending a product, in light of the fact that the price will often alter a consumer's purchase decision. the key to optimizing the discount is to predict consumer's willingness-to-pay (wtp), namely, the highest price a consumer is willing to pay for a product. purchase data used by traditional e-commerce recommender systems provide points below or above the decision boundary. in this paper we collected training data to better predict the decision boundary. we implement a new e-commerce mechanism adapted from laboratory lottery and auction experiments that elicit a rational customer's exact wtp for a small subset of products, and use a machine learning algorithm to predict the customer's wtp for other products. the mechanism is implemented on our own e-commerce website that leverages amazon's data and subjects recruited via mechanical turk. the experimental results suggest that this approach can help predict wtp, and boost consumer satisfaction as well as seller profit.
012b6ddb-1536-4bb5-9c37-866f63e05af2 towards predictions of the image quality of experience for augmented reality scenarios augmented reality (ar) devices are commonly head-worn to overlay context-dependent information into the field of view of the device operators. one particular scenario is the overlay of still images, either in a traditional fashion, or as spherical, i.e., immersive, content. for both media types, we evaluate the interplay of user ratings as quality of experience (qoe) with (i) the non-referential brisque objective image quality metric and (ii) human subject dry electrode eeg signals gathered with a commercial device. additionally, we employ basic machine learning approaches to assess the possibility of qoe predictions based on rudimentary subject data. corroborating prior research for the overall scenario, we find strong correlations for both approaches with user ratings as mean opinion scores, which we consider as qoe metric. in prediction scenarios based on data subsets, we find good performance for the objective metric as well as the eeg-based approach. while the objective metric can yield high qoe prediction accuracies overall, it is limited i its application for individual subjects. the subject-based eeg approach, on the other hand, enables good predictability of the qoe for both media types, but with better performance for regular content. our results can be employed in practical scenarios by content and network service providers to optimize the user experience in augmented reality scenarios.
012b71ee-e660-4290-ac8d-8957b735e485 fuzzy decision trees for dynamic data the fuzzy decision tree based approach is a very popular machine learning method that deals with imprecise and uncertain data. this approach offers a good way to handle static data. however, few works have been conducted on the use of this approach to deal with stream of data or temporal data when the training set is built incrementally time after time. to handle such kind of data brings out a number of problems for the algorithms used to construct such fuzzy decision trees. in this paper, a new approach is proposed to construct a fuzzy decision tree (fdt) when the training set is built incrementally and when training examples are provided temporally. a new measure of discrimination is defined in order to rank attributes during the process of construction of the fdt and to take into account aging of examples.
012bcb2a-6073-49fc-9430-44782bb7e363 fractures distribution modeling using fractal and multi-fractalneural network analysis in tabnak hydrocarbon field, fars, iran modeling of fractures distribution in naturally fractured hydrocarbon reservoirs is a complex process that contains large amount of uncertainty. high gas production from the dehram group of tabnak hydrocarbon reservoir in fars province indicates the presence of natural fractures. this study presents a novel methodology that integrates various features of geological, statistical and artificial intelligence techniques in a nested loop to characterize field fractures and then to model them. their characterization is, to some extent, technique dependent. secondary properties such as fracture density, fractal dimension and fractal spectrum are there defined for better description of fractures spatial distribution. irregular geometry laws showed that the connectivity of fractured media depends on power-law exponent and some fracture characteristics. a neural network is incorporated in the proposed methodology to determine these relationships, by processing field data available from the six outcrops with similar lithology, image logs and core analyses. the value of 1/82 was calculated for fractal dimension of surface fractures. the fracture density in the range of 0.2 to 1.4, fractal dimension and fractal spectrum (d(q = 0)) of fractures in the range of 1.0 to 1.6 and 1.8 to 3.0 respectively were calculated for the 10-m intervals within the well and extended with fractalneural network algorithm. determination of fractal dimension and fractal spectrum from image logs and dual application of neural network and fractal geometry in fracture modeling are innovative in this study. finally fracture distribution models estimated are of great interest with gas production rates of the tabnak hydrocarbon reservoir and have more reality results compared with dfn algorithm models using open flow software.
012c0279-323b-4bfc-881a-ca555b4ee489 data mining for fuzzy decision tree structure with a genetic program a resource manager (rm), a fuzzy logic based expert system, has been developed. the rm automatically allocates resources in real-time over many dissimilar agents. a new data mining algorithm that uses a genetic program, an algorithm that evolves other computer programs, as a data mining function has been developed to evolve fuzzy decision trees for the resource manager. it not only determines the fuzzy decision tree structure it also creates fuzzy rules while mining scenario databases. the genetic program's structure is discussed as well as the terminal set, function set, the operations of cross-over and mutation, and the construction of the database used for data mining. finally, an example of a fuzzy decision tree generated by this algorithm is discussed.
012c6262-c0c5-4172-9509-2a5561456730 rapid separation of desloratadine and related compounds in solid pharmaceutical formulation using gradient ion-pair chromatography. abstract   we reported the development of an ion-pair chromatographic method to separate desloratadine and all known related compounds in clarinex tablets, which use desloratadine as active pharmaceutical ingredient (api). for the first time, baseline separation for desloratadine and all known related compounds was achieved by utilizing a ymc-pack pro c 18  column (150 mm  4.6 mm i.d., 3 m particle size, 120 a pore size) and a gradient elution method. the mobile phase a contains 3 mm sodium dodecylsulfate (sds), 15 mm sodium citrate buffer at ph 6.2, and 40 mm sodium sulfate, while the mobile phase b is acetonitrile. chromsword  , an artificial intelligence method development tool, was used to optimize several key chromatographic parameters simultaneously including buffer ph/solvent strength, and temperature/gradient profile. the resolution of desloratadine and desloratadine 3,4-dehydropiperidine derivative, one of the critical pairs was improved by adding 40 mm sodium sulfate. ultraviolet detection at 267 nm was used to achieve the detection for desloratadine and all compounds. this method has been successfully validated according to ich guidelines in terms of linearity, accuracy, quantitation limit/detection limit, precision, specificity and robustness. it could be used as a stability indicating method for desloratadine drug substances or drug products that use desloratadine as active pharmaceutical ingredient.
012c65f1-2f02-4107-8965-c7a270d7c33a applied catalysis. a, general 
012c7e29-1c69-4821-a1c7-2d231db08cd0 reasoning about plans temporal reasoning and planning 1.1 introduction 1.1.1 background: actions as state change 1.1.2 temporally explicit representations 1.2 representing time 1.2.1 temporal logic 1.3 the logic of action 1.4 the logic for planning 1.4.1 predicting the future 1.4.2 planning with a temporal world model 1.5 the planning system 1.5.1 a nonlinear planning algorithm 1.5.2 assumptions about temporal extension 1.5.3 discussion of the algorithm 1.6 the door-latch problem 1.7 hierarchical planning 1.8 conclusions a formal theory of plan recognition and its implementation henry a. kautz 2.1 introduction 2.1.1 background 2.1.2 overview 2.1.3 plan recognition and the frame problem 2.2 representing event hierarchies 2.2.1 the language 2.2.2 representation of time, properties, and events 2.2.3 the event hierarchy 2.2.4 example: the cooking world 2.3 the formal theory of recognition 2.3.1 exhaustiveness assumptions (exa) 2.3.2 disjointness assumptions (dja) 2.3.3 component/use assumptions (cua) 2.3.4 minimum cardinality assumptions (mca) 2.3.5 example: the cooking world 2.3.6 circumscription and plan recognition 2.4 examples 2.4.1 an operating system 2.4.2 medical diagnosis 2.5 algorithms for plan recognition 2.5.1 directing inference 2.5.2 explanation graphs 2.5.3 implementing component/use assumptions 2.5.4 constraint checking 2.5.5 algorithms planning with simultaneous actions and external events richard n. pelavin 3.1 introduction 3.1.1 key considerations 3.1.2 overview 3.2 representations that treat simultaneous events 3.2.1 interval logic 3.2.2 a model based on events 3.2.3 branching time models 3.2.4 adapting the state-change model to handle simultaneous events 3.3 the semantic model 3.3.1 the interval logic structure 3.3.2 the branching time structure 3.3.3 plan instances and basic actions 3.3.4 the closeness function 3.3.5 composition and interaction of basic action instances 3.4 the language 3.4.1 the syntax: additions to interval logic 3.4.2 interpretations 3.5 planning 3.5.1 planner correctness 3.5.2 the world description 3.5.3 the action specifications 3.5.4 plan instance composition 3.5.5 persistence and maintenance plan instances 3.6 a planning algorithm 3.6.1 the input to the planning algorithm 3.6.2 the algorithm 3.6.3 planning examples 3.6.4 limitations of the planning algorithm 3.7 discussion 3.7.1 issues outside the scope of the deductive logic 3.7.2 incomplete descriptions and obtaining additional information 3.7.3 planning with an incorrect world description 3.7.4 addressing the frame problem 3.8 appendix a. the semantic model and logical language 3.8.1 the semantic model 3.8.2 the syntax 3.8.3 the interpretation of the language 3.9 appendix b. proof theory abstraction in planning josh d. tenenberg 4.1 introduction 4.1.1 inheritance abstraction 4.1.2 relaxed model abstraction 4.1.3 macro expansion 4.1.4 outline 4.1.5 strips 4.2 inheritance abstraction 4.2.1 generalizing inheritance 4.2.2 predicate mappings 4.2.3 model abstraction 4.2.4 theory abstraction 4.2.5 theory mappings 4.2.6 proof-theoretic relationship between levels 4.2.7 abstract strips systems 4.2.8 downward solution property 4.2.9 example 4.2.10 the frame problem 4.2.11 comparison with kautz 4.2.12 related research in plan generation 4.2.13 summary 4.3 abstraction using relaxed models 4.3.1 abstrips 4.3.2 planning with inconsistent systems 4.3.3 restricted abstrips 4.3.4 simple restrictions 4.3.5 relaxing the simple restrictions 4.3.6 example 4.3.7 upward solution property 4.3.8 the monotonicity property 4.3.9 search 4.3.10 related research 4.3.11 summary 4.4 conclusion 4.4.1 discrete levels 4.4.2 solution properties 4.5 proofs 4.5.1 proofs for inheritance abstraction 4.5.2 proofs for relaxed model abstraction references index
012cfe6f-0a29-47fc-9309-7d1381793694 harmonic analysis of jazz midi files using statistical parsing music and machine learning workshop, edinburgh harmonic analysis involves identifying hierarchical structure, similar to that found in the syntax and semantics of language, in the harmonic progressions underlying tonal melodies. in previous work, we have used grammar-based parsing, with related machine-learning techniques, for automatic harmonic analysis of jazz chord sequences. we now turn to the harder task of harmonic analysis of jazz midi performances using similar techniques. first, we evaluate a strict pipeline approach: we use an hmm to perform chord recognition and our previous system to analyze the output. then, we use the chord recognizer to propose a chord lattice and analyze this using an adaptation of the previous system.
012e2580-a1e8-47c6-9007-d4f1f4e0d840 learning weighted distances for relevance feedback in image retrieval we present a new method for relevance feedback in image retrieval and a scheme to learn weighted distances which can be used in combination with different relevance feedback methods. user feedback is a crucial step in image retrieval to maximise retrieval performance as was shown in recent image retrieval evaluations. machine learning is expected to be able to learn how to rank images according to users needs. most image retrieval systems incorporate user feedback using rather heuristic means and only few groups have formally investigated how to maximise the benefit from it using machine learning techniques. we incorporate our distance-learning method into our new relevance feedback scheme and into two different approaches from the literature. the methods are compared on two publicly available databases, one which is purely content-based and one which uses additional textual information. it is shown that the new relevance feedback scheme outperforms the other methods and that all methods benefit from weighted distance learning.
012ef607-57e1-4e57-8346-ae6d135ed30e a model for term selection in text categorization problems in the last ten years, automatic text categorization (tc) has been gaining an increasing interest from the research community, due to the need to organize a massive number of digital documents. following a machine learning paradigm, this paper presents a model which regards tc as a classification task supported by a wrapper approach and combines the utilization of a genetic algorithm (ga) with a filter. first, a filter is used to weigh the relevance of terms in documents. then, the top-ranked terms are grouped in several nested sets of relatively small size. these sets are explored by a ga which extracts the subset of terms that best categorize documents. experimental results on the reuters-21578 dataset state the effectiveness of the proposed model and its competitiveness with the learning approaches proposed in the tc literature.
012f55d2-e0f4-4629-b0df-926a62092ab8 learning with kernels and logical representations in this chapter, we describe a view of statistical learning in the inductive logic programming setting based on kernel methods. the relational representation of data and background knowledge are used to form a kernel function, enabling us to subsequently apply a number of kernel-based statistical learning algorithms. different representational frameworks and associated algorithms are explored in this chapter. in kernels on prolog proof trees, the representation of an example is obtained by recording the execution trace of a program expressing background knowledge. in declarative kernels, features are directly associated with mereotopological relations. finally, in kfoil, features correspond to the truth values of clauses dynamically generated by a greedy search algorithm guided by the empirical risk.
012f5f1f-eb75-426e-98ec-f0ea6100bf26 enaction-based artificial intelligence: toward co-evolution with humans in the loop this article deals with the links between the enaction paradigm and artificial intelligence. enaction is considered a metaphor for artificial intelligence, as a number of the notions which it deals with are deemed incompatible with the phenomenal field of the virtual. after explaining this stance, we shall review previous works regarding this issue in terms of artificial life and robotics. we shall focus on the lack of recognition of co-evolution at the heart of these approaches. we propose to explicitly integrate the evolution of the environment into our approach in order to refine the ontogenesis of the artificial system, and to compare it with the enaction paradigm. the growing complexity of the ontogenetic mechanisms to be activated can therefore be compensated by an interactive guidance system emanating from the environment. this proposition does not however, resolve that of the relevance of the meaning created by the machine (sense-making). such reflections lead us to integrate human interaction into this environment in order to construct relevant meaning in terms of participative artificial intelligence. this raises a number of questions with regards to setting up an enactive interaction. the article concludes by exploring a number of issues, thereby enabling us to associate current approaches with the principles of morphogenesis, guidance, the phenomenology of interactions and the use of minimal enactive interfaces in setting up experiments which will deal with the problem of artificial intelligence in a variety of enaction-based ways.
013035f0-1f24-4100-baf5-75716cb98f1c the graphic-linguistic distinctionexploring alternatives what properties, if any, distinguish graphical representations from linguistic representations? this paper looks for answers in the literature of philosophy, logic, artificial intelligence, and cognitive psychology, and extracts seven alternative binary classifications of representations that may characterize the graphic-linguistic boundary. we assess each alternative by two standards: (a) whether it extensionally fits the graphic-linguistic distinction, and (b) how far it explains the properties commonly attributed to graphic representations but not to linguistic ones.
01304b02-2ee4-4669-96ea-f74ae8136317 an artificial intelligence-based approach for simulating pedestrian movement this paper proposes a novel approach for simulating pedestrian movement behavior based on artificial intelligence technology. within this approach, a large volume of microscopic pedestrian movement behavior types were collected and encapsulated into an artificial neural network via network training. the trained network was then fed back into a simulation environment to predict the pedestrian movement. two simulation experiments were conducted to evaluate the performance of the approach. first, a pedestrian-collision-avoidance test was conducted, and the results showed that virtual pedestrians with learned pedestrian behavior can move reasonably to avoid potential collisions with other pedestrians. in addition, a critical parameter, i.e., defined as reacting distance and determined to be 2.5 m, represented the boundary of the collision buffer zone. second, a pedestrian counterflow in a road-crossing situation was simulated, and the results were compared with the real-life scenario. the comparison revealed that the pedestrian distributions, erratic trajectories, and densityspeed fundamental diagram in the simulation are reasonably consistent with the real-life scenario. furthermore, a quantitative indicator, i.e., the relative distance error, was calculated to evaluate the simulation error of pedestrians' trajectories between the simulation and the real-life scenario, the mean of which was calculated to be 0.322. this revealed that the simulation results were acceptable from an engineering perspective, and they also showed that the approach could reproduce the lane-formation phenomenon. we considered the proposed approach to be capable of simulating human-like microscopic pedestrian flow.
01307c7f-cb73-4816-9f9c-2ecdf4d42962 an efficient intrusion detection based on decision tree classifier using feature reduction large computational value has always been a restraint in processing huge network intrusion data. this problem can be extenuated through feature selection to abbreviate the size of the network data involved. in this paper, we first deal existing feature selection methods that are computationally executable for processing vast network intrusion datasets. in this paper, we study and analysis of four machine learning algorithms (j48, bayesnet, oner, nb) of data mining for the task of detecting intrusions and compare their relative performances. based on this study, it can be concluded that j48 decision tree is the most suitable associated algorithm than the other three algorithms with high true positive rate (tpr) and low false positive rate (ftr) and low computation time with high accuracy.
01318de3-a979-4d5c-a1ea-1fa0a527f1ee novel super resolution restoration of remote sensing images based on compressive sensing and example patches-aided dictionary learning a novel machine learning and compressive sensing (cs) based super-resolution (sr) algorithm for the restoration of remote sensing images is proposed in this paper. this new algorithm relies on the idea that high-resolution (hr) image patches can be correctly recovered from the downsampled low- resolution (lr) image patches under two mild conditions, i.e., the sparsity of image patches, and the incoherence between the sensing and projection matrix. consequently if most of hr image patches can be represented as a sparse linear combination of elements from a dictionary that is incoherent with sensing matrix, the hr image patches can be recovered accurately from its lr version. to find a dictionary which can sparsely represent hr image patches to guarantee the reconstruction error over a set of patches be minimal, an example patches-aided dictionary learning algorithm named ksvd algorithm is adopted. moreover, the incoherence between the learned dictionary and sensing matrix is experimentally investigated. the new proposed method is tested on the restoration of remote sensing images came from usc-sipi image database, and the results show that the proposed algorithm can provide substantial improvement in resolution of remote sensing images, and the restored images are superior in quality to that of other related methods.
0131d4e1-0bf0-4b96-8259-9e0ffe3581ed common feature extraction in multi-source domains for transfer learning in transfer learning scenarios, finding a common feature representation is crucial to tackle the problem of domain shift where the training (source domain) and test (target domain) sets have difference in their distribution. however, classical dimensionality reduction approaches such as fisher discriminant analysis (fda), are not in good yields whenever dealing with shift problem. in this paper we introduce comut, a common feature extraction in multi-source domains for transfer learning, that finds a common feature representation between different source and target domains. comut projects the data into a latent space to reduce the drift in distributions across domains and concurrently preserves the separability between classes. comut constructs the latent space in semi-supervised manner to bridge across domains and relate the different domains to each other. the projected domains have distribution similarity and classical machine learning methods can be applied on them to classify target data. empirical results indicate that comut outperforms other dimensionality reduction methods on different artificial and real datasets.
013277eb-ac8a-4b3d-a957-6679cf9219e8 facility detection and popularity assessment from text classification of social media and crowdsourced data advances in technology have continually progressed our understanding of where people are, how they use the environment around them, and why they are at their current location. having a better knowledge of when various locations become popular through space and time could have large impacts on research fields like urban dynamics and energy consumption. in this paper, we discuss the ability to identify and locate various facility types (e.g. restaurant, airport, stadiums) using social media, and assess methods in determining when these facilities become popular over time. we use standard natural language processing tools and machine learning classifiers to interpret geotagged twitter text and determine if a user is seemingly at a location of interest when the tweet was sent. on average our classifiers are approximately 85% accurate varying across multiple facility types, with a peak precision of 98%. by using these standard methods to classify unstructured text, geotagged social media data can be an extremely useful tool to better understanding the composition of places and how and when people use them.
01351ada-44ec-450c-b42a-107f9cc78192 introducing the ia-64 architecture microprocessors continue on the relentless path to provide more performance. every new innovation in computing-distributed computing on the internet, data mining, java programming, and multimedia data streams-requires more cycles and computing power. even traditional applications such as databases and numerically intensive codes present increasing problem sizes that drive demand for higher performance. design innovations, compiler technology, manufacturing process improvements, and integrated circuit advances have been driving exponential performance increases in microprocessors. to continue this growth in the future, hewlett packard and intel architects examined barriers in contemporary designs and found that instruction-level parallelism (ilp) can be exploited for further performance increases. this article examines the motivation, operation, and benefits of the major features of ia-64. intel's ia-64 manual provides a complete specification of the ia-64 architecture.
01351eef-7cbc-41c7-9b64-7985c72b5d42 recent advances in electronic tongues this minireview describes the main developments of electronic tongues (e-tongues) and taste sensors in recent years, with a summary of the principles of detection and materials used in the sensing units. e-tongues are sensor arrays capable of distinguishing very similar liquids employing the concept of global selectivity, where the difference in the electrical response of different materials serves as a fingerprint for the analysed sample. they have been widely used for the analysis of wines, fruit juices, coffee, milk and beverages, in addition to the detection of trace amounts of impurities or pollutants in waters. among the various principles of detection, electrochemical measurements and impedance spectroscopy are the most prominent. with regard to the materials for the sensing units, in most cases use is made of ultrathin films produced in a layer-by-layer fashion to yield higher sensitivity with the advantage of control of the film molecular architecture. the concept of e-tongues has been extended to biosensing by using sensing units capable of molecular recognition, as in films with immobilized antigens or enzymes with specific recognition for clinical diagnosis. because the identification of samples is basically a classification task, there has been a trend to use artificial intelligence and information visualization methods to enhance the performance of e-tongues.
01354fdd-d67c-41ff-a59c-4b799d7b3897 exploring arduino for building educational context-aware recommender systems that deliver affective recommendations in social ubiquitous networking environments one of the most challenging context features to detect when making recommendations in educational scenarios is the learner's affective state. usu- ally, this feature is explicitly gathered from the learner herself through ques- tionnaires or self-reports. in this paper, we analyze if affective recommendations can be produced with a low cost approach using the open source electronics prototyping platform arduino together with corresponding sensors and actua- tors. tormes methodology (which combines user centered design methods and data mining techniques) can support the recommendations elicitation pro- cess by identifying new recommendation opportunities in these emerging social ubiquitous networking scenarios.
0136cc30-453a-4f02-bf95-9285b9001163 aplicaciones de data mining al estudio de la biodiversidad en relevamientos metagenomicos el trabajo aqui presentado trata acerca de las mediciones de biodiversidad en comunidades microbianas que suelen involucrar dos aspectos: la riqueza y la distribucion de los taxones. una metodologia usual para estudiar esas comunidades comprende la utilizacion de genes marcadores, tal como el que codifica para el rrna 16s. se [...]
0136dd2a-5989-47b6-9acc-31d7d8c56220 a survey of interestingness measures for knowledge discovery it is a well-known fact that the data mining process can generate many hundreds and often thousands of patterns from data. the task for the data miner then becomes one of determining the most useful patterns from those that are trivial or are already well known to the organization. it is therefore necessary to filter out those patterns through the use of some measure of the patterns actual worth. this article presents a review of the available literature on the various measures devised for evaluating and ranking the discovered patterns produced by the data mining process. these so-called interestingness measures are generally divided into two categories: objective measures based on the statistical strengths or properties of the discovered patterns and subjective measures that are derived from the user's beliefs or expectations of their particular problem domain. we evaluate the strengths and weaknesses of the various interestingness measures with respect to the level of user integration within the discovery process.
0136e9ba-8de8-4cbd-8347-2a9b1e3c5f39 prediction of mechanical properties of csp using greedy unsupervised pre-trained bp neural network a bp neural network based on greedy unsupervised pre-training is proposed to improve the prediction accuracy of mechanical properties in hot-rolled strips produced by thin slab casting and rolling (csp) technique. with this method, unlabeled data are used for greedy pre-training. after the training process, trained weights and bias are used as the initial weights and bias of the bp neural network. the advantage of this method is that it not only can make full use of unlabeled data, but also improves the prediction accuracy of bp neural network. the measured ones showed that compared with bp neural network, prediction accuracy of mechanical properties with the method of greedy unsupervised pre-training bp neural network is improved about 2.2%, 4.4% and 4.2%. compared with support vector machine, prediction accuracy of mechanical properties with the method of greedy unsupervised pre-training bp neural network is improved about 0.4%, 2.6% and 0.4%. it is suitable for on-line mechanical property prediction of thin slab casting and rolling.
013792fd-07a2-4cd3-91d1-b67a502c5b36 artificial intelligence in the prediction of operative findings in low back surgery in a prospective trial of 150 patients undergoing first time low back surgery for sciatica, the performance of a computer was compared to that of clinicians in predicting the likely operative findings. the results indicate that artificial intelligence techniques implemented on a computer can be used for predicting operative findings in low back surgery, can out-perform clinicians and can be used to develop better methods of human prediction.
0137a9f4-c325-4b69-8146-8ef4e04dc369 olindda: a cluster-based approach for detecting novelty and concept drift in data streams a machine learning approach that is capable of treating data streams presents new challenges and enables the analysis of a variety of real problems in which concepts change over time. in this scenario, the ability to identify novel concepts as well as to deal with concept drift are two important attributes. this paper presents a technique based on the k-means clustering algorithm aimed at considering those two situations in a single learning strategy. experimental results performed with data from various domains provide insight into how clustering algorithms can be used for the discovery of new concepts in streams of data.
0137d80d-97e9-4aec-95d6-4712468ffd08 systematics of the subfamily danioninae (teleostei: cypriniformes: cyprinidae). the members of the cyprinid subfamily danioninae form a diverse and scientifically important group of fishes, which includes the zebrafish,  danio rerio . the diversity of this assemblage has attracted much scientific interest but its monophyly and the relationships among its members are poorly understood. the phylogenetic relationships of the danioninae are examined herein using sequence data from mitochondrial cytochrome  b , mitochondrial cytochrome  c  oxidase i, nuclear opsin, and nuclear recombination activating gene 1. a combined data matrix of 4117 bp for 270 taxa was compiled and analyzed. the resulting topology supports some conclusions drawn by recent studies on the group and certain portions of the traditional classification, but our results also contradict key aspects of the traditional classification. the subfamily danioninae is not monophyletic, with putative members scattered throughout cyprinidae. therefore, we restrict danioninae to the monophyletic group that includes the following genera:  amblypharyngodon ,  barilius ,  cabdio ,  chela ,  chelaethiops ,  danio ,  danionella ,  devario  (including  inlecypris ),  esomus ,  horadandia ,  laubuca ,  leptocypris ,  luciosoma ,  malayochela ,  microdevario ,  microrasbora ,  nematabramis ,  neobola ,  opsaridium ,  opsarius ,  paedocypris ,  pectenocypris ,  raiamas ,  rasbora  (including  boraras  and  trigonostigma ),  rasboroides ,  salmostoma ,  securicula , and  sundadanio . this danioninae  sensu stricto  is divided into three major lineages, the tribes chedrini, danionini, and rasborini, where chedrini is sister to a danioninirasborini clade. each of these tribes is monophyletic, following the restriction of danioninae. the tribe chedrini includes a clade of exclusively african species and contains several genera of uncertain monophyly ( opsarius ,  raiamas ,  salmostoma ). within the tribe rasborini, the species-rich genus  rasbora  is rendered non-monophyletic by the placement of two monophyletic genera,  boraras  and  trigonostigma , hence we synonymize those two genera with  rasbora . in the tribe danionini, the miniature genus  danionella  is recovered as the sister group of  danio , with  d. nigrofasciatus  sister to  d. rerio .
0137ee7b-a6e0-438a-8e76-b68842873d71 student's uncertainty modeling through a multimodal sensor-based approach detecting the student internal state during learning is a key construct in educational environment and particularly in intelligent tutoring systems (its). students uncertainty is of primary interest as it is deeply rooted in the process of knowledge construction. in this paper we propose a new sensor-based multimodal approach to model users uncertainty from their affective reactions and cognitive and personal characteristics. an experimental protocol was conducted to record participants brain activity and physiological signals while they interacted with a computer-based problem solving system and self-reported their perceived level of uncertainty during the tasks. we study key indicators from affective reactions, trait-questionnaire responses, and individual differences that are related to uncertainty states. then we develop models to automatically predict levels of uncertainty using machine learning techniques. evidence indicated that students uncertainty is associated to their mental and emotional reactions. personal characteristics such as gender, skill level, and personality traits also showed a priori tendencies to be more or less in particular uncertainty states. the svm algorithm demonstrated the best accuracy results for classifying students uncertainty levels. our findings have implications for its seeking to continuously monitor users internal states so they can ultimately provide efficient interventions to enhance learning.
0138e9d7-f459-487b-bc12-af4ffda191b1 the satellite optimization design using collaborative optimization method based on normal cloud model collaborative optimization is a good method for solving multidisciplinary optimization design of the complex system. satellite design is a complex and systematic project, the efficiency of the traditional optimization design methods are not efficiently. recently cloud computing has been widely used in intelligent control, data mining, optimization computation and the other fields. the normal cloud model based on the normal distribution is an important cloud model for cloud computing and has been a wide range of adaptability. in this paper, the normal cloud model is in combination with collaborative optimization. with the structure of the collaborative optimization, the velocity and direction of optimized search can be controlled by the cloud droplets generated by the normal cloud model. numerical results indicate the high quality of the algorithm on precision stability and convergence rate and improve the efficiency of the satellite optimization design.
013926ed-3aa2-48ad-ba68-7445b42bd92e a hybrid fuzzy-firefly approach for rule-based classification pattern classification algorithms have been applied in data mining and signal processing  to extract the knowledge from data in a wide range of applications. the fuzzy inference systems have successfully been used to extract rules in rule-based applications. in this paper, a novel hybrid methodology using: (i) fuzzy logic (in form of if-then rules) and (ii) a bio-inspired optimization technique (firefly algorithm) is proposed to improve performance and accuracy of classification task. experiments are done using nine standard data sets in uci machine learning repository. the results show that overall the accuracy and performance of our classification are better or very competitive compared to others reported in literature.
013ae3c7-c0da-4335-8d09-13c4d42634ac a general 13c nmr spectrum predictor using data mining techniques. abstract a general-case neural network model for 13c nmr spectrum prediction (estimation) was built from more than 8,300 carbon atoms having various environments. building the model from the data set required a few weeks' work using commercial software. average deviation on test data is ca. 4 ppm. there is no limit on molecule complexity. estimation error does not depend on molecule size or complexity. the emphasis is on the data, the method and the results, not on the processes that take place inside the modelling software. advantages, disadvantages and peculiarities of neural network-based data modelling (data mining) are described at length. the differences in data handling between the data mining approach and traditional statistical modelling techniques are discussed and illustrated in detail. the spectrum predictor is available from pmsi at no charge.
013aebf2-b43e-461c-bf6f-85f946eef9d4 recognizing human actions in the motion trajectories of shapes people naturally anthropomorphize the movement of nonliving objects, as social psychologists fritz heider and marianne simmel demonstrated in their influential 1944 research study. when they asked participants to narrate an animated film of two triangles and a circle moving in and around a box, participants described the shapes' movement in terms of human actions. using a framework for authoring and annotating animations in the style of heider and simmel, we established new crowdsourced datasets where the motion trajectories of animated shapes are labeled according to the actions they depict. we applied two machine learning approaches, a spatial-temporal bag-of-words model and a recurrent neural network, to the task of automatically recognizing actions in these datasets. our best results outperformed a majority baseline and showed similarity to human performance, which encourages further use of these datasets for modeling perception from motion trajectories. future progress on simulating human-like motion perception will require models that integrate motion information with top-down contextual knowledge.
013b3c54-f86f-48c2-bc97-a88eebb8217f exploring spatial coherence in inter-annual changes and annual extremes of rainfall over india forecasts of monsoon rainfall for india are made at national scale. but there is spatial coherence and heterogeneity that is relevant to forecasting. this paper considers year-to-year rainfall change and annual extremes at sub-national scales. we use data mining techniques to gridded rain-gauge data for 1901-2011 to characterize coherence and heterogeneity and identify spatially homogeneous clusters. we study the direction of change in rainfall between years (phase), and extreme annual rainfall at both grid level and national level. grid-level phase is found to be spatially coherent, and significantly correlated with all-india mean rainfall (aimr) phase. grid-level extreme-rainfall years are not strongly associated with corresponding extremes in aimr, although in extreme aimr years local extremes of the same type occur with higher spatial coherence. years of extremes in aimr entail widespread phase of the corresponding sign. furthermore, local extremes and phase are found to frequently co-occur in spatially contiguous clusters.
013b51fd-480e-4775-9061-f554a25c1236 behavior analysis based on coordinates of body tags this paper describes fall detection, activity recognition and the detection of anomalous gait in the confidence project. the project aims to prolong the independence of the elderly by detecting falls and other types of behavior indicating a health problem. the behavior will be analyzed based on the coordinates of tags worn on the body. the coordinates will be detected with radio sensors. we describe two confidence modules. the first one classifies the user's activity into one of six classes, including falling. the second one detects walking anomalies, such as limping, dizziness and hemiplegia. the walking analysis can automatically adapt to each person by using only the examples of normal walking of that person. both modules employ machine learning: the paper focuses on the features they use and the effect of tag placement and sensor noise on the classification accuracy. four tags were enough for activity recognition accuracy of over 93% at moderate sensor noise, while six were needed to detect walking anomalies with the accuracy of over 90%.
013b7a39-22cd-4fc9-a5ad-1e5da3a67f70 mathematical models used for adaptive control of machine tools abstract   many adaptive control (ac) systems have been developed in the laboratory, but despite this research effort, few ac systems have been applied in industrial settings in the past 25 years. the failure of ac systems for machine tools has been at an enormous cost to automated factories. there are a number of flexible manufacturing systems (fms) in the united states that are in operation less than 10 percent of the time. such systems are sometimes said to be controlled by computers and operated by humans. one of the reasons for the failures of machine tool ac is the lack of appropriate mathematical models of the metal cutting processes. metal cutting processes are stochastic, nonlinear and ill-defined. this paper presents a review of the literature spanning the past 25 years which discusses the mathematical models used for the most common metal cutting processes used in machine tool ac systems. it is well recognized that modeling is a critical problem, and this paper imparts the evolution of the modeling approaches taken. models used to implement the control are discussed as well. the paper will begin with a brief explanation of adaptive control; this is necessary since the term is used one way by control engineers and another way by manufacturing engineers. classification of ac systems into adaptive control constraint (acc) and adaptive control optimization (aco) categories is explained. however, this paper defines categories consistent with control theoretic definitions and the literature is reviewed within this context. two categorizations for the type of metal cutting model used are defined. early research efforts are presented, followed by the attempts for industrial application. problems encountered in the mechanics and methodology are discussed. after the review, the authors present their two unique approaches to adaptive control of machine tools. one approach is to develop a state space model. the model presented is for semi-orthogonal metal cutting on a lathe. the second approach is a model which includes characteristics of an actual human operator. this approach is based on fuzzy logic control and artificial intelligence to simulate the human operator.
013bac76-35bb-4e09-99f8-ab4da9920585 neighborhood correlation analysis for semi-paired two-view data canonical correlation analysis (cca) is a widely used technique for analyzing two datasets (two views of the same objects). however, cca needs that the samples of the two views are fully-paired. actually, we are often faced up with the semi-paired scenario where the number of available paired samples is limited and yet the number of unpaired samples is sufficient. for such a scenario, cca is generally prone to overfitting and thus performs poorly, since its definition itself makes it only able to utilize those paired samples. to overcome such a shortcoming, several semi-paired variants of cca have been proposed. however, unpaired samples in these methods are just used in the way of single-view leaning to capture individual views' structure information for regularizing cca. intuitively, using unpaired samples in the way of two-view learning should be more natural and more attractive since cca itself is a two-view learning method. as a result, a novel ccas semi-paired variant named neighborhood correlation analysis (neca), which uses unpaired samples in the two-view learning way, is developed through incorporating between-view neighborhood relationships into cca. the relationships are acquired through leveraging within-view neighborhood relationships of each view's all data (including paired and unpaired data) and between-view paired information. thus, it can take more sufficient advantage of the unpaired samples and then mitigate overfitting effectively caused by the limited paired data. promising experiments results on several popular multi-view datasets show its feasibility and effectiveness.
013be2bf-caa7-4aa4-a287-d3ce8d9d029e analysing decision variables that influence preliminary feasibility studies using data mining techniques the development of infrastructure contributes to the social and economic improvement of a country, and generally requires huge and immediate investments. to decide on appropriate infrastructure projects, many countries use preliminary feasibility studies (pfs). however, a preliminary feasibility study takes a relatively long time to complete. during this time, decision-making parameters such as the estimated project cost as well as the project environment may change. to identify the decision parameters that affect the feasibility analysis, data mining techniques are applied to analyse the go/no go decision-making process in infrastructure projects. the data mining analysis uses pfs data obtained from large-scale infrastructure projects in korea. classification models found 57 hidden rules in the pfs. prediction models were also developed for go/no go decision making using an artificial neural network (ann) and logistic regression analysis. in order to validate the results, the study evaluated the accuracies and errors of both the classification and the prediction model.
013c64da-0aa8-4f31-9dd9-2962a08dea7a new directions in biomedical text annotation: definitions, guidelines and corpus construction background#r##n#while biomedical text mining is emerging as an important research area, practical results have proven difficult to achieve. we believe that an important first step towards more accurate text-mining lies in the ability to identify and characterize text that satisfies various types of information needs. we report here the results of our inquiry into properties of scientific text that have sufficient generality to transcend the confines of a narrow subject area, while supporting practical mining of text for factual information. our ultimate goal is to annotate a significant corpus of biomedical text and train machine learning methods to automatically categorize such text along certain dimensions that we have defined.
013ce63b-a745-4cc8-ad73-79a7dea913ec the future of intelligent interfaces: not just how?, but what? and why? 
013cf80c-c902-45f4-b528-254d31f79375 techniques for textual document indexing and retrieval via knowledge sources and data mining 
013ea383-a90f-4a39-8b4f-4fe6e5720c2c gpu accelerated array queries: the good, the bad, and the promising 
013f3789-28c4-491b-80b1-1bb42181022a approximate representation and processing of arbitrary correlation structures for correlation handling in databases implementations of the present disclosure include receiving user input, the user input indicating a distribution type and a correlation factor, providing the distribution type and correlation factor for identifying an approximate correlation representation (acr) histogram from a plurality of acr histograms based on the distribution type and the correlation factor, receiving the acr histogram, retrieving a first distribution associated with a first uncertain value and a second distribution associated with a second uncertain value from computer-readable memory, processing the acr histogram, the first distribution and the second distribution to generate a correlation histogram that represents a correlation between the first uncertain value and the second uncertain value, and displaying the correlation histogram on a display.
013fd5de-8864-4de7-bb33-1958df61ce04 privacy-preserving classification of customer data without loss of accuracy. privacy has become an increasingly important issue in data mining. in this paper, we consider a scenario in which a data miner surveys a large number of customers to learn classiflcation rules on their data, while the sensitive attributes of these customers need to be protected. solutions have been proposed to address this problem using randomization techniques. such solutions exhibit a tradeofi of accuracy and privacy: the more each customers private information is protected, the less accurate result the miner obtains; conversely, the more accurate the result, the less privacy for the customers. in this paper, we propose a simple cryptographic approach that is ecient even in a many-customer setting, provides strong privacy for each customer, and does not lose any accuracy as the cost of privacy. our key technical contribution is a privacy-preserving method that allows a data miner to compute frequencies of values or tuples of values in the customers data, without revealing the privacy-sensitive part of the data. unlike general-purpose cryptographic protocols, this method requires no interaction between customers, and each customer only needs to send a single ow of communication to the data miner. however, we are still able to ensure that nothing about the sensitive data beyond the desired frequencies is revealed to the data miner. to illustrate the power of our approach, we use our frequency mining computation to obtain a privacypreserving naive bayes classifler learning algorithm. initial experimental results demonstrate the practical eciency of our solution. we also suggest some other applications of privacy-preserving frequency mining.
013fe4f0-994f-477d-a7a2-251f8bc8851d prediction of informative regions in medical text using machine learning techniques machine learning is the science of getting computers to act without being explicitly programmed. machine learning is gradually making its way into medical domain and has become the most reliable and accurate tool. this paper provides an overview of the development of intelligent data analysis in medical data (medline) using a machine learning perspective. medical data contains information about the nature of disease and the effectiveness of treatments. the process of identifying and disseminating the disease and treatment related to a sentence from the medline abstracts is a difficult task. in this paper we aim at investigating the performance of supervised learning algorithms in order to develop an application that can accurately identify and disseminate the healthcare information that can be utilized by both healthcare providers and patients. this is the first step towards achieving reliable/accurate disease and treatment related information which is useful for better understanding.
0141486e-7f82-42c7-bffc-db8f371dbe3f intelligent scheduling with machine learning capabilities: the induction of scheduling knowledge abstract dynamic scheduling of manufacturing systems has primarily involved the use of dispatching rules. in the context of conventional job shops, the relative performance of these rules has been found to depend upon the system attributes, and no single rule is dominant across all possible scenarios. this indicates die need for developing a scheduling approach which adopts a state-dependent dispatching rule selection policy. the importance of adapting the dispatching rule employed to the current state of the system is even more critical in a flexible manufacturing system because of alternative machine routing possibilities and me need for increased coordination among various machines. this study develops a framework for incorporating machine learning capabilities in intelligent scheduling. a pattern-directed method, with a built-in inductive learning module, is developed for heuristic acquisition and refinement. this method enables the scheduler to classify distinct manufacturing patterns and to generate...
01427893-52e6-480c-9e5a-0a6d940848e6 protchew: automatic extraction of protein names from biomedical literature with the increasing amount of biomedical literature, there is a need for automatic extraction of information to support biomedical researchers. due to incomplete biomedical information databases, the extraction is not straightforward using dictionaries, and several approaches using contextual rules and machine learning have previously been proposed. our work is inspired by the previous approaches, but is novel in the sense that it is fully automatic and doesnt rely on expert tagged corpora. the main ideas are 1) unigram tagging of corpora using known protein names for training examples for the protein name extraction classi- fier and 2) tight positive and negative examples by having protein-related words as negative examples and protein names/synonyms as positive examples. we present preliminary results on medline abstracts about gastrin, further work will be on testing the approach on biocreative benchmark data sets.
01427ada-3a8c-44cd-a703-127dcf0802ee teleological argumentation to and from motives this paper uses tools from argumentation and artificial intelligence to build a system to analyze reasoning from a motive to an action and reasoning from circumstantial evidence of actions to a motive. the tools include argument mapping, argumentation schemes, inference to the best explanation, and a hybrid method of combining argument and explanation. several examples of use of relevant motive evidence in law are used to illustrate how the system works. it is shown how adjudicating cases where motive of evidence is relevant depends on a balance of argumentation that can be tilted to one side or the other using plausible reasoning that combines arguments and explanations.
01428852-d759-45f4-9423-80c514382a14 ontological uncertainty and design: requirements for creative machines this paper proposes that a particular type of uncertainty, ontological uncertainty, is what distinguishes creative design from other activities that might be considered design, and that much of the work in design methods and design computation is misplaced in the latter due to widely held notions of what computation is. to illustrate the process of design, an example is given of a real instance of creative design, antony gormleys body expansion sculptures, from which the thesis is made that ontological change is the key to creative thought. this thesis is then used to address a classic argument against artificial intelligence: john searles chinese room. it concludes that certain types of computation provide the capability of machines to change ontologies, and therefore participate in creative design.
0142beee-a936-47c1-b329-779e45b203bb a comparative analysis of outlier mining techniques with emphasis on density based technique: local outlier factor outlier can be termed as an observation that does not follow the normal characteristics of the system, hence it is very far from other data points in the system. this property of outlier makes it very suspicions that it may be generated by a different mechanism followed by other data points. the identification of outliers in the area of data mining can lead to very useful and meaningful knowledge and can be used in practical applications areas such as public safety, public health, climatology, and financial services. in many areas of kdd applications such as frauds in e-commerce the identification and analysis of these abnormal data points is more important than normal data points. there are different approaches for outlier detection; one of them is density based approach. in this paper we provide a detailed comparison of density based outlier finding algorithms such as lof and its variants lof' and lof". section ii discusses the density based approach and lof. section iii discusses the improvement on lof, lof' and lof". a complexity comparison is given section iv. finally, section v concludes with a summary of those outlier detection algorithms. keywords- outlier, local reachability density, density based approach.
0142c967-77af-4e57-af79-f2d60fa46ae9 density-based clustering of polygons clustering is an important task in spatial data mining and spatial analysis. we propose a clustering algorithm p-dbscan to cluster polygons in space. p-dbscan is based on the well established density-based clustering algorithm dbscan. in order to cluster polygons, we incorporate their topological and spatial properties in the process of clustering by using a distance function customized for the polygon space. the objective of our clustering algorithm is to produce spatially compact clusters. we measure the compactness of the clusters produced using p-dbscan and compare it with the clusters formed using dbscan, using the schwartzberg index. we measure the effectiveness and robustness of our algorithm using a synthetic dataset and two real datasets. results show that the clusters produced using p-dbscan have a lower compactness index (hence more compact) than dbscan.
01440659-4992-4526-afa9-dc6b5a9652ab automatic acoustic siren detection in traffic noise by part-based models state-of-the-art classifiers like hidden markov models (hmms) in combination with mel-frequency cepstral coefficients (mfccs) are flexible in time but rigid in the spectral dimension. in contrast, part-based models (pbms) originally proposed in computer vision consist of parts in a fully deformable configuration. the present contribution proposes to employ pbms in the spectro-temporal domain for detection of emergency siren sounds in traffic noise,standard generative training resulting in a classifier that is robust to shifts in frequency induced, e.g., by doppler-shift effects. two improvements over standard machine learning techniques for pbm estimation are proposed: (i) spectro-temporal part (appearance) extraction is initialized by interest point detection instead of random initialization and (ii) a discriminative training approach in addition to standard generative training is implemented. evaluation with self-recorded police sirens and traffic noise gathered on-line demonstrates that pbms are successful in acoustic siren detection. one hand-labeled and two machine learned pbms are compared to standard hmms employing mel-spectrograms and mfccs in clean and multi condition (multiple snr) training settings. results show that in clean condition training, hand-labeled pbms and hmms outperform machine-learned pbms already for test data with moderate additive noise. in multi condition training, the machine learned pbms outperform hmms on most snrs, achieving high accuracies and being nearly optimal up to 5 db snr. thus, our simulation results show that pbms are a promising approach for acoustic event detection (aed).
0144af83-4ea1-4622-9cef-c83cad7de789 a constructive and autonomous integration scheme of low-cost gps/mems imu for land vehicular navigation applications the integration of gps and ins provides a system that has superior performance in comparison with either a gps or an ins stand-alone systems. most integrated gps/ins positioning systems have been implemented using kalman filter (kf) technique. although of being widely used, kf has some drawbacks related to computation load, immunity to noise effects and observability. in addition, kf only works well under certain predefined error models and provides accurate estimation of ins errors only during the availability of gps signal. upon losing the gps signals, if the inertial sensor errors do not have an accurate stochastic model, kalman filter delivers poor prediction of ins errors, and thus a considerable increase in position errors may be observed. the impact of these limitations affects the integrated system positional accuracy during gps signal outages. recently, the field of artificial intelligence has been receiving more attention in the development of alternative gps/ins integration schemes. therefore, in this paper, an alternative scheme is proposed which implements a constructive neural network (cnn). the proposed scheme has flexible topology when compared to the recently utilized multi-layer feed-forward neural networks (mfnns). the topologies of mfnn-based schemes are decided empirically with intensive training efforts and they remain fixed during navigation. in contrast, the proposed cnn scheme can adjust its architecture (i.e. the number of hidden neurons) autonomously during navigation based on the complexity of the problem in hand (i.e. dynamic variations) without the need for human intervention. the proposed scheme is implemented and tested using mems imu data collected in land-vehicle environment. it does not require prior knowledge or empirical trials to implement the proposed architecture since it is able to adjust its architecture "on the fly" based on the complexity of the vehicle dynamic variations. this is a significant improvement compared to the previously developed mfnn scheme that requires extensive empirical trials. in addition, the proposed cnn architecture remains fixed after the final design. the proposed scheme performance is compared to both mfnn and kf during several gps signal outages. the results of all schemes are then analyzed and discussed.
0144b527-d397-4f18-ad74-e47f04a4fd1f bond graph based bayesian network for fault diagnosis model-based fault diagnosis using artificial intelligence techniques often deals with uncertain knowledge and incomplete information. probability reasoning is a method to deal with uncertain or incomplete information, and bayesian network is a tool that brings it into the real world application. a novel approach for constructing the bayesian network structure on the basis of a bond graph model is proposed. specification of prior and conditional probability distributions (cpds) for the bayesian network can be completed by expert knowledge and learning from historical data. the resulting bayesian network is then applied for diagnosing faulty components from physical systems. the performance of the proposed fault diagnosis scheme based on bond graph derived bayesian network is demonstrated through simulation studies.
0145115a-5a9a-480b-a126-0b3059834e24 fractal characterization by frequency analysis. ii: a new method summary#r##n##r##n#a new frequency analysis method, fractal analysis by circular average (faca), and an image replication procedure are proposed that together produce accurate measurements of the fractal dimension of surfaces and profiles, eliminating fourier transform artefacts which arise from the lack of periodic continuity in real surfaces and profiles.
0145975e-ca80-442f-b09c-9235e03205bd prioritizing travel time reports in peer-to-peer traffic dissemination vehicular ad-hoc networks (vanets) is a promising approach to the dissemination of spatio-temporal information such as the current traffic condition of a road segment or the availability of a parking space. due to the constraint of the communication bandwidth, only a limited number of information items may be transmitted upon a vehicle-to-vehicle communication opportunity. ranking becomes critical in this situation, by enabling the most important information to be transmitted under the bandwidth constraint. in this paper we propose a method for online learning of spatio-temporal information ranking for a travel time dissemination application within a vanet. in this method, vehicles judge the relevance of incoming information items and use them as training examples for naive bayesian learning. additionally, a separate machine learning algorithm is used to estimate the probability of a duplicate item being transmitted. the method is used in place of commonly used heuristics, and is shown to be superior in the application of travel time dissemination.
01464f5a-01b7-4f88-88fd-964a2055eba4 a review on mobile threats and machine learning based detection approaches the research of mobile threats detection using machine learning algorithms have got much attention in recent years due to increase of attacks. in this paper, mobile vulnerabilities were examined based on attack types. in order to prevent or detect these attacks machine learning methods used were analyzed and papers published in between 2009 and 2014 have been evaluated. most important mobile vulnerabilities implementation format for these threats, detection methods and prevention approaches with the help of machine learning algorithms are presented. the obtained results are compared from their achievements were summarized. the results have shown that selecting and using datasets play an important role on the success of the system. additionally, supervised learning techniques produce better results while compared with unsupervised ones in intrusion detection.
0146d892-f046-46f3-a37b-944caecaa7e0 study on grafting informationization onto advanced railway traffic equipment manufacturing quality management advanced railway traffic equipment manufacturing is a synthesized and complicated big production activity, which needs bulkiness and high content technology. the amalgamation phase model (between it and enterprise) was given. quality management actuality was analyzed such as the original and behindhand management method can not realize the real dynamic quality management. then the system design was given, which used the b/s structure, data mining, dynamic quality management and network system structure. meanwhile, the quality management system was developed, which has construction log management subsystems. finally, information technology was given. the practicability, validity and economy of the system were proved through the application in an advanced railway traffic equipment manufacturing company.
0146f630-4959-47af-98b5-8248982975a0 supervised learning approach to remote heart rate estimation from facial videos a supervised machine learning approach to remote video-based heart rate (hr) estimation is proposed. we demonstrate the possibility of training a discriminative statistical model to estimate the blood volume pulse signal (bvp) from the human face using ambient light and any off-the-shelf webcam. the proposed algorithm is 120 times faster than state of the art approach and returns a confidence metric to evaluate the hr estimates plausibility. the algorithm was evaluated against the state-of-the-art on 120 minutes of face videos, the largest video-based heart rate evaluation to date. the evaluation results showed a 53% decrease in the root mean squared error (rmse) compared to state-of-the-art.
01475c0e-7b64-4412-a398-31aef40462b5 neural network approximation and estimation of functions approximation and estimation bounds were obtained by barron (see proc. of the 7th yale workshop on adaptive and learning systems, 1992, ieee transactions on information theory, vol.39, pp.930-944, 1993 and machine learning, vol.14, p.113-143, 1994) for function estimation by single hidden-layer neural nets. this paper highlights the extension of his results to the two hidden-layer case. the bounds derived for the two hidden-layer case depend on the number of nodes t/sub 1/ and t/sub 2/ in each hidden-layer, and also on the sample size n. it is seen from our bounds that in some cases, an exponentially large number of nodes, and hence parameters, is not required.
01476f4f-bdc7-4ae6-a738-a9b56cbd15a4 a unified semi-supervised dimensionality reduction framework for manifold learning we present a general framework of semi-supervised dimensionality reduction for manifold learning which naturally generalizes existing supervised and unsupervised learning frameworks which apply the spectral decomposition. algorithms derived under our framework are able to employ both labeled and unlabeled examples and are able to handle complex problems where data form separate clusters of manifolds. our framework offers simple views, explains relationships among existing frameworks and provides further extensions which can improve existing algorithms. furthermore, a new semi-supervised kernelization framework called ''kpca trick'' is proposed to handle non-linear problems.
0147795f-1b9b-424b-a20b-c5e0c68a0750 investigation of efficient features for image recognition by neural networks in the paper, effective and simple features for image recognition (named lira-features) are investigated in the task of handwritten digit recognition. two neural network classifiers are considered-a modified 3-layer perceptron lira and a modular assembly neural network. a method of feature selection is proposed that analyses connection weights formed in the preliminary learning process of a neural network classifier. in the experiments using the mnist database of handwritten digits, the feature selection procedure allows reduction of feature number (from 60 000 to 7000) preserving comparable recognition capability while accelerating computations. experimental comparison between the lira perceptron and the modular assembly neural network is accomplished, which shows that recognition capability of the modular assembly neural network is somewhat better.
0148d956-5e58-4db7-b9f2-bba533bfcba1 extending factor graphs so as to unify directed and undirected graphical models the two most popular types of graphical model are bayesian networks (bns) and markov random fields (mrfs). these types of model offer complementary properties in model construction, expressing conditional independencies, expressing arbitrary factorizations of joint distributions, and formulating messagepassing inference algorithms. we show how the notation and semantics of factor graphs (a relatively new type of graphical model) can be extended so as to combine the strengths of bns and mrfs. every bn or mrf can be easily converted to a factor graph that expresses the same conditional independencies, expresses the same factorization of the joint distribution, and can be used for probabilistic inference through application of a single, simple message-passing algorithm. we describe a modified "bayes-ball" algorithm for establishing conditional independence in factor graphs, and we show that factor graphs form a strict superset of bns and mrfs. in particular, we give an example of a commonly-used model fragment, whose independencies cannot be represented in a bn or an mrf, but can be represented in a factor graph. for readers who use chain graphs, we describe a further extension of factor graphs that enables them to represent properties of chain graphs.
01492b76-1fae-4430-8277-e20db68c84b2 traffic identification using flexible neural trees traditional traffic classification techniques like port-based and payload-based techniques are becoming ineffective owning to more and more internet applications using dynamic port number and encryption techniques. therefore, in the past few years, many researches have addressed machine learning-based techniques. most researches of machine learning-based traffic identification use traffic samples collected on key nodes of networks for their learning. these samples do not have accurate application information i. e. the ground truth which is crucial for machine learning algorithms. in this paper, we first designed a distributed host based traffic collecting platform (dhtcp) to gather traffic samples with accurate application information on user hosts. then we built a data set using dhtcp, and applied flexible neural trees (fnt)  a special kind of artificial neural network which has been successfully applied in many areas, for traffic identification. web and p2p traffics were studied in our work. although the proposed technique is at an early stage of development, experimental results show that it is a promising solution of internet traffic identification.
01493622-f32e-4459-8db4-413967af39f2 robot-mediated interviews with children : what do potential users think? luke wood, hagen lehmann, kerstin dautenhahn, ben robins, austen rayner, and dag syrdal, robot-mediated interviews with children: what do potential users think?, paper presented at the 50th annual convention of the society for the study of artificial intelligence and the simulation of behaviour, 1 april 2014  4 april 2014, london, uk.
0149afd1-b446-4650-87af-bccdf7be093b exploring new data sources to improve uk land parcel valuation the paper describes a novel approach for building a uk-wide automated land valuation model and its implementation into commercial online software. we examine existing approaches to land valuation used in the uk, notably trade area analysis, spatial interaction and comparable sales. we make the case that land use analysis, demographics and societal preferences affect the potential income and optimal use of parcels of land and hence the value of those parcels. this hypothesis leads to the introduction of a number of additional factors required to facilitate estimated land value, including traffic flow, population and site suitability. a number of artificial intelligence (ai) and machine learning spatial-temporal techniques are introduced to predict the value of all land parcels sold since 1995. we introduce a new technique, which includes (i) the application of support vector machines to land use analysis; (ii) the use of predictive techniques for macro-environmental factors; (iii) the use of large, open-source data sets to improve valuation; (iv) industry alignment in predefined industrial tool. a number of different mathematical techniques are used to validate the proposed model and we show that our model demonstrates 92% accuracy for residential pricing predictions.
014a13e4-26b2-4f6d-afd2-ca5353b1316f agentsheets: a tool for building domain-oriented dynamic, visual environments cultures deal with their environments by adapting to them and simultaneously changing them. this is particularly true for technological cultures, such as the dynamic culture of computer users. to date, the ability to change computing environments in non-trivial ways has been dependent upon the skill of programming. because this skill has been hard to acquire, most computer users must adapt to computing environments created by a small number of programmers. in response to the scarcity of programming ability, the computer science community has concentrated on producing general-purpose tools that cover wide spectrums of applications. as a result, contemporary programming languages largely ignore the intricacies arising from complex interactions between different people solving concrete problems in specific domains.#r##n#this dissertation describes agentsheets, a substrate for building domain-oriented, visual, dynamic programming environments that do not require traditional programming skills. it discusses how agentsheets supports the relationship among people, tools, and problems in the context of four central themes: (1) agentsheets features a versatile construction paradigm to build dynamic, visual environments for a wide range of problem domains such as art, artificial life, distributed artificial intelligence, education, environmental design, and computer science theory. the construction paradigm consists of a large number of autonomous, communicating agents organized in a grid, called the agentsheet. agents utilize different communication modalities such as animation, sound, and speech. (2) the construction paradigm supports the perception of programming as problem solving by incorporating mechanisms to incrementally create and modify spatial and temporal representations. (3) to interact with a large number of autonomous entities agentsheets postulates participatory theater, a human-computer interaction scheme combining the advantages of direct manipulation and delegation into a continuous spectrum of control and effort. (4) metaphors serve as mediators between problem solving-oriented construction paradigms and domain-oriented applications. metaphors are used to represent application semantics by helping people to conceptualize problems in terms of concrete notions. furthermore, metaphors can simplify the implementation of applications. application designers can explore and reuse existing applications that include similar metaphors.
014a3f30-0aaa-4e4c-b60d-c8e189fe8aae proby aplikacji paradygmatu ucielesnionego umysu ... w tworzeniu sztucznej inteligencji despite some significant achievements in the early stage of works on the development of artificial intelligence, scientists failed to program machines to imitate human thinking. the next generation of scientists included proposals of the embodied mind paradigm in their new research programme. the paradigm states that human intelligence is formed through a reciprocal interaction between the body and an environment. this work discusses the application of the main proposals of the new artificial intelligence that were applied in the process of constructing machines and modelling their behaviour. it presents important projects that met the philosophical criteria and that were aimed at embodying artificial intelligence.
014a47ea-71aa-4b34-ae90-07f8a2f7a975 proactive meeting management for distributed collaborative design this paper presents the cairo ( c ollaborative  a gent  i nteraction control and synch ro nization) system which is a distributed design negotiation environment that allows individuals to interact over computer networks without the physical and temporal constraints experienced in a traditional meeting environment. requirements for a collaborative design negotiation environment are identified based on background research in meeting and negotiation processes as well as distributed artificial intelligence concepts. furthermore, the paper describes an agent-based coordination and facilitation mechanism to support the design negotiation process, and a framework for effective documentation of meeting proceedings. these mechanisms are built on top of the cairo distributed communication infrastructure.
014aa933-ab27-4503-9a4a-cb7cf4348603 a review of fabric identification based on image analysis technology this paper provides a review of the automatic methods used for the identification of woven fabrics developed in nearly 30 years starting from the mid-1980s until now. compared with the manual method based on human eyes and experiences, the objective evaluation technology based on image processing and artificial intelligence holds the advantages of quick response, digital solution and accuracy. this paper describes briefly the background of weave pattern recognition and its development based on an overview of many researches done before. the reported methods can be classified into five categories (diffraction analysis-based, photoelectric analysis-based, frequency domain analysis- based, spatial domain analysis-based, jointed methods and other ones). both the merits and demerits of frequency domain analysis-based and spatial domain analysis-based methods have been summarized and discussed in this paper. therefore, it can provide a good reference platform for the researchers to understand and utilize these methods presented for the recognition of woven fabric weave pattern.
014acef2-30e1-434f-ac6f-f1edd9e1e63c neural networks and carbon-13 nmr shift prediction 
014c121b-1e6b-472b-a734-c38bdbd0b55b the aggregate complexity of decisions in the game of go artificial intelligence (ai) research is fast approaching, or perhaps has already reached, a bottleneck whereby further advancement towards practical human-like reasoning in complex tasks needs further quantified input from large studies of human decision-making. previous studies in psychology, for example, often rely on relatively small cohorts and very specific tasks. these studies have strongly influenced some of the core notions in ai research such as the reinforcement learning and the exploration versus exploitation paradigms. with the goal of contributing to this direction in ai developments we present our findings on the evolution towards world-class decision-making across large cohorts of subjects in the formidable game of go. some of these findings directly support previous work on how experts develop their skills but we also report on several previously unknown aspects of the development of expertise that suggests new avenues for ai research to explore. in particular, at the level of play that has so far eluded current ai systems for go, we are able to quantify the lack of predictability of experts and how this changes with their level of skill.
014cd687-b986-42c1-b03d-d381f5b737cf sapkos: experimental czech multi-label document classification and analysis system this paper presents an experimental multi-label document classification and analysis system called sapkos. the system which integrates the state-of-the-art machine learning and natural language processing approaches is intended to be used by the czech news agency (ctk). its main purpose is to save human resources in the task of annotation of newspaper articles with topics. another important functionality is automatic comparison of the ctk production with popular czech media. the results of this analysis will be used to adapt the ctk production to better correspond to the todays market requirements. an interesting contribution is that, to the best of our knowledge, no other automatic czech document classification system exists. it is also worth mentioning that the system accuracy is very high. this score is obtained due to the unique system architecture which integrates a maximum entropy based classification engine with the novel confidence measure method.
014d4384-890d-411a-b113-b3377ca54c92 analysis of quality of experience by applying fuzzy logic : a study on response time to be successful in today's competitive market, service providers should look at user's satisfaction as a critical key. in order to gain a better understanding of customers' expectations, a proper evaluations which considers intrinsic characteristics of perceived quality of service is needed. due to the subjective nature of quality, the vagueness of human judgment and the uncertainty about the degree of users' linguistic satisfaction, fuzziness is associated with quality of experience. considering the capability of fuzzy logic in dealing with imprecision and qualitative knowledge, it would be wise to apply it as a powerful mathematical tool for analyzing the quality of experience (qoe). this thesis proposes a fuzzy procedure to evaluate the quality of experience. in our proposed methodology, we provide a fuzzy relationship between qoe and quality of service (qos) parameters. to identify this fuzzy relationship a new term called fuzzi ed opinion score (fos) representing a fuzzy quality scale is introduced. a fuzzy data mining method is applied to construct the required number of fuzzy sets. then, the appropriate membership functions describing fuzzy sets are modeled and compared with each other. the proposed methodology will assist service providers for better decision-making and resource management.
014d96e3-0d9a-4961-a8be-3e0381c65dbb effective information retrieval approach based on parallel matrix method and mapreduce framework the information technology is developing rapidly, large amount of database and huge data has made a tremendous challenge in data mining. as a result, the amounts of data missing and unstructured information are increasing at an unprecedented rate. in the existing system, mapreduce supports efficient analysis of large amount of data and can process complete information. the rough set theory is also an efficient process used for generation of set of decision rules in data set. by these techniques only structured information's can be processed from a data set and fails to process incomplete information. to overcome this shortcoming, we have proposed a combinatory method of mapreduce and opinion mining algorithm. the combinatory algorithm is used to process large-scale unstructured data into structured data.
014de911-9050-4a5d-8285-3dee3877eafd prediction of survival in patients with liver cancer using artificial neural networks and classification and regression trees this study established a survival prediction model for liver cancer using data mining technology. the data were collected from the cancer registration database of a medical center in northern taiwan between 2004 and 2008. a total of 227 patients were newly diagnosed with liver cancer during this time. with literature review, and expert consultation, nine variables pertaining to liver cancer survival were analyzed using t-test and chi-square test. six variables showed significant. artificial neural network (ann) and classification and regression tree (cart) were adopted as prediction models. the models were tested in three conditions; one variable (clinical stage alone), six significant variables, and all nine variables (significant and non significant). 5-year survival was the output prediction. the results showed that the ann model with nine input variables was superior predictor of survival (p&#60;0.001). the area under receiver operating characteristic curve (auc) was 0.915, 0.87, 0.88, and 0.87 for accuracy, sensitivity, and specificity respectively. the ann model is significant more accurate than cart model when predict survival for liver cancer and provide patients information for understanding the treatment outcomes.
014e177e-a8d6-463c-8614-e84702a29996 prediction of financial distress for electricity sectors using data mining 
014ea9a5-90cc-4101-a5ad-409a54ed22ca engineering design optimization based on intelligent response surface methodology an intelligent response surface methodology (irsm) was proposed to achieve the most competitive metal forming products, in which artificial intelligence technologies are introduced into the optimization process. it is used as simple and inexpensive replacement for computationally expensive simulation model. in irsm, the optimal design space can be reduced greatly without any prior information about function distribution. also, by identifying the approximation error region, new design points can be supplemented correspondingly to improve the response surface model effectively. the procedure is iterated until the accuracy reaches the desired threshold value. thus, the global optimization can be performed based on this substitute model. finally, we present an optimization design example about roll forming of a u channel product.
014ff3ab-73f6-46fa-b023-6a92cc86aacc role of statistical and machine learning methods in special protection scheme logic design and failure assessment abstractwith tremendous growth in variable renewable generation and uncertainties in power system operations, there has been lot of installations of special protection schemes in many parts of the world. a special protection scheme, also termed a remedial action scheme, is the high-end last line of defense mechanism to relieve the system from catastrophic consequences under highly stressed conditions. this article delineates the special protection scheme logic design process and, with the help of a nordic grid case study, proposes a new paradigm of special protection scheme failure assessment and logic design process, namely the system-view framework. in this context, the article also demonstrates the application of decision tree-based methods to derive special protection scheme operational logic and provides a pathway to use efficient statistical sampling techniques for implementing the system-view special protection scheme failure assessment method, thereby advancing the logic redesign process.
015086f3-b9a3-45ad-9551-93a4655b20f5 classification using discriminative restricted boltzmann machines recently, many applications for restricted boltzmann machines (rbms) have been developed for a large variety of learning problems. however, rbms are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. in this paper, we argue that rbms provide a self-contained framework for deriving competitive non-linear classifiers. we present an evaluation of different learning algorithms for rbms which aim at introducing a discriminative component to rbm training and improve their performance as classifiers. this approach is simple in that rbms are used directly to build a classifier, rather than as a stepping stone. finally, we demonstrate how discriminative rbms can also be successfully employed in a semi-supervised setting.
0150c103-d9a2-4ae8-9fcf-affe6c3b15e3 in-flight vehicle health management abstract#r##n##r##n#integrated vehicle health management (ivhm) is a complex, multidisciplinary science that draws upon numerous fields including aerospace engineering, electrical engineering, computer science, mechanical engineering, statistics, and many sub-disciplines of these fields. as operational costs rise for modern aerospace systems, health management technologies are becoming more critical from both safety and maintenance perspectives.#r##n##r##n##r##n##r##n#the ultimate goal of an ivhm system is to detect, diagnose, predict, and mitigate adverse events during the operation of a system. many traditional approaches to systems engineering break down the system into its constituent subsystems and components, each of which draws from its own scientific or engineering discipline. from a health management perspective, this is not sufficient; system-level integration of subsystem health management capabilities is critical to the overall effectiveness of the final implementation. moreover, the health management system design is sometimes an afterthought rather than an integral part of the design process. this can lead to a situation where appropriate sensing technologies, computational environments, and information management systems are not included in the final design.#r##n##r##n##r##n##r##n#this chapter describes a systematic approach to the development of ivhm technologies and systems that emphasizes ivhm's most important goals  detection, diagnosis, prognosis, and mitigation  in a system or subsystem-independent manner.#r##n##r##n##r##n#keywords:#r##n##r##n#integrated vehicle health management;#r##n#detection;#r##n#diagnosis;#r##n#prognosis;#r##n#mitigation;#r##n#system engineering;#r##n#maintenance;#r##n#safety;#r##n#reliability;#r##n#data mining
01515a53-f54a-40fd-90d6-304b54f8ef5b hdd: a hypercube division-based algorithm for discretisation discretisation, as one of the basic data preparation techniques, has played an important role in data mining. this article introduces a new hypercube division-based (hdd) algorithm for supervised discretisation. the algorithm considers the distribution of both class and continuous attributes and the underlying correlation structure in the data set. it tries to find a minimal set of cut points, which divides the continuous attribute space into a finite number of hypercubes, and the objects within each hypercube belong to the same decision class. finally, tests are performed on seven mix-mode data sets, and the c5.0 algorithm is used to generate classification rules from the discretised data. compared with the other three well-known discretisation algorithms, the hdd algorithm can generate a better discretisation scheme, which improves the accuracy of classification and reduces the number of classification rules.
0151e086-917c-4312-95da-cea41f888c44 quantum adiabatic machine learning we develop an approach to machine learning and anomaly detection via quantum adiabatic evolution. this approach consists of two quantum phases, with some amount of classical preprocessing to set up the quantum problems. in the training phase we identify an optimal set of weak classifiers, to form a single strong classifier. in the testing phase we adiabatically evolve one or more strong classifiers on a superposition of inputs in order to find certain anomalous elements in the classification space. both the training and testing phases are executed via quantum adiabatic evolution. all quantum processing is strictly limited to two-qubit interactions so as to ensure physical feasibility. we apply and illustrate this approach in detail to the problem of software verification and validation, with a specific example of the learning phase applied to a problem of interest in flight control systems. beyond this example, the algorithm can be used to attack a broad class of anomaly detection problems.
01524dd9-c802-4b05-9ef6-d156f755f64f a study of hierarchical and flat classification of proteins automatic classification of proteins using machine learning is an important problem that has received significant attention in the literature. one feature of this problem is that expert-defined hierarchies of protein classes exist and can potentially be exploited to improve classification performance. in this article, we investigate empirically whether this is the case for two such hierarchies. we compare multiclass classification techniques that exploit the information in those class hierarchies and those that do not, using logistic regression, decision trees, bagged decision trees, and support vector machines as the underlying base learners. in particular, we compare hierarchical and flat variants of ensembles of nested dichotomies. the latter have been shown to deliver strong classification performance in multiclass settings. we present experimental results for synthetic, fold recognition, enzyme classification, and remote homology detection data. our results show that exploiting the class hierarchy improves performance on the synthetic data but not in the case of the protein classification problems. based on this, we recommend that strong flat multiclass methods be used as a baseline to establish the benefit of exploiting class hierarchies in this area.
0153093b-d7ec-4009-8e56-31b1d5640430 feature selection on boolean symbolic objects with the boom in it technology, the data sets used in application are more and more larger and are described by a huge number of attributes, therefore, the feature selection become an important discipline in knowledge discovery and data mining, allowing the experts to select the most relevant features to improve the quality of their studies and to reduce the time processing of their algorithm. in addition to that, the data used by the applications become richer. they are now represented by a set of complex and structured objects, instead of simple numerical matrixes. the purpose of our algorithm is to do feature selection on rich data, called boolean symbolic objects (bsos). these objects are described by multivalued features. the bsos are considered as higher level units which can model complex data, such as cluster of individuals, aggregated data or taxonomies. in this paper we will introduce a new feature selection criterion for bsos, and we will explain how we improved its complexity.
0153c39f-11dc-48b9-8f4a-1987361d798d fast transrating for high efficiency video coding based on machine learning to incorporate the newly developed high efficiency video coding (hevc) standard in real-life network applications, efficient transrating algorithms are required. we propose a fast transrating scheme, based on the early prediction of the partition split-flags in p pictures. using machine learning techniques, the correlation between co-located partitions at different quantizations is investigated. this results in a model which predicts the split-flag and gives the associated prediction accuracy so that the splitting process in the transcoder is optimized. at each partition depth, the model indicates whether the full rate-distortion cost evaluations should be performed at the current depth, or if the partition can be split immediately. experimental results show that the proposed transcoder reduces the complexity of the transrating process by 76.04%, while maintaining the coding efficiency of a cascaded decoder-encoder.
0153d9c1-ae4e-4b9b-8a9d-8cb81e8de6cd prediksi nasabah yang berpotensi membuka simpanan deposito menggunakan naive bayes berbasis particle swarm optimization deposito masih merupakan pilihan utama masyarakat untuk berinvestasi dan hal ini merupakan kesempatan bagi bank untuk menentukan strategi pemasaran dan promosi yang lebih efisien dan efektif. atas dasar permasalahan tersebut, maka dilakukan penelitian untuk memprediksi nasabah yang berpotensi membuka deposito dengan menggunakan teknik data mining khususnya algoritma naive bayes berbasis pso. pso pada penelitian ini akan digunakan untuk seleksi fitur. hasil dari penelitian ini, akurasi algoritma naive baiyes adalah 82,19% dan akurasi algoritma naive baiyes berbasis pso adalah 89,70%. penggunaan algoritma pso ternyata meningkatkan akurasi sebesar 7,51% dan algoritma tersebut dapat digunakan untuk sistem pendukung keputusan pada penelitian ini.
015448af-6bc1-45c9-a8aa-da5f35513fc9 application of lc-high-resolution ms with 'intelligent' data mining tools for screening reactive drug metabolites. biotransformation of chemically stable compounds to reactive metabolites that can bind covalently to macromolecules (such as proteins and dna) is considered an undesirable property of drug candidates. due to the possible link, which has not yet been conclusively demonstrated, between reactive metabolites and adverse drug reactions, screening for metabolic activation of lead compounds through in vitro chemical trapping experiments has become an integral part of the drug discovery process in many laboratories. in this review, we provide an overview of the recent advances in the application of high-resolution ms. these advances facilitated the development of accurate-mass-based data mining tools for high-throughput screening of reactive drug metabolites in drug discovery.
01556c15-6615-4650-b47c-65b9fbb8473c university course timetable system design and implementation based on mathematical model in this paper, through the analysis and the summarization of the existing problems, a mathematical model for the course timetable system is proposed. at the same time, through the use of the pattern recognition technology in artificial intelligence, aiming at this mathematical model a new university course timetable system design program is proposed and realized. this program not only can well solve the shortages of the existing course timetable system, but also is simple and easy to operate, has strong versatility.
0155b8bd-e37b-4f8e-992b-d61b79dbaf3e estimating expected error rates of random forest classifiers: a comparison of cross-validation and bootstrap statistical learning has recently seen an expansion of applications in different areas of science, finance and industry, as it plays a great role within the fields of statistics, data mining and artificial intelligence. hence, it intersects with areas of engineering and other disciplines as well. it is used for both regression and classification problems. solving these problems usually involves building/training a model/classifier and validating its performance for a given task. in this paper we compare two resampling methods for assessment of a random forest classifier: k-fold cross-validation and bootstrap. we use these methods to estimate the generalization error and to create learning curves. both methods yield similar results on our data. the most important requirement for good generalization error estimates of either method is that the used data sample (i.e. the training dataset) represents the unknown true distribution of the data. this requirement cannot always be met in practice and results of resampling methods have to be interpreted with care if it is violated.
01560f8c-6781-4f53-9491-15d238c11431 negotiation and cooperation in multi-agent environments abstract   automated intelligent agents inhabiting a shared environment must coordinate their activities. cooperationnot merely coordinationmay improve the performance of the individual agents or the overall behavior of the system they form. research in distributed artificial intelligence (dai) addresses the problem of designing automated intelligent systems which interact effectively. dai is not the only field to take on the challenge of understanding cooperation and coordination. there are a variety of other multi-entity environments in which the entities coordinate their activity and cooperate. among them are groups of people, animals, particles, and computers. we argue that in order to address the challenge of building coordinated and collaborated intelligent agents, it is beneficial to combine ai techniques with methods and techniques from a range of multi-entity fields, such as game theory, operations research, physics and philosophy. to support this claim, we describe some of our projects, where we have successfully taken an interdisciplinary approach. we demonstrate the benefits in applying multi-entity methodologies and show the adaptations, modifications and extensions necessary for solving the dai problems
01567061-ce8c-4990-b54d-b84e65e986c6 parameter learning for multi-factors of entity answer extracting entity extraction involves multi-factors, and the different factor has an impact on the answer in varying degrees, this paper presents a machine learning approach to parameter learning for entity answer. firstly, in view of characteristics of the question answering system (qa), we define three elements of the text score, passage score and entity score which influenced the answer extraction, also give the relevant computational method about them. then collect 400 entity answers of product, person, and organization according to trec2009 entity task requirements. with the help of search engines, retrieve related pages and calculate the score of the various factors related to the answer respectively. thereafter compute the score of entity answers according to a linear combination of the various factors. define an initial score to extract the entity answer and get a sorted list of answers. finally, mark these entities answer to obtain the correct marked answers corpus, then build parameter learning model by the em algorithm iterate gradually to find the optimal answer weight of different factors that influenced the answer extraction. we carried on the experiment in the trec2009 entity task; it shows very good results for this method. the accuracy of entity answer has achieved 88.93%.
0156f79b-e1d2-4b15-a3d6-6509c0e4e792 development of a new method for data mining based rough set theory in the paper, a kind of expanding model of rough set, which is with membership grade degree and contribution was presented after lucubrating the deficiencies of the theory of traditional rough set. in this model, the information system with membership grade and contribution, the processing of noises, similar space partition, the dependency grade calculation of decision attribute on condition one, attribute reduction algorithm, mining process of interconnected rule were discussed.
0157e4c4-cdf0-4a3a-948e-6510a3842456 a mathematics morphology based algorithm of obstacles clustering as a large amount of data stored in spatial databases, people may like to find groups of data which share similar features. thus cluster analysis becomes an important area of research in data mining. in the real world, there exist many physical obstacles such as rivers, lakes and highways, and their presence may affect the result of clustering substantially. however, most of clustering algorithms can not deal with obstacles. in this paper, a new clustering algorithm mmo is proposed for the problem of clustering in the presence of obstacles. the main contributions are: two new mathematics morphological operators are introduced to discover clusters in the presence of obstacles. our new operators are more accurate than the ordinary operators: open and close. the performance tests show that: mmo is effective in discovering clusters of arbitrary shape in the presence of obstacles; it is very efficient with a complexity of o(n+m) , where n is the number of data points, and m is the number of obstacles; it is not sensitive to noise.
01581b8f-461d-41a9-9472-da2180a8dc92 educational aid for learners on the autism spectrum smart e-learning environments are no more novice but have become the order-of-the-day, with the strengths gained and exhibited by the recent technologies. also, personalization is the current norm with each learner customizing his/her own learning environments, not only on his interests but also to his tastes. personalization of the e- learning environments for the 'cognitively disabled' is still to be evolved, as personalization cannot be left to the discretion of the disabled user. it has to be done by the supporting people like the teacher, therapist or the parent. this manual intervention can be minimized, using self- learning environments, which can interact with the child, understand him and dynamically personalize the lesson plans, based on his interests, mood and current attention span. proposing a smart e-learning tutoring model for the autistic with machine learning capabilities that help in generating dynamic e-learning sessions, thus maximizing each learner's opportunity to grasp and learn independently.
01581fa6-81e3-4302-86d7-81c91aacf5ac computation on information, meaning and representations. an evolutionary approach (2011) understanding computation as a process of the dynamic change of information brings to look at the different types of computation and information. computation of information does not exist alone by itself but is to be considered as part of a system that uses it for some given#r##n#purpose. information can be meaningless like a thunderstorm noise, it can be meaningful like an alert signal, or like the representation of a desired food. a thunderstorm noise participates to the generation of meaningful information about coming rain. an alert signal has a meaning as allowing a safety constraint to be satisfied. the representation of a desired food participates to the satisfaction of some metabolic constraints for the organism. computations on information and representations will be different in nature and in complexity as the systems that link them have different constraints to satisfy. animals have survival constraints to satisfy. humans have many specific constraints coming in addition. and computers will compute what the designer and programmer ask for.#r##n#we propose to analyze the different relations between information, meaning and representation by taking an evolutionary approach on the systems that link them. such a bottom-up approach allows starting with simple organisms and avoids an implicit focus on humans, which is the most complex and difficult case. to make available a common#r##n#background usable for the many different cases, we use a systemic tool that defines the generation of meaningful information by and for a system submitted to a constraint [menant, 2003]. this systemic tool allows to position information, meaning and representations for systems#r##n#relatively to environmental entities in an evolutionary perspective.#r##n#we begin by positioning the notions of information, meaning and representation and recall the characteristics of the meaning generator system (mgs) that link a system submitted to a constraint to its environment. we then use the mgs for animals and highlight the network nature of the  interrelated meanings about an entity of the environment. this brings us to define the representation of an item for an agent as being the network of meanings relative to the item for the agent.#r##n#such meaningful representations embed the agents in their environments and are far from the good old fashion artificial intelligence type ones.#r##n#the mgs approach is then used for humans with a limitation resulting of the unknown nature of human consciousness.#r##n#application of the mgs to artificial systems brings to look for compatibilities with different levels of artificial intelligence (ai) like embodied-situated ai, the guidance theory of representations, and enactive ai. concerns relative to different types of autonomy and organic or artificial constraints are highlighted. we finish by#r##n#summarizing the points addressed and by proposing some continuations.
015871b2-9b0b-4151-90ec-8e1925bfc2b8 toward seamless environments for dispute prevention and resolution the work described in this paper was developed under the tiarac - telematics and artificial intelligence in alternative conflict resolution project (ptdc/jur/71354/2006), which is a research project supported by fct (science & technology foundation), portugal
0158b7cf-3a0c-4bf6-952f-e53432339827 the role of metadata in web video mining: issues and perspectives due to the complexity in rapid growth of audiovisual information over the web, it is becoming difficult to extract useful information from the web audiovisual data such as youtube, face book, and yahoo screen etc. web video mining is the process of extracting useful information from the web videos by applying data mining techniques. there are two approaches for web video mining- using traditional image processing/signal processing approach and metadata based approach. a number of techniques and algorithms are developed in image/signal processing approach to mine the video contents. but nowadays, mining of web videos without using image processing techniques is a challenging task. this paper represents a new approach for mining web videos using metadata as leading contribution for knowledge discovery.
0159269d-f9a8-4a9c-aab6-3027f3fe05b6 decision tree learning for drools decision trees can be used to represent a large number of expert system rules in a compact way. we describe machine learning algorithms for learning decision trees. we have implemented the algorithms, including bagging and boosting techniques. we have deployed the algorithms in the context of the jboss drools rule engine. we present experimental results evaluating the impact of bagging and boosting techniques on the classification accuracy and sizes of trees for several publicly available data sets.
01593327-a4c3-44e4-9c44-2c0f69c28eae comments on "dynamical optimal training for interval type-2 fuzzy neural network (t2fnn) in this comment, it will be shown that the backpropagation (bp) equations by wang are not correct. these bp equations were used to tune the parameters of the antecedent type-2 membership functions as well as the consequent part of the interval type-2 fuzzy neural networks (t2fnns). these incorrect equations would have led to erroneous results, and hence this might affect the comparisons and findings presented by wang this comment will highlight the correct bp tuning equations for the t2fnn
015a7550-8bf6-4682-9e7c-d6b5663bca3a thesis: clustering and instance based learning in first order logic instance based learning and clustering are popular methods in propositional machine learning. both methods use a notion of similarity between objects. this dissertation investigates these methods in a relational setting. first, a number of new metrics are proposed. next, these metrics are used to upgrade clustering and instance based learning to first order logic.
015af429-feab-4048-bdb5-d0430a91e197 use of neural nets for dynamic modeling and control of chemical process systems abstract   neural computing is one of the fastest growing areas of artificial intelligence. neural nets are inherently parallel and they hold great promise because of their ability to learn nonlinear relationships. this paper discusses the use of backpropagation neural nets for dynamic modeling and control of chemical process systems. the backpropagation algorithm and its rationale are reviewed. the algorithm is applied to model the dynamic response of ph in a cstr. compared to traditional arma modeling, the backpropagation technique is shown to be able to pick up more of the nonlinear characteristics of the cstr. the use of backpropagation models for control, including learning process inverses, is briefly discussed.
015affe6-afa2-44ab-bfe7-fe16a225877d modified binary pso for feature selection using svm applied to mortality prediction of septic patients this paper proposes a modified binary particle swarm optimization (mbpso) method for feature selection with the simultaneous optimization of svm kernel parameter setting, applied to mortality prediction in septic patients. an enhanced version of binary particle swarm optimization, designed to cope with premature convergence of the bpso algorithm is proposed. mbpso control the swarm variability using the velocity and the similarity between best swarm solutions. this paper uses support vector machines in a wrapper approach, where the kernel parameters are optimized at the same time. the approach is applied to predict the outcome (survived or deceased) of patients with septic shock. further, mbpso is tested in several benchmark datasets and is compared with other pso based algorithms and genetic algorithms (ga). the experimental results showed that the proposed approach can correctly select the discriminating input features and also achieve high classification accuracy, specially when compared to other pso based algorithms. when compared to ga, mbpso is similar in terms of accuracy, but the subset solutions have less selected features.
015b062f-4a24-4418-baf7-1ab2caeefa6b modeling the pavement present serviceability index of flexible highway pavements using data mining 
015b2a0b-8fa6-4288-ab8a-c8177ff7e42e hybrid mobility prediction of 802.11 infrastructure nodes by location tracking and data mining in an ieee 802.11 infrastructure network, as the mobile node is moving from one access point to another, the resource allocation and smooth hand off may be a problem. if some reliable prediction is done on mobile nodes next move, then resources can be allocated optimally as the mobile node moves around. this would increase the performance throughput of wireless network. we plan to investigate on a hybrid mobility prediction scheme that uses location tracking and data mining to predict the future path of the mobile node. we also propose a secure version of the same scheme. through simulation and analysis, we present the prediction accuracy of our proposal.
015b5eae-a343-4007-9bd5-183c56ec0bc6 imputing missing values using inverse distance weighted interpolation for time series data data mining is the process of analyzing and retrieving meaningful information from a database. temporal data mining deals with time stamped data. in the real world, temporal data obtained may contain noisy, inconsistent data and in most cases the data may be missing; hence data preprocessing is one of the important steps that has to be carried out in data mining. missing values may generate biased results and affect the accuracy of classification. in order to overcome this it is necessary to impute the missing values based on other information in the dataset. the work focuses on imputing missing values using inverse distance weighted interpolation method which best suits for data sampled at uneven intervals of time. this method assigns values to unknown points from a weighted sum of values of known points. machine learning techniques applied to the imputed dataset will give better accuracy than that of the incomplete dataset.
015b6131-68ba-410c-a005-3699f7470db4 cnn-merp: an fpga-based memory-efficient reconfigurable processor for forward and backward propagation of convolutional neural networks large-scale deep convolutional neural networks (cnns) are widely used in machine learning applications. while cnns involve huge complexity, vlsi (asic and fpga) chips that deliver high-density integration of computational resources are regarded as a promising platform for cnn's implementation. at massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the vlsi chip, becomes the system bottleneck. moreover, vlsi solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of cnns. this paper presents cnn-merp to address these issues. cnn-merp incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/off-chip data allocation, data flow optimization and data reuse. the proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of fpga. as a result, an external memory bandwidth requirement of 1.94mb/gflop is achieved, which is 55% lower than prior arts. under limited dram bandwidth, a system throughput of 1244gflop/s is achieved at the vertex ultrascale platform, which is 5.48 times higher than the state-of-the-art fpga implementations.
015b77b6-1204-4361-ac12-5240a2bf367a association rule mining: an overview data mining is the process of discovering previou sly unknown, undiscovered and hidden patterns from large information repositories like relational databases and warehouses. it is of great importance in the re cent time where the amount of data has reached to gigabytes and even terabytes. data mining makes use of various techniques such as clustering, classification, association rule mining and regression. association rule mining finds association between the items in the database. this paper gives us a brief idea regardin g association rule mining and applications of association rule mining in different areas for effective decision making.
015b7e62-7e3f-436d-8f4a-a281e45818e4 real-time detection of drowsiness related lane departures using steering wheel angle drowsy driving is a significant factor in many motor vehicle crashes in the united states and across the world. efforts to reduce these crashes have developed numerous algorithms to detect both acute and chronic drowsiness. these algorithms employ behavioral and physiological data, and have used different machine learning techniques. this work proposes a new approach for detecting drowsiness related lane departures, which uses unfiltered steering wheel angle data and a random forest algorithm. using a data set from the national advanced driving simulator the algorithm was compared with a commonly used algorithm, perclos and a simpler algorithm constructed from distribution parameters. the random forest algorithm had higher accuracy and area under the receiver operating characteristic curve (auc) than perclos and had comparable positive predictive value. the results show that steering-angle can be used to predict drowsiness related lane-departures six seconds before they occur, and suggest that the random forest algorithm, when paired with an alert system, could significantly reduce vehicle crashes. language: en
015c40ac-6ebd-4fc4-8777-229cdb2818a4 deterministic system identification using rbf networks this paper presents an artificial intelligence application using a nonconventional mathematical tool: the radial basis function (rbf) networks, aiming to identify the current plant of an induction motor or other nonlinear systems. here, the objective is to present the rbf response to different nonlinear systems and analyze the obtained results. a rbf network is trained and simulated in order to obtain the dynamical solution with basin of attraction and equilibrium point for known and unknown system and establish a relationship between these dynamical systems and the rbf response. on the basis of several examples, the results indicating the effectiveness of this approach are demonstrated.
015cb888-350a-4c11-82e6-e781ffa66556 machine learning, clustering, and polymorphy abstract   this paper describes a machine induction program (witt) that attempts to model human categorization. properties of categories to which human subjects are sensitive includes best or prototypical members, relative contrasts between putative categories, and polymorphy (neither necessary or sufficient features). this approach represents an alternative to usual artificial intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules, equivalence classes, and simple search and match schemes. witt is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes. applications of this approach in the domains of expert systems and information retrieval are also discussed.
015d0438-2e6a-42e0-97ac-69768ff58b6d study of customer acquisition support system for mobile operators abstract   the mobile operators are struggling for improving the market share and the revenues. one important method is to acquire the potential customers from the competitors. this article presents a whole acquisition process and an integrated framework for customer acquisition support system (cass). the core of the system is the customer acquisition identification models which are built based on data mining technologies. the cass can automate the acquisition process and decrease the cost and implement precise marketing strategy for mobile operators.
015d8185-f9b2-43e8-898a-79ca63ebedf6 fair and balanced? quantifying media bias through crowdsourced content analysis it is widely thought that news organizations exhibit ideological bias, but rigorously quantifying such slant has proven methodologically challenging. through a combination of machine learning and crowdsourcing techniques, we investigate the selection and framing of political issues in 15 major u.s. news outlets. starting with 803,146 news stories published over 12 months, we first used supervised learning algorithms to identify the 14% of articles pertaining to political events. we then recruited 749 online human judges to classify a random subset of 10,950 of these political articles according to topic and ideological position. our analysis yields an ideological ordering of outlets consistent with prior work. we find, however, that news outlets are considerably more similar than generally believed. specifically, with the exception of political scandals, we find that major news organizations present topics in a largely non-partisan manner, casting neither democrats nor republicans in a particularly favorable or unfavorable light. moreover, again with the exception of political scandals, there is little evidence of systematic differences in story selection, with all major news outlets covering a wide variety of topics with frequency largely unrelated to the outlet's ideological position. finally, we find that news organizations express their ideological bias not by directly advocating for a preferred political party, but rather by disproportionately criticizing one side, a convention that further moderates overall differences. 
015ddea2-3192-4ca3-8e39-5d6971289c5e transferring localization models across space machine learning approaches to indoor wifi localization involve an offline phase and an online phase. in the offline phase, data are collected from an environment to build a localization model, which will be applied to new data collected in the online phase for location estimation. however, collecting the labeled data across an entire building would be too time consuming. in this paper, we present a novel approach to transferring the learning model trained on data from one area of a building to another. we learn a mapping function between the signal space and the location space by solving an optimization problem based on manifold learning techniques. a low-dimensional manifold is shared between data collected in different areas in an environment as a bridge to propagate the knowledge across the whole environment. with the help of the transferred knowledge, we can significantly reduce the amount of labeled data which are required for building the localization model. we test the effectiveness of our proposed solution in a real indoor wifi environment.
015f17be-bd41-45da-ba2f-65666b526126 the research and implementation of decision support system for oil production based on soa the decision support system for oil production based on service-oriented architecture (soa) uses b/s structure of the three levels design pattern. it integrates the application of modern communications, graphics and image processing, remote transmission control, data mining technology and so on. it uses olap to analysis the field condition timely, and it plays role in production decision-making and control. this paper discusses the architecture and the physical structure of the decision support system (dss), emphatically describes the system structure of oil production command dds. this system not only performs in oil engineering management system (oms), but also has great advantages in the integration of office automation system (oa) of oil industry and enterprise resource planning system (erp).
015f3a19-ce0d-4033-b600-005eb41e2fbb data mining: a clustering application 
015fefd7-c037-4c9c-9d73-8ddf28bbb84b iqm: an extensible and portable open source application for image and signal analysis in java image and signal analysis applications are substantial in scientific research. both open source and commercial packages provide a wide range of functions for image and signal analysis, which are sometimes supported very well by the communities in the corresponding fields. commercial software packages have the major drawback of being expensive and having undisclosed source code, which hampers extending the functionality if there is no plugin interface or similar option available. however, both variants cannot cover all possible use cases and sometimes custom developments are unavoidable, requiring open source applications. in this paper we describe iqm, a completely free, portable and open source (gnu gplv3) image and signal analysis application written in pure java. iqm does not depend on any natively installed libraries and is therefore runnable out-of-the-box. currently, a continuously growing repertoire of 50 image and 16 signal analysis algorithms is provided. the modular functional architecture based on the three-tier model is described along the most important functionality. extensibility is achieved using operator plugins, and the development of more complex workflows is provided by a groovy script interface to the jvm. we demonstrate iqms image and signal processing capabilities in a proof-of-principle analysis and provide example implementations to illustrate the plugin framework and the scripting interface. iqm integrates with the popular imagej image processing software and is aiming at complementing functionality rather than competing with existing open source software. machine learning can be integrated into more complex algorithms via the weka software package as well, enabling the development of transparent and robust methods for image and signal analysis.
01603fcf-181a-461f-8834-81ca198f9961 preprint version - in press 35 framework for knowledge discovery from journal articles using text mining techniques the enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. therefore, specific (pre-) processing methods and algorithms are required in order to extract useful patterns. text mining also known as text data mining, refers to the discovery of previously unknown knowledge that can be found in text collections. in this study, we discuss text mining as a young interdisciplinary field in the intersection of the related areas such as information access - otherwise known as information retrieval, computational linguistics, data mining, statistics and natural language processing. we discuss some application areas of text mining and identify the related works. we also describe the main analysis tasks/processes in text data mining such as, information extraction, pre-processing, text transformation and feature selection. an architectural framework for knowledge discovery from journal articles using text mining techniques is also presented.
016071bc-5b13-40ea-9caa-c81f3c2e95c4 cbcm: a cell-based clustering method for data mining applications data mining applications have recently required a large amount of high-dimensional data. however, most clustering methods for the data miming applications do not work efficiently for dealing with large, high-dimensional data because of the so-called 'curse of dimensionality' and the limitation of available memory. in this paper, we propose a new cell-based clustering method (cbcm) which is more efficient for large, high-dimensional data than the existing clustering methods. our cbcm provides an efficient cell creation algorithm using a space-partitioning technique and uses a filtering-based index structure using an approximation technique. in addition, we compare the performance of our cbcm with the clique method in terms of cluster construction time, precision, and retrieval time.
0161bae7-d6dd-4ada-854c-f33e8d747f95 analysis of results of ecological simulation models with machine learning 
0161c4ef-090a-40da-ad32-6bbd510c06e2 active semi-supervised defect categorization defects are inseparable part of software development and evolution. to better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. ibm proposes orthogonal defect categorization (odc) which include various classifications of defects based on a number of orthogonal dimensions (e.g., symptoms and semantics of defects, root causes of defects, etc.). to help developers categorize defects, several approaches that employ machine learning have been proposed in the literature. unfortunately, these approaches often require developers to manually label a large number of defect examples. in practice, manually labelling a large number of examples is both time-consuming and labor-intensive. thus, reducing the onerous burden of manual labelling while still being able to achieve good performance is crucial towards the adoption of such approaches. to deal with this challenge, in this work, we propose an active semi-supervised defect prediction approach. it is performed by actively selecting a small subset of diverse and informative defect examples to label (i.e., active learning), and by making use of both labeled and unlabeled defect examples in the prediction model learning process (i.e., semi-supervised learning). using this principle, our approach is able to learn a good model while minimizing the manual labeling effort.   to evaluate the effectiveness of our approach, we make use of a benchmark dataset that contains 500 defects from three software systems that have been manually labelled into several families based on odc. we investigate our approach's ability in achieving good classification performance, measured in terms of weighted precision, recall, f-measure, and auc, when only a small number of manually labelled defect examples are available. our experiment results show that our active semi-supervised defect categorization approach is able to achieve a weighted precision, recall, f-measure, and auc of 0.651, 0.669, 0.623, and 0.710, respectively, when only 50 defects are manually labelled. furthermore, it outperforms an existing active multi-class classification algorithm, proposed in the machine learning community, by a substantial margin.
016276b2-cbb1-49a5-99f1-5c4b9e8761c0 data mining in petroleum upstreamthe use of regression and classification algorithms data mining (dm) technique has seen enormous successes in some fields, but its application to petroleum upstream (pup) is still at initial stage, as pup is quite different from the other fields in many aspects. the most popular dm algorithms in pup are regression and classification. through many dm applications to pup, we have found that: a) the preferable algorithm for regression is back-propagation neural network (bpnn), the next are regression of support vector machine (r-svm) and multiple regression analysis (mra), and the preferable algorithm for classification is classification of support vector machine (c-svm), the next is bayesian successive discrimination (baysd); b) c-svm can also be applied in data cleaning; c) both mra and baysd can also be applied in dimension-reduction, and baysd is preferable one; d) r-mode cluster analysis (rca) can be applied in dimension-reduction, while q-mode cluster analysis (qca) can be applied in sample-reduction. a case study in pup indicates that r-svm, bpnn and mra are not applicable for regression, whereas c-svm is applicable for classification in this case.
0162ce4b-f3a3-4591-bf10-360b842051d2 prediction of global and local model quality in casp8 using the modfold server abstract#r##n##r##n#the development of effective methods for predicting the quality of three-dimensional (3d) models is fundamentally important for the success of tertiary structure (ts) prediction strategies. since casp7, the quality assessment (qa) category has existed to gauge the ability of various model quality assessment programs (mqaps) at predicting the relative quality of individual 3d models. for the casp8 experiment, automated predictions were submitted in the qa category using two methods from the modfold servermodfold version 1.1 and modfoldclust. modfold version 1.1 is a single-model machine learning based method, which was used for automated predictions of global model quality (qmode1). modfoldclust is a simple clustering based method, which was used for automated predictions of both global and local quality (qmode2). in addition, manual predictions of model quality were made using modfold version 2.0an experimental method that combines the scores from modfoldclust and modfold v1.1. predictions from the modfoldclust method were the most successful of the three in terms of the global model quality, whilst the modfold v1.1 method was comparable in performance to other single-model based methods. in addition, the modfoldclust method performed well at predicting the per-residue, or local, model quality scores. predictions of the per-residue errors in our own 3d models, selected using the modfold v2.0 method, were also the most accurate compared with those from other methods. all of the mqaps described are publicly accessible via the modfold server at: http://www.reading.ac.uk/bioinf/modfold/. the methods are also freely available to download from: http://www.reading.ac.uk/bioinf/downloads/. proteins 2009.  2009 wiley-liss, inc.
016355a2-ce77-4a7f-b0e9-25dc33281c40 emphasis on rough set theory for image retrieval this paper highlights concept of rough set theory and focus on its active participation in various technologies such as cloud computing, artificial intelligence, data mining, expert system etc. we focus particularly on exploitation of rough set theory image retrieval with hybridization approach. we start with fundamental of rough set theory, its recent development, application, trends, impact on recent technology like cloud computing. in simple term, rs theory work or analysis on the basis of information associated with objects or events. rough set theory is promising theory that work on intelligent analysis of imperfect data and produce meaningful information related to data according to requirement of application. other theories like bayesian inference or fuzzy set theory are complementary theory that work parallel with rough set theory on rough data or imperfect data, which is important and basic processing step of many application like search engine, data mining, web mining etc. in image retrieval, rough set theory play important role in classification, clustering, feature selection, feature extraction, rule learning etc. our interest is to study its various combinational hybridized methods used in image retrieval to address semantic gap. we are proposed content based image retrieval framework with rough set theory for addressing semantic gap by using semantic classifier with semantics decision rule based on image information system. this framework also improves precision, recall and accuracy of image retrieval.
01645e1e-f10b-4646-982d-473dfd1dfa6f the complexity of reasoning about spatial congruence in the recent literature of artificial intelligence, an intensive research effort has been spent, for various algebras of qualitative relations used in the representation of temporal and spatial knowledge, on the problem of classifying the computational complexity of reasoning problems for subsets of algebras. the main purpose of these researches is to describe a restricted set of maximal tractable subalgebras, ideally in an exhaustive fashion with respect to the hosting algebras.#r##n##r##n#in this paper we introduce a novel algebra for reasoning about spatial congruence, show that the satisfiability problem in the spatial algebra mc-4 is np-complete, and present a complete classification of tractability in the algebra, based on the individuation of three maximal tractable subclasses, one containing the basic relations. the three algebras are formed by 14, 10 and 9 relations out of 16 which form the full algebra.
0164f0ac-914c-4786-a4e0-814f280c8b55 a comparative analysis of horizontal layout representation of data data mining is a process of extracting useful knowledge from the database. to do that one can need aggregative function for data set preparation. now a day, it is time consuming process and cumbersome process for the researcher. this paper compared the horizontal layout representation of aggregated data using structure query language and the data generated by data conda software tool.
01650ad1-6668-4bb5-a52b-76072e0b44d9 integrated connectionist models: building ai systems on subsymbolic foundations symbolic artificial intelligence is motivated by the hypothesis that symbol manipulation is both necessary and sufficient for intelligence. in symbolic systems, knowledge is encoded in terms of explicit symbolic structures, and inferences are based on handcrafted rules that sequentially manipulate these structures. such systems have been quite successful, for example, in modeling in-depth natural language processing, episodic memory, and symbolic problem solving. however, much of the inferencing for everyday natural language understanding appears to take place immediately, without conscious control, apparently based on associations with past experience. this type of reasoning is difficult to model in the symbolic framework. in contrast, subsymbolic (distributed connectionist) networks represent knowledge in terms of correlations, coded in the weights of the network. for a given input, the network computes the most likely answer given its past experience. a number of human-like information processing properties such as learning from examples, context sensitivity, generalization, robustness of behavior, and intuitive reasoning emerge automatically in subsymbolic systems. the major motivation for subsymbolic ai, therefore, is to give a better account for cognitive phenomena that are statistical, or intuitive, in nature. >
0165bf8c-313b-4185-9886-591870190ad2 finding common ground among experts' opinions on data clustering: with applications in malware analysis data clustering is a basic technique for knowledge discovery and data mining. as the volume of data grows significantly, data clustering becomes computationally prohibitive and resource demanding, and sometimes it is necessary to outsource these tasks to third party experts who specialize in data clustering. the goal of this work is to develop techniques that find common ground among experts' opinions on data clustering, which may be biased due to the features or algorithms used in clustering. our work differs from the large body of existing approaches to consensus clustering, as we do not require all data objects be grouped into clusters. rather, our work is motivated by real-world applications that demand high confidence in how data objects - if they are selected - are grouped together.we formulate the problem rigorously and show that it is np-complete. we further develop a lightweight technique based on finding a maximum independent set in a 3-uniform hypergraph to select data objects that do not form conflicts among experts' opinions. we apply our proposed method to a real-world malware dataset with hundreds of thousands of instances to find malware clusters based on how multiple major av (anti-virus) software classify these samples. our work offers a new direction for consensus clustering by striking a balance between the clustering quality and the amount of data objects chosen to be clustered.
01670b35-e4ea-4c95-b843-9ebc5b148863 profier: a prolog-based support system for developing picture recognition program abstract#r##n##r##n#this paper introduces a prolog-based support system for use in developing a picture recognition program. earlier, an image recognition program was introduced using such items as cigarette packages, tableware, etc. however, because of the disadvantages of that program, a new one was necessitated.#r##n##r##n##r##n##r##n#prolog and lisp, languages designed for use in artificial intelligence, are more suitable for use in high-level processing, e.g., image recognition and understanding. however, prolog shows greater potential for use in writing routines in the recognition process. but prolog is not very effective in handling numerical calculations involving large amounts of two-dimensional images. as a result, prolog systems are examined herein that have been developed using additional features that allow for implementation of other languages and databases.#r##n##r##n##r##n##r##n#here, the profier (prolog system with fortran as image analyzer) system is described. in this system, the knowledge and control structures are written in prolog, and for raw data processing and image feature extraction, processes that are difficult to handle with prolog, the system is expanded for programming in fortran. the configuration and characteristics of profier are described, and the performance is evaluated.
01671666-0c12-4ad4-bc37-c42631986bd0 an improved iwo-fcm data mining algorithm 
0167279f-07f0-48cb-833b-1c67f235a39a feeding data warehouses data warehouses have become very popular with academics, industry, and users. the idea of a global repository for strategic information is seen as a sesame door to a world of magic, where spontaneously generated information breakthroughs turn ordinary business into a multimillionaire production activity. this is mainly due to the current focus on sophisticated olap functionality and on data mining techniques, which promise to lead to discovery of most precious and unexpected strategic information. while all of this may well be true, although it does not come for free and the magnificence of the result is not guaranteed, there is one aspect that is underestimated or neglected. a data warehouse will not deliver anything worthwhile if it does not have the right information. getting this information is an extremely hard and painstaking task. the article focuses on this aspect to explore the main issues related to data acquisition.
01678013-3a04-46fa-ac9f-770561fe74d4 support and monitoring trajectory paths for vehicles using mobile devices currently artificial intelligence combined with computer vision has helped in performing daily activities, the current tendency of semi-autonomous vehicles assisted by artificial intelligence and computer vision has created a new field of image processing research. the image processing to support autonomous route tracking systems they have been tested in controlled media. in this investigation we perform the implementation of a system of guide and trajectory tracking in real scenarios (not controlled media) as road and city streets. propose a new method for detecting lines as the hough transform, in a simple and efficient for the detection of lane on a mobile manner. with the above determined whether or not the vehicle conserves his lane by opening angles of triangles detected with the mobile camera, to support and assist the driver. the methodology is tested to determine its performance qualitatively and quantitatively.
01681b6e-45d2-48c9-a03e-88c0d75f738a mining association rules in learning management systems learning management systems collect huge amounts of data that can later be analysed. the university of rijeka uses mudri e-learning system, which is based on the moodle open source software. this paper focuses on the programming course, for which data over several years are available. the data can be interpreted and valuable knowledge can be obtained and used for improving the quality of lectures, as well as making the lectures more suitable for students based on the actions and material deemed the most popular. since the mudri database contains many facts that might affect each other (e.g. homework might affect the final grade), association rule mining, which discovers regularities in data, is the most suitable data mining method. apriori algorithm for the discovery of association rules is used for finding connections between various actions and final grades. many interesting rules and information are discovered, which lead to conclusions on actions that seem to be in relation with the course success.
01682f09-439d-4349-afce-83426a2e4f20 a bayesian based machine learning application to task analysis. 
01695a3a-15e7-450e-b46f-5656e6111dd9 machine learning from remote sensing analysis an intelligent system (seidam-system of experts for intelligent data management) is being developed for answering queries about the forests and the environment through the integration of remote sensing, geographic information, models and field measurements. seidam consists of an hierarchical group of expert systems. machine learning and planning can be used to create plans that execute image analysis software in order to recognize specific objects and perform a variety of different tasks. a query (task) could require, for example, that forest inventory stored in a gis be updated to reflect past harvesting. as sensors become more numerous, the choices of data and options to recognize objects become more complex. these complexities can be reduced by making use of case-based reasoning. only the data needed to answer the query will be used. the aim of case-based reasoning is to avoid having to build a solution to a problem from first principles, or by drawing on rare expertise, by adapting a known solution for an old problem to the new problem. there is a constantly growing variety of data sets. providing information at various degrees of accuracy and at often different cost. a non-expert user of this data could greatly benefit from reusing specific cases of queries, which convert the data into knowledge that other users were seeking before. a case, in this context, consists of a query and an example of the process (plan) that answers that query using a single or a multi-sensor data set, and geographic information, such as forest cover, topography, hydrology, etc. in their earlier work, the authors constructed a planner (lear), a planning system for creating expert systems by executing software for a case and interacting with a human expert. they now wish to raise the machine learning methods from creating expert systems for executing existing software to creating new rules (knowledge) derived from observing remote sensing analysis cases. knowledge about objects acquired by the lear planner can be used to assist a case-based reasoner during both its retrieval step and its adaptation step. >
0169b168-c971-4eba-b807-748a9d250bd5 a hybrid ensemble of machine and statistical learning using confidence-based boosting nowadays, the classification problems have become more challenging due to the various types of data set. some data are appropriated for machine learning techniques and some data are appropriated for statistical leaning techniques. this work proposes a new hybrid ensemble of machine and statistical learning models using confidence-based boosting. the proposed method which uses variants of based classifiers can solve classification problems in variant data set. moreover, combining the confidence value to the current boosting method can improve the performance of classification. the performance of proposed method is compared to the ensemble of decision trees and mrn created by adaboost.m1 on data sets from uci. the experimental results show that the proposed method can improve the accuracy in both binary and multiclass classification problems.
016a0bbc-d3dc-4372-bbed-3aac3a45083b the elements of statistical learning 
016a21f2-9a90-480b-986d-993d1b24ea84 multitask learning for protein subcellular location prediction protein subcellular localization is concerned with predicting the location of a protein within a cell using computational methods. the location information can indicate key functionalities of proteins. thus, accurate prediction of subcellular localizations of proteins can help the prediction of protein functions and genome annotations, as well as the identification of drug targets. machine learning methods such as support vector machines (svms) have been used in the past for the problem of protein subcellular localization, but have been shown to suffer from a lack of annotated training data in each species under study. to overcome this data sparsity problem, we observe that because some of the organisms may be related to each other, there may be some commonalities across different organisms that can be discovered and used to help boost the data in each localization task. in this paper, we formulate protein subcellular localization problem as one of multitask learning across different organisms. we adapt and compare two specializations of the multitask learning algorithms on 20 different organisms. our experimental results show that multitask learning performs much better than the traditional single-task methods. among the different multitask learning methods, we found that the multitask kernels and supertype kernels under multitask learning that share parameters perform slightly better than multitask learning by sharing latent features. the most significant improvement in terms of localization accuracy is about 25 percent. we find that if the organisms are very different or are remotely related from a biological point of view, then jointly training the multiple models cannot lead to significant improvement. however, if they are closely related biologically, the multitask learning can do much better than individual learning.
016a8772-eccc-4df1-b55e-c1f88c52da1e data-driven constructive induction in the learnable evolution model the learnable evolution model (lem) is a non-darwinian evolutionary computation method which applies symbolic machine learning to guide the evolutionary optimization process. this paper investigates application of data-driven constructive induction to automatically improve representation spaces in lem. this includes investigation of methods for modifying representation spaces and methods for creating new candidate solutions from hypotheses learned in the modified spaces. experimental results indicate that lem equipped with constructive induction outperforms lem working only in the original representation spaces.
016acc2f-94bd-4912-8411-bd21f7b82e2a the illusion of agency: two engineering approaches to compromise autonomy and reactivity in an artificial system this article describes and compares two approaches that can be used to build artificial systems that users tend to recognize as agents, based on a review of two systems previously built by the authors. one system, an interactive musical instrument, is a typical artificial intelligence (ai) implementation, based on the technique of constraint programming. the other, a dancing robot, is a typical artificial life (al) endeavor, specified as a non-linear dynamical system. although very different in their design, we found that both systems have the same goal of compromising autonomy and reactivity in their user interaction. they elicit interaction andawe argue hereaan elusive feeling of agency , because they are neither too predictable nor too random. creating and controlling such a compromise in a programmatic way is not a trivial problem: we find that both approaches (ai and al) raise similar pragmatic problems that are in fact rooted in human perception science. psychological experimentation is needed to clarify the relation between the internal dynamics of the artificial systems and the ongoing feeling of agency (or absence thereof) imparted in their human user. as a first step toward such experimentation, we derive a minimal mathematical model which subsumes both implementations and abstracts them from their respective contexts of music and dance. this model is similar to a van der pol oscillator, forced by an input signal coupled to its output in a non-programmatic way. it isolates two critical variables controlling the illusion of agency: the modelas sampling rate and the polynomial order of its reactive term.
016bd09c-4237-485c-a50e-dd5d92eb357a hybridization of web content and structure mining (hwcsm) technique by means of content based ranking algorithm current day scenario most of the applications moved on to web to enable any where computing. mining approaches are proved as the best to extract knowledge. organizations have to use various mining techniques to extract useful information. this information will help in day-to-day business functionality of organization. we can say web mining as the applications of the general data mining techniques to the web. however, the internal properties of the web force us to modify and extend the traditional techniques considerably. in this paper we are proposing an approach to hybridize web content and web structure mining to improve the performance of web mining. keywords: web mining, content mining, structure mining
016c30ac-6562-43de-9c2f-68944c9bdd7d a data science course for undergraduates: thinking with data data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. these data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. a first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. however, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. more importantly, these students require a framework for thinking structurally about data. we describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. the ...
016c5963-048a-4329-8631-7f56e96ca25a scheduling non-enforceable contracts among autonomous agents with the emergence of fast and standardized communication infrastructures over which separately designed agents of different organizations can interact in real-time, there is an increasing demand for cooperation mechanisms that allow to carry out inter-organizational cooperations in a safe way. the lack of external control over an agent's decisions, resources and actions hamper the usage of traditional transaction and workflow technology to make self-interested agents cooperate, i.e., agents cannot not be forced from a mediating cooperation instance to continue a cooperation. the challenge is, therefore, to design a cooperation mechanism that motivates cooperating agents to carry out a specified contract and, in case of unilateral defection, ensures that none of the cooperators can benefit from the situation. in this paper we present a domain independent framework how non-enforceable cooperations can be made safe against unilateral defection. we have developed a utility-based scheduling algorithm that keeps a cooperation in equilibrium and that motivates agents to continue a cooperation as long as it is for all participants beneficial.
016d9e3c-bf09-44a4-9f67-d0784d8dbbfb a neural network-based differential diagnosis assessment instrument medical educators have been unable to produce convincing evidence of the construct validity of written or simulation-based assessments of differential diagnosis (ddx) competencies. in 1987, a team of investigators at our institution introduced preliminary reports regarding the psychometric properties of an artificial intelligence-derived ddx assessment instrument. these investigations produced evidence of the construct validity (experts' ddx performance > novices') of the measures derived from this instrument, a linear, fuzzy set-like expert system.in this investigation, the authors used a non-linear, back propagation neural network as a ddx assessment instrument. an acute chest pain knowledge base was acquired from each of twenty-four board certified emergency medicine specialists and seventy-four junior and senior medical students. the neural network used these knowledge bases to simulate and assess each subject's individual ddx performance against twenty acute chest pain/myocardial infarction test ca...
016e5fdd-9b84-44e4-bb1e-31bebadbfbc8 groutability prediction of microfine cement based soil improvement using evolutionary ls-svm inference model abstractpermeation grouting is a widely used technique for soil improvement in construction engineering. thus, predicting the results of the grouting activity is a particularly interesting topic that has drawn the attention of researchers both from the academic field and industry. recent literature has indicated that artificial intelligence (ai) approaches for groutability prediction are capable of delivering better performance than traditional formula-based ones. in this study, a novel ai method, evolutionary least squares support vector machine inference model for groutability prediction (elsim-gp), is proposed to forecast the result of grouting activity that utilizes microfine cement grout. in the model, least squares support vector machine (ls-svm) is a supervised machine learning technique that is employed to learn the decision boundary for classifying high dimensional data. differential evolution (de) is integrated into elsim-gp for automatically optimizing its tuning parameters. 240 historical case...
017052ef-47d6-437c-827f-c29cc76e9471 a unified approach for discovery of interesting association rules in medical databases association rule discovery is an important technique for mining knowledge from large databases. data mining researchers have studied subjective measures of interestingness to reduce the volume of discovered rules and to improve the overall efficiency of the knowledge discovery in databases process (kdd). the objective of this paper is to provide a framework that uses subjective measures of interestingness to discover interesting patterns from association rules algorithms. the framework works in an environment where the medical databases are evolving with time. in this paper we consider a unified approach to quantify interestingness of association rules. we believe that the expert mining can provide a basis for determining user threshold which will ultimately help us in finding interesting rules. the framework is tested on public datasets in medical domain and results are promising.
0170bf20-2e58-4131-bab9-ca81f90f0c12 mining candidate viruses as potential bio-terrorism weapons from biomedical literature in this paper we present a semantic-based data mining approach to identify candidate viruses as potential bio-terrorism weapons from biomedical literature. we first identify all the possible properties of viruses as search key words based on geissler's 13 criteria; the identified properties are then defined using mesh terms. then, we assign each property an importance weight based on domain experts' judgment. after generating all the possible valid combinations of the properties, we search the biomedical literature, retrieving all the relevant documents. next our method extracts virus names from the downloaded documents for each search keyword and identifies the novel connection of the virus according to these 4 properties. if a virus is found in the different document sets obtained by several search keywords, the virus should be considered as suspicious and treated as candidate viruses for bio-terrorism. our findings are intended as a guide to the virus literature to support further studies that might then lead to appropriate defense and public health measures.
0170e16d-312e-485b-b4a3-65cacbaa1788 variational problems in machine learning and their solution with finite elements many machine learning problems deal with the estimation#r##n#of conditional probabilities $p(y \mid x)$ from data#r##n#$(x_1,y_i),\ldots,(x_n,y_n)$. this includes classification,#r##n#regression and density estimation. given a prior for#r##n#$p(y \mid x)$ the maximum a-posteriori method estimates#r##n#$p(y \mid x)$ as the most likely probability given the#r##n#data. this principle can be formulated rigorously  using the#r##n#cameron-martin theory of stochastic processes  and allows#r##n#a variational characterisation of the estimator.#r##n#the resulting nonlinear#r##n#galerkin equations are solved numerically. convexity and#r##n#total positivity lead to existence, uniqueness and error#r##n#bounds. for machine learning problems dealing with large#r##n#numbers of features we suggest to use sparse grid approximations.
0170f21d-34bb-4b77-a0b3-5ce0336b0b5b a new approach of dynamic monitoring of 5-day snow cover extent and snow depth based on modis and amsr-e data from northern xinjiang region abstract#r##n##r##n#taking the northern xinjiang region as an example, we develop a snow depth model by using the advanced microwave scanning radiometer-earth observing system (amsr-e) horizontal and vertical polarization brightness temperature difference data of 18 and 36 ghz bands and in situ snow depth measurements from 20 climatic stations during the snow seasons novembermarch) of 20022005. this article proposes a method to produce new 5-day snow cover and snow depth images, using terra and aqua moderate resolution imaging spectroradiometer (modis) daily snow cover products and amsr-e snow water equivalent and daily brightness temperature products. the results indicate that (1) the brightness temperature difference (tb18htb36h) provides the most accurate and precise prediction of snow depth; (2) the snow, land and overall classification accuracies of the new images are separately 89.2%, 77.7% and 87.2% and are much better than those of amsr-e or modis products (in all weather conditions) alone; (3) the snow classification accuracy increases as snow depth increases; and (4) snow accuracies for different land cover types vary as 88%, 92.3%, 79.7% and 80.1% for cropland, grassland, shrub, and urban and built-up, respectively. we conclude that the new 5-day snow coversnow depth images can provide both accurate cloud-free snow cover extent and the snow depth dynamics, which would lay a scientific basis for water management and prevention of snow-related disasters in this dry and cold pastoral area. after validations of the algorithms over other regions with different snow and climate conditions, this method would also be used for monitoring snow cover and snow depth elsewhere in the world. copyright  2011 john wiley & sons, ltd.
0171179a-0327-4d94-8eb1-5b394fd275b0 prediction of mechanical properties of hot rolled, low-carbon steel strips using artificial neural network artificial intelligence (ai) has been used in many application areas of engineering. in the present work, an artificial neural network (ann)-based model is developed to predict the mechanical properties such as yield strength (ys), ultimate tensile strength (uts), and elongation (el) of the hot rolled (hr) steel strips/coils. different network topologies have been investigated to find the appropriate network to simulate the problem. finally, the best network was chosen as the one with 7-19-3 topology-7 neurons in the input layer, 19 in the hidden layer, and 3 in the output layer. it has been shown that a single network with three output neurons is sufficient to address the problem. the model has been tested with 121 unknown patterns, and the match between the actual values and the simulated ones is found to be very good. the model has been implemented in the hot strip mill (hsm) of tata steel, india. this paper describes the methodology adopted to develop the model.
017142ce-92b4-4894-8545-763f1e10d65b multiple parameter control for ant colony optimization applied to feature selection problem the ant colony optimization algorithm (aco) was initially developed to be a metaheuristic for combinatorial optimization problem. in scores of experiments, it is confirmed that the parameter settings in aco have direct effects on the performance of the algorithm. however, few studies have specially reported the parameter control for aco. the aim of this paper was to put forward some strategies to adaptively adjust the parameter in aco and further provide a deeper understanding of aco parameter control, including static and dynamic parameters. we choose well-known ant system (as) and ant colony system (acs) to be controlled by our proposed strategies. the parameters in as and acs include , pheromone evaporation rate (?), exploration probability factor (q0) and number of ants (m). we have proposed three adaptive parameter control strategies (si, sii and siii) based on fuzzy logic control which adjusts ?, q0 and m, respectively. the feature selection problem is considered for evaluating the parameter control strategies. in addition, because as and acs are not intrinsically fit for feature selection problem, we have modified the as and acs, which are named as fuzzy adaptive ant system (faas) and fuzzy adaptive ant colony system (faacs), to make them more suitable for feature selection problem. because only one parameter is allowed to be dynamically adjusted in faas or faacs, the remaining parameters should be statically specified. thus, we have developed parametric guidelines for proper combination of static parameter settings. the performance of faas and faacs is compared with that of the as-based, acs-based, particle swarm optimization-based and genetic algorithm-based methods on a comprehensive set of 10 benchmark data sets, which are taken from uci machine learning and statlog databases. the numerical results and statistical analysis show that the proposed algorithms outperform significantly than other methods in terms of prediction accuracy with smaller subset of features.
017159a2-f5ba-42f9-8fcf-d719f099f7f2 an integer support vector machine data mining is a technique to discover patterns and trends in data and can be used to create a model to predict those patterns and trends. this is particularly useful for data sets that are not amenable to traditional statistical analysis. one particular data mining task is classification, predicting a quantity that can only take on a finite number of values. an important class of binary classifiers are support vector machines (svms). traditional svms use constrained optimization to find a separating hyperplane. a new data point is classified based on which side of the separating hyperplane it happens to fall on. all svms try to minimize the number of potential errors the classifier makes by minimizing a sum of distances from the hyperplane. however, the actual task of classification does not place any importance on a distance. in order to model this more closely, we propose the integer support vector machine classifier (isvm). isvm uses binary indicator error variables to directly minimize the number of potential errors the classifier can make.
0171f1f7-6d5e-4bf9-b763-f77b2b1e22fb a selective review of research in content personalization purpose  to provide a selective bibliography in the emerging area of library content personalization for the benefit of library and information professionals.design/methodology/approach  a range of recently published works (in the period 19932004), which aim to provide pragmatic application of content personalization rather than theoretical works, are discussed and sorted into classified sections to help library professionals understand more about the various options for formulating content as per the specific needs of their clientele.findings  this paper provides information about each category of tool and technique of personalization, indicating what is achieved and how particular developments can help other libraries or professionals. it recognises that personalization of library resources is a viable way of helping users deal with the information explosion, conserving their time for more productive intellectual tasks. it identifies how computer and information technology has enabled document map...
01722a2a-c356-41bd-8b3a-e7e66aefe59f teaching introductory artificial intelligence through java-based games we introduce a java graphical gaming framework that enables students in an introductory artificial intelligence (ai) course to immediately apply and visualize the topics from class. we have used this framework in teaching a mixed undergraduate/graduate ai course for six years. we believe that the use of games motivates students. the graphical nature of each game enables students to quickly see how well their algorithm works. because the topics in an introductory ai course vary widely, students apply their algorithms to multiple game environments. a final challenging environment enables them to tie together the concepts for the entire semester.
017234cb-0fb8-42f8-8ba5-47265542a4ba a comparison of two data mining techniques to predict abnormal stock market returns two data mining techniques were compared for their ability to improve the prediction of abnormal returns using insider stock trading data. the two were neural networks (nn) and multivariate adaptive regressive splines (mars). in the comparison, both analyzed abnormal stock market returns from the same 343 companies over the identical 4\frac{1}{2} year period (1/93-6/97). the major findings were: 1) both nn and mars generally identified the same industries that had the most predictive abnormal stock returns 2) both found that predictions further in the future (12 and 9 months ahead) were more accurate than predictions closer to the trading date (6 and 3 months ahead) 3) both obtained better predictive accuracy using four - rather than two - months of back aggregated stock data 4) nn identified a substantially greater percentage of stocks in the group with the highest explained variance than did mars 5) data from small and midsize companies led to higher predictive accuracy than data from large size (s&p 500) companies using nn, but not mars. the findings illustrate that the very complex interaction between insider trading data and abnormal stock returns can be systematically analyzed using non-linear techniques. of the two assessed, nn led to comparatively more accurate predictions than did mars.
017272c0-85c9-4cd4-a597-9f47f2169b3f extensiveform games with heterogeneous populations: solution concepts, equilibria characterization, learning dynamics the adoption of nash equilibrium (ne) in real-world settings is often impractical due to its too restrictive assumptions. game theory and artificial intelligence provide alternative (relaxed) solution concepts. when knowledge about opponents' utilities and types are not available, the appropriate solution concept for extensive-form games is the self- confirming equilibrium (sce), which relaxes ne allowing agents to have wrong beliefs off the equilibrium path. in this paper, we provide the first computational and learning study of the situations in which a two-agent extensive-form game is played by heterogeneous populations of individuals that repeatedly match (e.g., ebay): we extend the sce concept, we study the equilibrium computation problem, and we study how these equilibria affect learning dynamics. we show that sces are crucial for characterizing both stable states of learning dynamics and the dynamics themselves.
0172ed7a-a57e-4523-aaf7-6010da1f100b interval type-2 fuzzy classifier design using genetic algorithms this paper aims at investigating the advantages of using an interval type-2 fuzzy system for classification problems. an evolutionary architecture was proposed to generate the rule base and to optimize the membership functions of a type-2 fuzzy classification system the proposed architecture is composed of three stages. in the first stage of the architecture, a genetic algorithm generates the rule base of the fuzzy classification system using predefined and fixed membership functions. in the second stage, another genetic algorithm optimizes the interval type-2 membership functions that were used in the first stage. finally, a third genetic algorithm is used for the optimization of the number of rules in the best fuzzy classification system generated in the two previous stages. some experiments have been run using different datasets from the uci machine learning repository in order to validate the proposed approach and to compare the results with the ones obtained with the wang&mendel method and a type-1 fuzzy classification system also generated by the evolutionary architecture proposed here. the results demonstrated that the type-2 fuzzy classification system performed better than the other classifiers used in the study.
0173032f-56fb-41a7-9742-bb971d5fc580 hvsm: a new sequential pattern mining algorithm using bitmap representation sequential pattern mining is an important problem for data mining with broad applications. this paper presents a first-horizontal-last-vertical scanning database sequential pattern mining algorithm (hvsm). hvsm considers a database as a vertical bitmap. the algorithm first extends itemsets horizontally, and digs out all one-large-sequence itemsets. it then extends the sequence vertically and generates candidate large sequence. the candidate large sequence is generated by taking brother-nodes as child-nodes. the algorithm counts the support by recording the first tid mark (1st-tid). experiments show that hvsm algorithm can find frequent sequences faster than spam algorithm in mining the large transaction databases.
01732808-7b7b-48c3-8d58-ff52744d701f a solution on the enterprise knowledge management with complexity from the point of the complexity of knowledge, based on the complex adaptive system theory, combined with computer science and artificial intelligence theories, the author analyzes the complexity of enterprise knowledge management and provides a solution to help enterprises to establish their own knowledge management system.
01732ef2-1aa9-4c17-83e6-b57cc7ea588a machine learning techniques to diagnose breast cancer machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to learn from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. as a result, machine learning is frequently used in cancer diagnosis and detection. in this paper, support vector machines, k-nearest neighbours and probabilistic neural networks classifiers are combined with signal-to-noise ratio feature ranking, sequential forward selection-based feature selection and principal component analysis feature extraction to distinguish between the benign and malignant tumours of breast. the best overall accuracy for breast cancer diagnosis is achieved equal to 98.80% and 96.33% respectively using support vector machines classifier models against two widely used breast cancer benchmark datasets.
01736317-a46c-483a-a2b8-ffe0c177da28 chapter 2  rules for managers it is a mistake to view a data science problem as  just  a management exercise or  just  a technical issue. some of the infrastructure required for success cannot be mandated or boughtsome issues require attitudinal changes within the personnel involved in the project. in this part of the book  data science for software engineering: sharing data and models , some of those changes are listed. they include (1) talk to your users more than your algorithms; (2) know your maths and tools but, just as important, know your problem domain; (3) suspect all collected data and, for all such data, rinse before use; (4) data science is a cyclic process, which means you will get it wrong (at first) along the way to getting it right.
01737d61-a685-45e4-bcbf-c5d48abc7cc6 a novel approach for extraction and representation of main data from web pages to android application world wide web is the largest and richest information repository available today. the enormous growth of web makes large scale analysis difficult. an automated method is needed for analyzing and extracting knowledge from the web, along with samples of the knowledge that can be extracted, without user intervention. data mining techniques are applied to extract knowledge from web data. web data is text, image, records, hyperlinks, tags et. rich site summary (rss) is a way to get briefed items on a website as the web site gets updated, and these are called feeds. a feed is often a series of headlines and brief summaries of all the articles published on a web page. instead of having to visit numerous web sites to get weather, sports, latest gossip, or latest political debates etc, just go to the mobile application and see it combined and aggregated into a single window with a good user interface. an application is developed for the popular mobile platform android.
0173b3c7-229d-45bd-9b26-a1b9d67756ec data mining for burr detection (in the drilling process) drilling is the most important operation in aeronautic industry carried out previous to riveting. its main problem lies with the burrs. nowadays, there is a burr elimination task (manual task) subsequent to drilling and previous to riveting that increases manufacturing cost. it is necessary to develop a monitoring system to detect automatically and on-line when the generated burr is out of aeronautic limits, and then deburring. this system would reduce holes deburring to the holes which really are out of tolerance limits, focusing in trying to avoid false negatives. the article shows an improvement in burr generation prediction, using data mining techniques versus current mathematical model. it gives an overview of the process from data preparation and selection to data analysis (with machine learning algorithms) and evaluation of the models.
0173e264-fcb0-4f34-a206-081ac027125a labeling update of segmented images using conceptual graphs and dempster-shafer theory of evidence the extraction of objects present in photographs is a major problem to tackle when considering photograph retrieval. such objects (or people) have to be detected in a way to allow retrieval based on concepts and not on physical characteristics like colors or textures. to achieve a reasonable detection rate, in-context image analysis has to be used. we propose to use conceptual graphs (a knowledge representation formalism that allow fast processing) with dempster-shafer theory of evidence to update original labeling coming from a segmentation that labels image regions out of context. we explain the whole process and the results obtained on real home photos. the encouraging results obtained show the potential of our proposal.
0174b114-94d8-44e9-9a63-928abeb38ff0 fuzzy intelligence approach for modeling the migration of contaminants in a reservoir affected by amd pollution the sancho reservoir, located in the huelva province (sw spain), is supplied by the meca river, which receives water contaminated by mining activities in tharsis. this study focused on determining the relationship that temperature, ph, and electrical conductivity (ec) had with rainfall. the temperature, ph, and ec were simultaneously measured every 30 min by two probes suspended in the sancho reservoir. it was anticipated that the use of fuzzy logic and data mining would lead to a model that would show how the contaminant load evolved over space and time. similar results were obtained for the two locations, except that the parameters had more outliers near the dam due to the greater distance from the contamination source. as expected, higher ph corresponded with lower ec, since, in the absence of chloride, sulphate was the principal anion. the dependency relationship of the variables as well as the causeeffect relationship with the rate of rainfall was more evident in the up-gradient sampling location than near the dam due to the different residence time and the transit time between the two points.
0174f42e-3dc8-41f7-acbc-96df6eaa5634 an autonomous network aware vm migration strategy in cloud data centres live virtual machine migration can have a major impact on how a cloud system performs, as it can consume significant amounts of network resources such as bandwidth. a virtual machine migration occurs when a host becomes over-utilised or under utilised. migration contributes to an increase in consumption of network resources which leads to longer migration times and ultimately has a detrimental effect on the performance of a cloud system. in this paper, we propose an autonomous network aware virtual machine migration strategy that observes the current demand level of a network and performs appropriate actions based on what it is experiencing. the artificial intelligence technique known as reinforcement learning acts as a decision support system, enabling an agent to learn an optimal time to schedule a virtual machine migration depending on the current network traffic demand. we show that an autonomous agent can learn to utilise available network resources when network saturation occurs at peak times.
01763ff8-5ebe-4538-a18d-ad4d58a6324e adaptive fighting game computer player by switching multiple rule-based controllers this paper proposes the design of a computer playerplayer. for fighting games that has the advantages of both rule-based and online machine learning players. this method combines multiple computer players and switches control of the fighting character among them at regular time intervals. in this way the computer player as a whole tries to act advantageously against the current opponent player. to select appropriate controllers against the opponent out of the multiple players, we use the sliding window upper confidence bound (sw-ucb) algorithm that is designed for non-stationary multi-armed bandit problems. we use the fightingice platform as a testbed for our proposed method. some experiments show the effectiveness of our proposed method in fighting games. the computer player consists of three rulebased computer players, and our method outperforms each of the three players. additionally the proposed method improves the performance slightly against an online machine learning player.
01764666-a458-4d30-9116-d9c7b6df2f46 autonomous gaussian decomposition we present a new algorithm, named autonomous gaussian decomposition (agd), for automatically decomposing spectra into gaussian components. agd uses derivative spectroscopy and machine learning to provide optimized guesses for the number of gaussian components in the data, and also their locations, widths, and amplitudes. we test agd and find that it produces results comparable to human-derived solutions on 21 cm absorption spectra from the 21 cm spectral line observations of neutral gas with the evla (21-sponge) survey. we use agd with monte carlo methods to derive the h i line completeness as a function of peak optical depth and velocity width for the 21-sponge data, and also show that the results of agd are stable against varying observational noise intensity. the autonomy and computational efficiency of the method over traditional manual gaussian fits allow for truly unbiased comparisons between observations and simulations, and for the ability to scale up and interpret the very large data volumes from the upcoming square kilometer array and pathfinder telescopes.
017662eb-7d7f-43f8-b46d-d5ad71e6d10b research of port bulk logistics comprehensive information integrated platform the comprehensive information integrated platform on port bulk logistics is constructed on the basis of system analysis and design in terms of hierarchical structure, logic structure and physical structure. the technology of data mining, distributed database and information integrated are adopted to realize the function of sharing resource, decision-support and information-transparent. the system will improves the performance of port bulk logistics organization, shortens the circle period of vehicle and ship, enhances the port capacity and efficiently reduces the occurrence of ship waiting for cargo or cargo waiting for ship in port.
0176cc6a-e024-427e-b1f3-051e09e2b555 how do we evaluate artificial immune systems the field of artificial immune systems (ais) concerns the study and development of computationally interesting abstractions of the immune system. this survey tracks the development of ais since its inception, and then attempts to make an assessment of its usefulness, defined in terms of 'distinctiveness' and 'effectiveness.' in this paper, the standard types of ais are examinednegative selection, clonal selection and immune networksas well as a new breed of ais, based on the immunological 'danger theory.' the paper concludes that all types of ais largely satisfy the criteria outlined for being useful, but only two types of ais satisfy both criteria with any certainty.
017712ef-0466-4285-9cc2-3727c20f05eb multiple-criteria genetic algorithms for feature selection in neuro-fuzzy modeling this paper discusses the use of multicriteria genetic algorithms for feature selection in classification problems. this feature selection approach is shown to yield a diverse population of alternative feature subsets with various accuracy/complexity trade-off. the algorithm is applied to select features for performing classification with fuzzy models, and is evaluated on two real-world data sets. we discuss when multicriteria genetic algorithm feature selection is preferable to a sequential feature selection procedure, namely backwards elimination. among the key features of the presented approach are its computational simplicity, effectiveness on real world problems and the potential it has to become a powerful tool aiding many empirical modeling and data mining processes.
0177179c-b258-498e-9f24-0ae9aecc0d9c robust automatic target recognition using learning classifier systems this work developed and demonstrated a machine learning approach for robust atr. the primary innovation of this work was the development of an automated way of developing inference rules that can draw on multiple models and multiple feature types to make robust atr decisions. the key realization is that this ''meta learning'' problem is one of structural learning, and that it can be conducted independently of parameter learning associated with each model and feature based technique. this was accomplished by using a learning classifier system, which is based on genetics-based machine learning, for the ill conditioned combinatorial problem of structural rule learning, while using statistical and mathematical techniques for parameter learning. this system was tested on mstar public release sar data using standard and extended operation conditions. these results were also compared against two baseline classifiers, a pca based distance classifier and a mse classifier. the classifiers were evaluated for accuracy (via training set classification) and robustness (via testing set classification). in both cases, the lcs based robust atr system performed well with accuracy over 99% and robustness over 80%.
017728c9-71a4-47ca-9a4b-bdda2119b62d a deep neural network modeling framework to reduce bias in satellite precipitation products abstractdespite the advantage of global coverage at high spatiotemporal resolutions, satellite remotely sensed precipitation estimates still suffer from insufficient accuracy that needs to be improved for weather, climate, and hydrologic applications. this paper presents a framework of a deep neural network (dnn) that improves the accuracy of satellite precipitation products, focusing on reducing the bias and false alarms. the state-of-the-art deep learning techniques developed in the area of machine learning specialize in extracting structural information from a massive amount of image data, which fits nicely into the task of retrieving precipitation data from satellite cloud images. stacked denoising autoencoder (sdae), a widely used dnn, is applied to perform bias correction of satellite precipitation products. a case study is conducted on the precipitation estimation from remotely sensed information using artificial neural networks cloud classification system (persiann-ccs) with spatial resolution of ...
01774911-2361-4823-a67a-aafb883a0e68 scene content selected by active vision. the primate visual system actively selects visual information from the environment for detailed processing through mechanisms of visual attention and saccadic eye movements. this study examines the statistical properties of the scene content selected by active vision. eye movements were recorded while participants free-viewed digitized images of natural and artificial scenes. fixation locations were determined for each image and image patches were extracted around the observed fixation locations. measures of local contrast, local spatial correlation and spatial frequency content were calculated on the extracted image patches. replicating previous results, local contrast was found to be greater at the points of fixation when compared to either the contrast for image patches extracted at random locations or at the observed fixation locations using an image-shuffled database. contrary to some results and in agreement with other results in the literature, a significant decorrelation of image intensity is observed between the locations of fixation and other neighboring locations. a discussion and analysis of methodological techniques is given that provides an explanation for the discrepancy in results. the results of our analyses indicate that both the local contrast and correlation at the points of fixation are a function of image type and, furthermore, that the magnitude of these effects depend on the levels of contrast and correlation present overall in the images. finally, the largest effect sizes in local contrast and correlation are found at distances of approximately 1 deg of visual angle, which agrees well with measures of optimal spatial scale selectivity in the visual periphery where visual information for potential saccade targets is processed.
01775488-30fe-40a2-ba96-f6ca738d5906 analysis of change in coordinate system on clustering clustering is one of the data mining techniques used in a knowledge discovery process. it is assumed that a good representation of data points may yield good clustering results [6]. this paper discusses the effect of the coordinate system on the clustering. in this paper, we propose a density based clustering approach to group objects represented using polar coordinate system. the experiment is carried out on different datasets. to evaluate the goodness of cluster result we have used internal and external validity measures. in most of the cases, data points representation using conventional cartesian coordinate system results in better clustering performance as compared to clustering obtained from same data represented using polar coordinate system. however, it is observed that for certain data sets our proposed density based approach on polar coordinates clustering results outperform conventional approach. hence, we can conclude that an appropriate representation of data points may yield more appropriate clustering results.
01779f98-423e-4556-93bd-20cffa6f620a can encrypted traffic be identified without port numbers, ip addresses and payload inspection? identifying encrypted application traffic represents an important issue for many network tasks including quality of service, firewall enforcement and security. solutions should ideally be both simple  therefore efficient to deploy  and accurate. this paper presents a machine learning based approach employing simple packet header feature sets and statistical flow feature sets without using the ip addresses, source/destination ports and payload information to unveil encrypted application tunnels in network traffic. we demonstrate the effectiveness of our approach as a forensic analysis tool on two encrypted applications, secure shell (ssh) and skype, using traces captured from entirely different networks. results indicate that it is possible to identify encrypted traffic tunnels with high accuracy without inspecting payload, ip addresses and port numbers. moreover, it is also possible to identify which services run in encrypted tunnels.
0177e184-043b-4267-8646-e479f1bee700 a new control architecture for robust controllers in rear electric traction passenger hevs it is well known that control systems are the core of electronic differential systems (edss) in electric vehicles (evs)/hybrid hevs (hevs). however, conventional closed-loop control architectures do not completely match the needed ability to reject noises/disturbances, especially regarding the input acceleration signal incoming from the driver's commands, which makes the eds (in this case) ineffective. due to this, in this paper, a novel eds control architecture is proposed to offer a new approach for the traction system that can be used with a great variety of controllers (e.g., classic, artificial intelligence (ai)-based, and modern/robust theory). in addition to this, a modified proportional-integral derivative (pid) controller, an ai-based neuro-fuzzy controller, and a robust optimal  h    controller were designed and evaluated to observe and evaluate the versatility of the novel architecture. kinematic and dynamic models of the vehicle are briefly introduced. then, simulated and experimental results were presented and discussed. a hybrid electric vehicle in low scale (helvis)-sim simulation environment was employed to the preliminary analysis of the proposed eds architecture. later, the eds itself was embedded in a dspace 1103 high-performance interface board so that real-time control of the rear wheels of the helvis platform was successfully achieved.
01787480-b20c-4cd6-be98-d97e05f351f6 learning shapes for image classification and retrieval shape descriptors have been used frequently as features to characterize an image for classification and image retrieval tasks. for example, the patent office uses the similarity of shape to ensure that there are no infringements of copyrighted trademarks. this paper focuses on using machine learning and information retrieval techniques to classify an image into one of many classes based on shape. in particular, we compare support vector machines, naive bayes and relevance language models for classification. our results indicate that, on the mpeg-7 database, the relevance model outperforms the machine learning techniques and is competitive with prior work on shape based retrieval. we also show how the relevance model approach may be used to perform shape retrieval using keywords. experiments on the mpeg-7 database and a binary version of the coil-100 database show good retrieval performance.
0178b767-4954-45e4-9639-cf7e8d3b6808 an approach to structure determination and estimation of hierarchical archimedean copulas and its application to bayesian classification copulas are distribution functions with standard uniform univariate marginals. copulas are widely used for studying dependence among continuously distributed random variables, with applications in finance and quantitative risk management; see, e.g., the pricing of collateralized debt obligations (hofert and scherer, quantitative finance, 11(5), 775---787, 2011). the ability to model complex dependence structures among variables has recently become increasingly popular in the realm of statistics, one example being data mining (e.g., cluster analysis, evolutionary algorithms or classification). the present work considers an estimator for both the structure and the parameters of hierarchical archimedean copulas. such copulas have recently become popular alternatives to the widely used gaussian copulas. the proposed estimator is based on a pairwise inversion of kendall's tau estimator recently considered in the literature but can be based on other estimators as well, such as likelihood-based. a simple algorithm implementing the proposed estimator is provided. its performance is investigated in several experiments including a comparison to other available estimators. the results show that the proposed estimator can be a suitable alternative in the terms of goodness-of-fit and computational efficiency. additionally, an application of the estimator to copula-based bayesian classification is presented. a set of new archimedean and hierarchical archimedean copula-based bayesian classifiers is compared with other commonly known classifiers in terms of accuracy on several well-known datasets. the results show that the hierarchical archimedean copula-based bayesian classifiers are, despite their limited applicability for high-dimensional data due to expensive time consumption, similar to highly-accurate classifiers like support vector machines or ensemble methods on low-dimensional data in terms of accuracy while keeping the produced models rather comprehensible.
017969ee-b2ff-49f2-b787-1acf691d5f3a construction of the system framework of spatial data warehouse in internet of things environments in the era of the internet of things (iot), the information world and the entity world are gradually integrated. thus, the ways that people obtain, transmit, store, update and analyze information are also undergoing profound change. these changes will undoubtedly bring some new challenges to the traditional spatial data warehouse (sdw) system. based on the analysis of the technical characteristics of iot and its potential application trends which combine iot technology with geographic information, firstly, we proposed an architecture solution for the spatial data warehouse system in iot environments (sdwit). secondly, the sdwit system was constructed which supported by the architecture solution. in this paper, the data source layer, the data processing layer, the data storage layer and the application analysis layer of the sdwit system are designed in detail. together, the four layers achieve the collection, transmission, storage and update for the real-time or near real-time information of perceived entities. thus, the sdwit system realizes data integration and data fusion for the abstract data type data (adtd) and entity perception data (epd), and improves the ability of data analysis and data mining. at the same time, it also ensures the interchange and process between person and person, person and things, things and things, and utilizes the integration advantage to compensate for some deficiencies of the traditional sdw system. finally, in the view of technology integration application of iot technology fusing with gps, virtual reality and mobile gis, a next generation innovation application mode of iot under the support of the sdwit system was briefly discussed.
0179951b-08dc-4e92-a401-9d910ad75a96 onboard feature indexing from satellite lidar images the purpose of the onboard feature indexing system is to perform pattern recognition and data compression onboard. we use the unsupervised machine learning algorithm kmeans to classify the lidar profile data and generate an index dictionary. then we train the radial basis function neural network with the index dictionary on ground computers. finally, we use the same rbf model for the onboard feature recognition and indexing. we implemented a prototype of the onboard computer with zisc (zero instruction set computing) chips and fpga (field programmable gate array) so that it takes advantage of intrinsic parallel computing and reconfigurability. we tested a set of 44k profiles as the training set to learn prototypical profiles that make up the indexing dictionary. with 64 indices, we reach a high compression rate 99.17% with reasonable error range. we found the required neurons are equal to the indices. we also compared our method to wavelet algorithm and found that it significantly outperforms the wavelet compression technique.
017a1c6f-0704-4bb2-bdfc-11eb18e12eeb reinforcement learning based control of tumor growth with chemotherapy in this paper, optimal drug schedule for patients in progressive cancer phase who take the drug through infusion pump is obtained. an objective of control is reducing tumor cell numbers effectively while minimizing total amount of drug regimen. this is done because of the known serious side effects and major damages resulting from chemotherapy. chemotherapy brings about weakness of the patient's immune system which is one of the most dangerous side effects. the optimal control problem is to design an effective drug-schedule to reduce the size of the tumors in a time-optimal fashion. to achieve this goal, a reinforcement learning (rl), which is one of the best unsupervised machine learning algorithms, is proposed for control. because rl has no need of environment model, i.e. it is model-free; it has absorbed interests during the recent year, especially in medical applications. performance evaluation of the proposed algorithm has been performed by simulating on the mathematical model of tumor cells interacting with immune system. simulation results show that a burst of treatment at the beginning is the best way to battle the tumor and constant decreasing the dosage of drug let the immune system to be reconstructed.
017ab0f3-9fbf-4412-aaf8-1738ce1d0a73 andromaly: a behavioral malware detection framework for android devices this article presents andromaly--a framework for detecting malware on android mobile devices. the proposed framework realizes a host-based malware detection system that continuously monitors various features and events obtained from the mobile device and then applies machine learning anomaly detectors to classify the collected data as normal (benign) or abnormal (malicious). since no malicious applications are yet available for android, we developed four malicious applications, and evaluated andromaly's ability to detect new malware based on samples of known malware. we evaluated several combinations of anomaly detection algorithms, feature selection method and the number of top features in order to find the combination that yields the best performance in detecting new malware on android. empirical results suggest that the proposed framework is effective in detecting malware on mobile devices in general and on android in particular.
017b3f19-9f06-4d2c-bc9c-2eb0e554391d deductive data modeling: a new trend in database management for decision support systems abstract   researches in data base are now placing more and more emphasis on deductive data base management systems (d-dbms). this paper discusses the use of d-dbms in decision support systems (dss). a d-dbms is an outgrowth of deductive systems in artificial intelligence (ai) and data base management systems (dbms). initially, the paper discusses deductive systems and evaluative systems. a framework is then presented which ties the two systems together. a macro definition of a deductive data model is also presented.
017b4a74-03ee-418f-af3e-bbfbcaf8356a a tale of two crowds: public engagement in plankton classification big data are becoming common in biological oceanography with the advent of sampling technologies that can generate multiple, high-frequency data streams. given the need for big data in ocean health assessments and ecosystem management, identifying and implementing robust and efficient processing approaches is a challenge for marine scientists. using a large plankton imagery data set, we present two crowd-sourcing approaches applied to the problem of classifying millions of organisms. the first used traditional crowd-sourcing by asking the public to identify plankton through a web-interface. the second challenged the data science community to develop algorithms via an industry partnership. we found traditional crowd-sourcing was an excellent way to engage and educate the public while crowd-sourcing data scientists rapidly generated multiple, effective solutions. as the need to process and visualize large and complex marine data sets is expected to grow over time, effective collaborations between oceanographers and computer and data scientists will become increasingly important.
017bb974-acf0-4e8e-b499-b687d135312c correlation based effective periodic pattern extraction from multimedia data periodic pattern mining, an interdisciplinary field of data mining is concerned with analyzing large volumes of time series or temporal data to discover patterns or trends or certain characteristics of data automatically. temporal data captures the evolution of a data value over time. the existing periodicity mining process is text-based which can be applied only to text data. the project proposed deals with the periodic patterns in multimedia data which includes text as well as audio and images. multimedia data such as digital images and audio can be treated as temporal values, since a timestamp is implicitly attached to every instant of the signal. a cross correlation based approach is proposed for periodic mining of multimedia data which has its main application in pattern recognition. in multimedia data mining, when the same signal is compared to phase shifted copies of itself, the procedure is known as autocorrelation basically cross correlation is a mathematical tool for finding repeating patterns in periodic signals by analyzing the degree of similarity between them. the periodic pattern retrieved from text data has its application in prediction, forecasting and detection of anomalies or unusual activities. the patterns extracted from audio and image finds its application in content based retrieval, compression and segmentation.
017d1bc0-9505-494f-9192-6b7ab1f4407a the sage handbook of grounded theory part one: origins and history grounded theory in historical perspective: an epistemological account - antony bryant & kathy charmaz discovery of grounded theory in practice: the legacy of multiple mentors - eleanor krassner covan living grounded theory: cognitive and emotional forms of pragmatism - susan leigh star part two: grounded theory method and formal grounded theory doing formal theory - barney g glaser on solid ground: essential properties for growing grounded theory - phyllis noerager stern from the sublime to the meticulous: the continuing evolution of grounded formal theory - margaret h kearney orthodoxy versus power: the defining traits of grounded theory - jane hood part three: grounded theory in practice grounding categories - ian dey development of categories: different approaches in grounded theory - udo kelle abduction: the logic of discovery of grounded theory - jo reichertz sampling in grounded theory - janice morse asking questions of the data: memo writing in the grounded theory tradition - lora lempert the coding process and its challenges - judith holton part four: practicalities making teams work in conducting grounded theory - carolyn wiener teaching grounded theory - sharlene nagy hesse-biber the evolving nature of grounded theory method: the case of the information systems discipline - cathy urquhart part five: grounded theory in the research methods context grounded theory using situational analysis - adele e clarke and carrie friese what can grounded theorists and action researchers learn from each other? - bob dick feminist qualitative research and grounded theory: complexities, criticisms, and opportunities - virginia l olesen accommodating critical theory - barry gibson grounded theory and the politics of interpretation - norman k denzin grounded theory & racail/ ethnic diversity - denise o'neil green, john w. creswell, ronald j. shope, vicki l. plano clark advancing ethnographic research through grounded theory practice - stefan timmermans and iddo tavory part six: grounded theory in the context of the social sciences grounded theory and reflexivity - katja mruck and guenter mey mediating structure and interaction in grounded theory - bruno hildenbrand rational control and irrational free-play: dual-thinking modes as necessary tension in grounded theorizing - karen locke research as pragmatic problem-solving: the pragmatist roots of empirically-grounded theorizing - joerg struebing
017d3d58-d759-4cec-bd53-d1c30a352b66 time series subsequence matching based on a combination of pip and clipping subsequence matching is a non-trivial task in time series data mining. in this paper, we introduce our proposed approach for solving subsequence matching which is based on ipip, our new method for time series dimensionality reduction. the ipip method is a combination of pip (perceptually important points) method and clipping technique in order that the new method not only satisfies the lower bounding condition, but also provides a bit level representation for time series. furthermore, we can make ipip indexable by showing that a time series compressed by ipip can be indexed with the support of skyline index. our experiments show that our ipip method is better than paa in terms of tightness of lower bound and pruning power, and in subsequence matching, ipip with skyline index can perform faster than paa based on traditional r*- tree.
017d8c23-298e-4aa2-bda2-9fd81cf85046 stochastic optimization for collision selection in high energy physics artificial intelligence has begun to play a critical role in basic science research. in high energy physics, ai methods can aid precision measurements that elucidate the underlying structure of matter, such as measurements of the mass of the top quark. top quarks can be produced only in collisions at high energy particle accelerators. most collisions, however, do not produce top quarks and making precise measurements requires culling these collisions into a sample that is rich in collisions producing top quarks (signal) and spare in collisions producing other particles (background). collision selection is typically performed with heuristics or supervised learning methods. however, such approaches are suboptimal because they assume that the selector with the highest classification accuracy will yield a mass measurement with the smallest statistical uncertainty. in practice, however, the mass measurement is more sensitive to some backgrounds than others. this paper presents a new approach that uses stochastic optimization techniques to directly search for selectors that minimize statistical uncertainty in the top quark mass measurement. empirical results confirm that stochastically optimized selectors have much smaller uncertainty. this new approach contributes substantially to our knowledge of the top quark's mass, as the new selectors are currently in use selecting real collisions.
017e00b3-63e2-4b2a-8c73-e9d15bb89c2a how the choice of safety performance function affects the identification of important crash prediction variables. across the nation, researchers and transportation engineers are developing safety performance functions (spfs) to predict crash rates and develop crash modification factors to improve traffic safety at roadway segments and intersections. generalized linear models (glms), such as poisson or negative binomial regression, are most commonly used to develop spfs with annual average daily traffic as the primary roadway characteristic to predict crashes. however, while more complex to interpret, data mining models such as boosted regression trees have improved upon glms crash prediction performance due to their ability to handle more data characteristics, accommodate non-linearities, and include interaction effects between the characteristics. an intersection data inventory of 36 safety relevant parameters for three- and four-legged non-signalized intersections along state routes in alabama was used to study the importance of intersection characteristics on crash rate and the interaction effects between key characteristics. four different spfs were investigated and compared: poisson regression, negative binomial regression, regularized generalized linear model, and boosted regression trees. the models did not agree on which intersection characteristics were most related to the crash rate. the boosted regression tree model significantly outperformed the other models and identified several intersection characteristics as having strong interaction effects. language: en
017e11c3-98ce-426e-afdf-d862229db4fd integration of text and data mining components and features of a text mining solution will be presented, illustrating the process fi-om accessing the document through the various processing steps. as a result of these steps, the texts will ultimately be transformed into a statisticalhumerical representation, enabling their use in an integrated text mining/data mining environment. an example illustrating such a use will be presented, together with some comparative performance data, showing the benfit of supplementing traditional data mining with text information.
017e7831-324a-4bc5-b0d9-778e48697223 detecting heterogeneity in pv modules from massive real-world step i-v curves: a machine learning approach we demonstrate that i-v curves with bypass diodes in forward bias can be useful in learning the heterogeneity in pv modules. in the laboratory-based experiments, we show that heterogeneity in a pv module can be detected from step iv curves that are collected under non-uniform irradiance. on the other hand, heterogeneous cell performance can lead to bypassing even under uniform irradiance. this hypothesis was tested using a fabricated 4-cell mini-module with cells that were engineered to have highly heterogeneous front contact resistivity and a spice-based circuit model. we find good agreement between the experimentally determined curve and simulations. we illustrate a technique for automatically classifying and analyzing massive real-world i-v curves and for gaining insights into the performance of pv modules. by classifying 3.7 million i-v curves, we demonstrate the occurrence of step i-v curves under two irradiance conditions: under non-uniform irradiance condition, mirror augmented pv module in cleveland, ohio; and under uniform irraidance condition at the negev desert, israel, gran canaria, spain and mount zugspitze, germany. under the uniform irradiance conditions, we found that the percentage of step i-v curves in all three i-v curve types gradually increase over time. this indicates the electrical characteristics within a pv module change from homogeneous to heterogeneous. since the step i-v curves have a lower maximum power and a lower fill factor than normal i-v curves at the same irradiance condition, the heterogeneity in i-v module directly cause power degradation.
017f6842-f67f-492a-8c36-bebf6a9f8703 balancedboost: a hybrid approach for real-time network traffic classification. 
017f6f4f-579b-476b-a006-6b2a5593bb87 deepsd: supply-demand prediction for online car-hailing services using deep neural networks the online car-hailing service has gained great popularity all over the world. as more passengers and more drivers use the service, it becomes increasingly more important for the the car-hailing service providers to effectively schedule the drivers to minimize the waiting time of passengers and maximize the driver utilization, thus to improve the overall user experience. in this paper, we study the problem of predicting the real-time car-hailing supply-demand, which is one of the most important component of an effective scheduling system. our objective is to predict the gap between the car-hailing supply and demand in a certain area in the next few minutes. based on the prediction, we can balance the supply-demands by scheduling the drivers in advance. we present an end-to-end framework called deep supply-demand (deepsd) using a novel deep neural network structure. our approach can automatically discover complicated supply-demand patterns from the car-hailing service data while only requires a minimal amount hand-crafted features. moreover, our framework is highly flexible and extendable. based on our framework, it is very easy to utilize multiple data sources (e.g., car-hailing orders, weather and traffic data) to achieve a high accuracy. we conduct extensive experimental evaluations, which show that our framework provides more accurate prediction results than the existing methods.
017fc8ee-67c4-4aae-b502-d5160a09745b genetics-based machine learning systems for classification task 
018027cc-7db4-49f3-9f3b-cb1e299623f7 complexity measures for software systems : towards multi-agent based software testing bringing together agents and other fields of software engineering might be difficult, as the advantages of agent technology are still not widely recognized. effectiveness claims of agent-oriented software engineering are based upon the strategies for addressing complex systems. agent technologies facilitate the automated software testing by virtue of their high-level decomposition, independency and parallel activation. the informal interpretations of qualitative agent theories are not sufficient to distinguish agent-based approaches from other approaches in software testing. in this paper, we do not just described the agent-based approach in software testing, also developed an evaluation framework for agent-oriented approach in software testing and proposed a multi-agent system for software testing. this paper therefore provides a timely summary and enhancement of agent theory in software testing, which motivates recent efforts in adapting concepts and methodologies for agent-oriented software testing (aost) to complex systems, which has not previously done. the 'multi-modal' approach proposed here is to offer a definition for encompassing to cover the software testing phenomena, based on agents, at the preliminary level, yet sufficiently tight that it can rule out complex systems that are clearly not agent-based.
01816602-d28e-48cf-a3c9-843078f0187f cognitive decision support system for medical diagnosis the expert medical diagnosis of infections is a difficult task where it considers numerous quantitative and qualitative values of physical factors, symptoms, laboratory test reports. the knowledge of doctors rendering to the physical clinical inspection and medical test are the key opinion to flourish a diagnosis and treatment of a patient. this paper presents different methods of representing the knowledge for expert medical diagnosis systems and utilising it for rational decision making to predict the problem. the focal point of the research work is the real time implementation of cognitive medical decision making systems using machine learning and fuzzy-cognitive-map model. also, compare these methods for its suitability as cognitive decision-making technologies for an expert system.
01817779-ae16-4a99-a600-ac7eb4819ada estimate criteria for efficacy of treatment in benign prostatic hyperplasia background: various treatment modalities for benign prostatic hyperplasia (bph) have emerged and are now in use or await evaluation of clinical usefulness. it is difficult, however, to compare their efficacies on a single scale, because standardized criteria for therapeutic efficacy of bph treatments have not been established.#r##n##r##n##r##n##r##n#patients and methods: a total of 692 bph patients from 8 institutions in japan received various treatments, and were judged by specialized physicians for overall efficacy, and for efficacy in 4 domains: symptom, function, anatomy, and quality of life (qol). efficacy of treatment was graded as excellent, good, fair or poor, and assessed using items based on conventional clinical measurements. these items included 1) the difference (post-pretreatment value), 2) relative ratio (post/pre) and, 3) the individual values of pre or posttreatment measurements. the cut off levels for each grade were heuristically selected by spearman's rank correlation and multiple regression analysis so that the results accurately predicted physicians' judgement, while the feasibility was maintained.#r##n##r##n##r##n##r##n#results: the results for each efficacy grade (range of excellent, range of good, range of, fair, range of poor) were summarized as follows:#r##n##r##n##r##n##r##n#symptom: (post/pre treatment ratio of i-pss)  0.25,  0.5,  0.75, > 0.75.#r##n##r##n##r##n##r##n#function: (post-pre of qmax)  10 ml/s,  5 ml/s,  0.25 ml/s, < 0.25 ml/s.#r##n##r##n##r##n##r##n#anatomy: (post/pre ratio of prostate volume)  0.5,  0.75,  0.9, > 0.9.#r##n##r##n##r##n##r##n#qol: (pre-post of qol index)  4, 3, 2 and 1,  0.#r##n##r##n##r##n##r##n#the overall efficacy grade was defined as the median of efficacy grades of 3 domains: symptom, function and qol. the agreement rates between the criteria and physicians' judgement on the dichotomous efficacy (either excellent plus good, or fair plus poor) were approximately 80% in individual domains and overall estimate, and consistent among various treatments.#r##n##r##n##r##n##r##n#conclusion: the proposed criteria are fairly accurate, simple, and practical, and thus may be useful as a standard method for assessing the clinical efficacy of bph treatments.
0182f67d-55d8-413f-8d2b-8323c082b518 data mining from multimedia patient records most current patient records mining applications (for classification, prediction, and for other data mining objectives) are based on a standard representation in the form of structured records with numerical and/or categorical values. the significant advances in pre-processing, pattern recognition, and interpretation of medical images, texts and signals can, and should, be coupled with other data mining and knowledge discovery techniques, to increase the benefits of mining multimedia patient records. this integration is expected to greatly improve the results of patient records mining specifically when applied to a comprehensive set of data that includes description of the patient history and status. to achieve these objectives, careful selection of appropriate techniques is required, especially in the preprocessing phase following a specified methodology. in this chapter, the importance of preprocessing and feature extraction phases in mining large collections of multimedia patient records is emphasized. selected techniques with illustrative examples are given showing the applicability of rule-based methodologies in the preparation phases of a data mining process.
0183c64f-ff4d-43d4-967c-d52462500c08 rough mappings rough sets, a tool for data mining, deal with the vagueness and granularity in information systems. approximating of functions specified by imperfect knowledge is one of the central issues in many areas. the rough function is presented by pawlak, and has been applied to rough controls. however, the function is only a special case of mapping. in this paper the concept of rough function is generalized to rough mapping and various set theoretic properties are exploited to characterize the rough mappings. in order to explain the idea of rough mapping, the two examples, rough functions and rough fuzzy sets are mentioned. finally, the rough integrals are proposed as an possible applications of the rough mappings.
018413ba-93ce-4b84-95d2-bc5846556922 antecedents of open source software defects: a data mining approach to model formulation, validation and testing this paper develops tests and validates a model for the antecedents of open source software (oss) defects, using data and text mining. the public archives of oss projects are used to access historical data on over 5,000 active and mature oss projects. using domain knowledge and exploratory analysis, a wide range of variables is identified from the process, product, resource, and end-user characteristics of a project to ensure that the model is robust and considers all aspects of the system. multiple data mining techniques are used to refine the model and data is enriched by the use of text mining for knowledge discovery from qualitative information. the study demonstrates the suitability of data mining and text mining for model building. results indicate that project type, end-user activity, process quality, team size and project popularity have a significant impact on the defect density of operational oss projects. since many organizations, both for profit and not for profit, are beginning to use open source software as an economic alternative to commercial software, these results can be used in the process of deciding what software can be reasonably maintained by an organization.
01843e6d-5ec4-4c95-9d8b-9efa1ed337c0 the attribute reduction of the information system based on new rough set attribute reduction is considered as an important preprocessing step for pattern recognition, machine learning, and data mining. the traditional rough set theory is mainly used to reduce the attributes and keep the lower approximation unchanged. in this paper we first give two forms of new rough sets: object-oriented rough set and attribute-oriented rough set, and then discuss their properties in detail. based on the new models, this paper studies the attribute reduction of information system. at last it studies the attribute reduction of decision information systems by combining the old rough set and new rough set together.
0184b179-06d1-493e-aa65-89b8521a5140 umduluth-cs8761-12: a novel machine learning approach for aspect based sentiment analysis this paper provides a detailed description of the approach of our system for the aspectbased sentiment analysis task of semeval2015. the task is to identify the aspect category (entity and attribute pair), opinion target and sentiment of the reviews. for the in-domain subtask that is provided with the training data, the system is developed using a supervised technique support vector machine and for the out-of-domain subtask for which the training data is not provided, it is implemented based on the sentiment score of the vocabulary. for in-domain subtask, our system is developed specifically for restaurant data.
